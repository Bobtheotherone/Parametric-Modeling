================================================================================
 FLATTENED REPOSITORY
 Source: \\wsl.localhost\Ubuntu-22.04\home\rnmercado\Parametric_Modeling\Agent_Control\formula-foundry-tri-agent\bridge
 Files included: 38
================================================================================

TABLE OF CONTENTS
----------------------------------------
  - __init__.py
  - atomic_io.py
  - config.json
  - design_doc.py
  - gemini.py
  - loop.py
  - merge_resolver.py
  - patch_integration.py
  - provider_adapters.py
  - reports.py
  - scheduler.py
  - smoke_route.py
  - streaming.py
  - task_plan.schema.json
  - turn.schema.json
  - turn_payload.schema.json
  - turns.py
  - verify_repair.py
  - agents\claude.sh
  - agents\codex.sh
  - loop_pkg\__init__.py
  - loop_pkg\config.py
  - loop_pkg\policy.py
  - loop_pkg\turn_normalizer.py
  - mock_scenarios\dual_agent_smoke.json
  - mock_scenarios\milestone_demo.json
  - mock_scenarios\smoke_route.json
  - mock_scenarios\streaming_demo.json
  - prompts\system.md
  - prompts\system_engineering.md
  - verify_repair\__init__.py
  - verify_repair\agent_tasks.py
  - verify_repair\bootstrap.py
  - verify_repair\classify.py
  - verify_repair\data.py
  - verify_repair\executor.py
  - verify_repair\loop.py
  - verify_repair\repairs.py


================================================================================
 FILE: __init__.py
================================================================================

"""Orchestrator bridge package."""

================================================================================
 FILE: atomic_io.py
================================================================================

#!/usr/bin/env python3
"""Atomic I/O utilities for robust file operations.

This module provides atomic write operations that ensure files are never
left in a corrupted or empty state, even if the process is interrupted.

Key guarantees:
- Files are written completely or not at all
- 0-byte files are never produced on interrupted writes
- JSON files are validated before being considered written
- All writes use fsync to ensure durability
"""

from __future__ import annotations

import json
import os
from collections.abc import Callable
from pathlib import Path
from typing import Any


class AtomicWriteError(Exception):
    """Raised when atomic write fails."""

    pass


class JSONValidationError(Exception):
    """Raised when JSON validation fails."""

    pass


def atomic_write_text(
    path: Path | str,
    content: str,
    encoding: str = "utf-8",
) -> None:
    """Write text to a file atomically.

    Uses write-to-temp + fsync + rename pattern to ensure:
    - The file is never left empty or partial
    - Interrupted writes don't corrupt the file
    - The operation is atomic on POSIX systems

    Args:
        path: Target file path
        content: Text content to write
        encoding: Text encoding (default: utf-8)

    Raises:
        AtomicWriteError: If write fails
    """
    path = Path(path)
    tmp_path = path.with_suffix(path.suffix + ".tmp")

    try:
        # Ensure parent directory exists
        path.parent.mkdir(parents=True, exist_ok=True)

        # Write to temp file with fsync
        with open(tmp_path, "w", encoding=encoding) as f:
            f.write(content)
            f.flush()
            os.fsync(f.fileno())

        # Atomic rename
        tmp_path.rename(path)

    except Exception as e:
        # Clean up temp file if it exists
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise AtomicWriteError(f"Failed to write {path}: {e}") from e


def atomic_write_json(
    path: Path | str,
    data: dict[str, Any] | list[Any],
    indent: int = 2,
    validate_schema: Callable[[dict], bool] | None = None,
) -> None:
    """Write JSON data to a file atomically with optional validation.

    Args:
        path: Target file path
        data: JSON-serializable data
        indent: JSON indentation level
        validate_schema: Optional validation function that returns True if valid

    Raises:
        AtomicWriteError: If write fails
        JSONValidationError: If validation function returns False
    """
    path = Path(path)

    # Serialize to string first to catch serialization errors early
    try:
        content = json.dumps(data, indent=indent, ensure_ascii=False)
    except (TypeError, ValueError) as e:
        raise JSONValidationError(f"Failed to serialize JSON: {e}") from e

    # Validate content if validator provided
    if validate_schema is not None:
        try:
            parsed = json.loads(content)
            if not validate_schema(parsed):
                raise JSONValidationError("Schema validation failed")
        except json.JSONDecodeError as e:
            raise JSONValidationError(f"Invalid JSON after serialization: {e}") from e

    # Write atomically
    atomic_write_text(path, content)


def atomic_copy_file(
    src: Path | str,
    dst: Path | str,
) -> None:
    """Copy a file atomically.

    Args:
        src: Source file path
        dst: Destination file path

    Raises:
        AtomicWriteError: If copy fails
    """
    src = Path(src)
    dst = Path(dst)

    if not src.exists():
        raise AtomicWriteError(f"Source file does not exist: {src}")

    content = src.read_bytes()
    tmp_path = dst.with_suffix(dst.suffix + ".tmp")

    try:
        dst.parent.mkdir(parents=True, exist_ok=True)

        with open(tmp_path, "wb") as f:
            f.write(content)
            f.flush()
            os.fsync(f.fileno())

        tmp_path.rename(dst)

    except Exception as e:
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise AtomicWriteError(f"Failed to copy {src} to {dst}: {e}") from e


def validate_json_file(
    path: Path | str,
    validator: Callable[[dict], tuple[bool, str | None]] | None = None,
) -> tuple[bool, dict | None, str | None]:
    """Validate a JSON file exists, is non-empty, and parses correctly.

    Args:
        path: Path to JSON file
        validator: Optional function that returns (is_valid, error_message)

    Returns:
        Tuple of (is_valid, parsed_data, error_message)
    """
    path = Path(path)

    # Check file exists
    if not path.exists():
        return False, None, f"File does not exist: {path}"

    # Check file is not empty
    try:
        content = path.read_text(encoding="utf-8")
    except Exception as e:
        return False, None, f"Failed to read file: {e}"

    if not content.strip():
        return False, None, "File is empty (0 bytes)"

    # Parse JSON
    try:
        data = json.loads(content)
    except json.JSONDecodeError as e:
        return False, None, f"Invalid JSON: {e}"

    # Run custom validator if provided
    if validator is not None:
        try:
            is_valid, error_msg = validator(data)
            if not is_valid:
                return False, data, error_msg
        except Exception as e:
            return False, data, f"Validation error: {e}"

    return True, data, None


def safe_read_json(
    path: Path | str,
    default: dict | list | None = None,
) -> tuple[dict | list | None, str | None]:
    """Safely read a JSON file, returning default if file is missing/invalid.

    Args:
        path: Path to JSON file
        default: Default value if file cannot be read

    Returns:
        Tuple of (data, error_message or None)
    """
    is_valid, data, error = validate_json_file(path)
    if is_valid:
        return data, None
    else:
        return default, error


def recover_or_create_json(
    path: Path | str,
    creator: Callable[[], dict],
    validator: Callable[[dict], tuple[bool, str | None]] | None = None,
    max_attempts: int = 2,
) -> tuple[bool, dict | None, str | None]:
    """Try to read a JSON file, create it if missing/invalid.

    This function implements a recovery pattern:
    1. Try to read and validate the existing file
    2. If invalid, call the creator function to regenerate
    3. Write the new content atomically
    4. Validate the result

    Args:
        path: Path to JSON file
        creator: Function to create new content if needed
        validator: Optional validation function
        max_attempts: Maximum creation attempts

    Returns:
        Tuple of (success, data, error_message)
    """
    path = Path(path)

    for attempt in range(max_attempts):
        # Try to read existing file
        is_valid, data, error = validate_json_file(path, validator)
        if is_valid:
            return True, data, None

        # File is missing or invalid, try to create
        try:
            new_data = creator()
            atomic_write_json(path, new_data)
        except Exception as e:
            if attempt == max_attempts - 1:
                return False, None, f"Failed to create file after {max_attempts} attempts: {e}"
            continue

    # Final validation
    return validate_json_file(path, validator)


class AtomicJSONWriter:
    """Context manager for atomic JSON file writing with rollback support."""

    def __init__(self, path: Path | str, indent: int = 2):
        self.path = Path(path)
        self.indent = indent
        self.data: dict | list | None = None
        self._backup_path: Path | None = None

    def __enter__(self) -> AtomicJSONWriter:
        # Create backup if file exists
        if self.path.exists():
            self._backup_path = self.path.with_suffix(self.path.suffix + ".backup")
            try:
                atomic_copy_file(self.path, self._backup_path)
            except Exception:
                self._backup_path = None
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            # Exception occurred, restore backup if available
            if self._backup_path and self._backup_path.exists():
                try:
                    self._backup_path.rename(self.path)
                except Exception:
                    pass
        else:
            # Success, remove backup
            if self._backup_path and self._backup_path.exists():
                try:
                    self._backup_path.unlink()
                except Exception:
                    pass
        return False

    def write(self, data: dict | list) -> None:
        """Write data to file atomically."""
        self.data = data
        atomic_write_json(self.path, data, indent=self.indent)

================================================================================
 FILE: config.json
================================================================================

{
  "agents": {
    "claude": {
      "quota_error_patterns": [
        "rate limit",
        "429",
        "quota",
        "hit your limit",
        "resets.*\\(",
        "You've hit your limit"
      ],
      "role": "reviewer",
      "script": "bridge/agents/claude.sh",
      "supports_write_access": true,
      "supports_tools": true,
      "supports_fs_read": true,
      "supports_fs_write": true,
      "supports_bash": true
    },
    "codex": {
      "quota_error_patterns": [
        "rate limit",
        "429",
        "quota"
      ],
      "role": "implementer",
      "script": "bridge/agents/codex.sh",
      "supports_write_access": true,
      "supports_tools": true,
      "supports_fs_read": true,
      "supports_fs_write": true,
      "supports_bash": true
    }
  },
  "allow_os_write": true,
  "enable_agents": [
    "codex",
    "claude"
  ],
  "fallback_order": [
    "codex",
    "claude"
  ],
  "limits": {
    "max_calls_per_agent": 15,
    "max_json_correction_attempts": 3,
    "max_total_calls": 150,
    "quota_retry_attempts": 5
  },
  "parallel": {
    "consecutive_samples": 3,
    "cpu_intensive_threshold_pct": 40.0,
    "disable_gpu_by_default": true,
    "kill_grace_s": 8.0,
    "max_workers_default": 10,
    "mem_intensive_threshold_pct": 40.0,
    "sample_interval_s": 1.0,
    "terminal_max_bytes_per_worker": 40000,
    "terminal_max_line_length": 600
  },
  "version": 2,
  "weights": {
    "claude": 1.0,
    "codex": 1.0
  }
}

================================================================================
 FILE: design_doc.py
================================================================================

#!/usr/bin/env python3
"""Design document parser for modular orchestration.

This module provides robust, format-agnostic parsing of design documents.
The orchestrator MUST NOT be tightly coupled to a specific design doc format.
It accepts arbitrary markdown structure and extracts what it can using heuristics.

Key design principles:
- Graceful degradation: If something can't be extracted, continue with warnings
- Multiple extraction strategies: Try various patterns for each field
- Contract modes: strict (CI gate), loose (development), off (bypass)

Usage:
    from bridge.design_doc import parse_design_doc, DesignDocSpec

    spec = parse_design_doc(Path("DESIGN_DOCUMENT.md"), contract_mode="loose")
    if not spec.is_valid:
        print(f"Warnings: {spec.warnings}")
        print(f"Errors: {spec.errors}")
"""

from __future__ import annotations

import hashlib
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Literal

ContractMode = Literal["strict", "loose", "off"]


@dataclass
class Requirement:
    """A parsed requirement from the design document."""

    id: str
    text: str
    line_no: int

    def to_dict(self) -> dict:
        return {"id": self.id, "text": self.text, "line_no": self.line_no}


@dataclass
class DesignDocSpec:
    """Parsed design document specification.

    This dataclass contains all extracted data from a design document,
    with warnings for anything that couldn't be extracted.
    """

    # Source information
    path: Path
    raw_text: str
    doc_hash: str  # SHA-256 of raw_text

    # Extracted fields (may be None/empty if not found)
    milestone_id: str | None = None
    title: str | None = None
    requirements: list[Requirement] = field(default_factory=list)
    definition_of_done: list[str] = field(default_factory=list)
    test_matrix: dict[str, list[str]] = field(default_factory=dict)

    # Validation results
    warnings: list[str] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)
    contract_mode_used: ContractMode = "loose"

    @property
    def is_valid(self) -> bool:
        """Return True if no errors (warnings are acceptable)."""
        return len(self.errors) == 0

    @property
    def requirement_ids(self) -> list[str]:
        """Return list of requirement IDs."""
        return [r.id for r in self.requirements]

    def get_requirement_text(self, req_id: str) -> str | None:
        """Get the text for a specific requirement ID."""
        for r in self.requirements:
            if r.id == req_id:
                return r.text
        return None

    def to_prompt_context(self, max_raw_chars: int = 20000) -> str:
        """Generate prompt context string for agent consumption.

        This provides structured data that agents can use without
        needing to parse the raw document themselves.
        """
        parts = []

        parts.append("## Design Document Context")
        parts.append(f"**Document Hash:** {self.doc_hash[:16]}")

        if self.milestone_id:
            parts.append(f"**Milestone:** {self.milestone_id}")

        if self.title:
            parts.append(f"**Title:** {self.title}")

        if self.requirements:
            parts.append(f"\n### Requirements ({len(self.requirements)} total)")
            for r in self.requirements:
                parts.append(f"- **{r.id}**: {r.text[:200]}{'...' if len(r.text) > 200 else ''}")

        if self.definition_of_done:
            parts.append("\n### Definition of Done")
            for item in self.definition_of_done:
                parts.append(f"- {item}")

        if self.test_matrix:
            parts.append("\n### Test Matrix")
            for req_id, tests in self.test_matrix.items():
                parts.append(f"- {req_id}: {', '.join(tests)}")

        # Include truncated raw text for additional context
        if max_raw_chars > 0:
            truncated = self.raw_text[:max_raw_chars]
            if len(self.raw_text) > max_raw_chars:
                truncated += f"\n\n[... truncated {len(self.raw_text) - max_raw_chars} chars ...]"
            parts.append(f"\n### Raw Document Excerpt\n```markdown\n{truncated}\n```")

        return "\n".join(parts)

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "path": str(self.path),
            "doc_hash": self.doc_hash,
            "milestone_id": self.milestone_id,
            "title": self.title,
            "requirements": [r.to_dict() for r in self.requirements],
            "definition_of_done": self.definition_of_done,
            "test_matrix": self.test_matrix,
            "warnings": self.warnings,
            "errors": self.errors,
            "contract_mode_used": self.contract_mode_used,
            "is_valid": self.is_valid,
        }


# =============================================================================
# Milestone ID Extraction
# =============================================================================

# Patterns to try for milestone extraction (in priority order)
MILESTONE_PATTERNS = [
    # **Milestone:** M1 — description
    re.compile(r"\*\*Milestone:\*\*\s*(M\d+)\b", re.IGNORECASE),
    # **Milestone ID:** M1
    re.compile(r"\*\*Milestone\s+ID:\*\*\s*(M\d+)\b", re.IGNORECASE),
    # Milestone: M1
    re.compile(r"^Milestone:\s*(M\d+)\b", re.MULTILINE | re.IGNORECASE),
    # # M1 Design Document
    re.compile(r"^#\s+(M\d+)\s+", re.MULTILINE),
    # M1 — at the start of a heading
    re.compile(r"^#+\s*(M\d+)\s*[—\-–:]", re.MULTILINE),
    # ## M1: Title
    re.compile(r"^#+\s*(M\d+):", re.MULTILINE),
]


def _extract_milestone_id(text: str) -> tuple[str | None, str | None]:
    """Extract milestone ID using multiple patterns.

    Returns:
        Tuple of (milestone_id, pattern_name_used) or (None, None)
    """
    for i, pattern in enumerate(MILESTONE_PATTERNS):
        match = pattern.search(text)
        if match:
            return match.group(1), f"pattern_{i}"
    return None, None


def _extract_all_milestones(text: str) -> list[str]:
    """Extract all milestone IDs found in the document."""
    milestones = set()
    for pattern in MILESTONE_PATTERNS:
        for match in pattern.finditer(text):
            milestones.add(match.group(1))
    return sorted(milestones, key=lambda x: int(x[1:]) if x[1:].isdigit() else 0)


# =============================================================================
# Title Extraction
# =============================================================================

TITLE_PATTERNS = [
    # # Title (first H1)
    re.compile(r"^#\s+(.+?)(?:\n|$)", re.MULTILINE),
    # **Title:** Something
    re.compile(r"\*\*Title:\*\*\s*(.+?)(?:\n|$)", re.IGNORECASE),
]


def _extract_title(text: str) -> str | None:
    """Extract document title."""
    for pattern in TITLE_PATTERNS:
        match = pattern.search(text)
        if match:
            title = match.group(1).strip()
            # Clean up markdown formatting
            title = re.sub(r"\*\*|\*|`", "", title)
            return title[:200] if title else None
    return None


# =============================================================================
# Requirements Extraction
# =============================================================================

# Primary pattern for requirements:
# - [REQ-M1-001] - with milestone prefix (most common)
# - [REQ-ABC-123] - with arbitrary prefix
# - [REQ-001] - without prefix (simple format)
# - REQ-BUG-001 - for bug requirements
# The pattern: REQ- followed by optional prefix (letters/numbers/hyphens) and then 3+ digits
REQ_ID_PATTERN = re.compile(r"\[?(REQ-(?:[A-Z0-9]+-)*\d{3,})\]?")

# Alternative patterns for requirement-like structures
ALT_REQ_PATTERNS = [
    # R1, R2, etc.
    re.compile(r"^\s*-\s*\[?(R\d+)\]?\s+(.+)", re.MULTILINE),
    # REQ1, REQ2, etc.
    re.compile(r"^\s*-\s*\[?(REQ\d+)\]?\s+(.+)", re.MULTILINE),
]


def _extract_requirements(text: str, lines: list[str]) -> tuple[list[Requirement], list[str]]:
    """Extract requirements from the document.

    Returns:
        Tuple of (requirements, warnings)
    """
    requirements: list[Requirement] = []
    seen_ids: set[str] = set()
    warnings: list[str] = []

    # Strategy 1: Look for REQ-* pattern with following text
    for line_no, line in enumerate(lines, 1):
        # Find all REQ IDs in this line
        matches = list(REQ_ID_PATTERN.finditer(line))
        for match in matches:
            req_id = match.group(1)

            if req_id in seen_ids:
                warnings.append(f"Duplicate requirement ID {req_id} at line {line_no}")
                continue

            seen_ids.add(req_id)

            # Extract the requirement text (everything after the ID on the same line)
            text_start = match.end()
            req_text = line[text_start:].strip()

            # Clean up leading punctuation/brackets
            req_text = re.sub(r"^[\]\s:\-–—]+", "", req_text).strip()

            # If no text on same line, try to get the next non-empty line
            if not req_text and line_no < len(lines):
                for next_line in lines[line_no : line_no + 3]:
                    next_line = next_line.strip()
                    if next_line and not next_line.startswith("#") and not next_line.startswith("-"):
                        req_text = next_line
                        break

            requirements.append(
                Requirement(
                    id=req_id,
                    text=req_text or "(no description)",
                    line_no=line_no,
                )
            )

    # Strategy 2: If no REQ-* found, try alternative patterns
    if not requirements:
        for pattern in ALT_REQ_PATTERNS:
            for match in pattern.finditer(text):
                req_id = match.group(1)
                req_text = match.group(2).strip() if match.lastindex >= 2 else ""

                if req_id not in seen_ids:
                    seen_ids.add(req_id)
                    requirements.append(
                        Requirement(
                            id=req_id,
                            text=req_text or "(no description)",
                            line_no=text[: match.start()].count("\n") + 1,
                        )
                    )

    return requirements, warnings


# =============================================================================
# Definition of Done Extraction
# =============================================================================

# Headers that might indicate DoD section (case-insensitive fuzzy matching)
DOD_HEADER_PATTERNS = [
    re.compile(r"^#+\s*Definition\s+of\s+Done\b", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^#+\s*DoD\b", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^#+\s*Done\s+When\b", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^#+\s*Acceptance\s+Criteria\b", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^#+\s*Success\s+Criteria\b", re.MULTILINE | re.IGNORECASE),
    re.compile(r"^\*\*Definition\s+of\s+Done[:\*]*\b", re.MULTILINE | re.IGNORECASE),
]


def _extract_definition_of_done(text: str, lines: list[str]) -> list[str]:
    """Extract Definition of Done bullets."""
    dod_items: list[str] = []

    # Find the start of the DoD section
    dod_start = None
    for pattern in DOD_HEADER_PATTERNS:
        match = pattern.search(text)
        if match:
            dod_start = match.end()
            break

    if dod_start is None:
        return []

    # Find where the section ends (next header or end of document)
    section_text = text[dod_start:]
    next_header_match = re.search(r"^#+\s", section_text, re.MULTILINE)
    if next_header_match:
        section_text = section_text[: next_header_match.start()]

    # Extract bullet points
    bullet_pattern = re.compile(r"^\s*[-*]\s+(.+)", re.MULTILINE)
    for match in bullet_pattern.finditer(section_text):
        item = match.group(1).strip()
        if item and len(item) > 2:  # Filter out very short items (e.g., just punctuation)
            dod_items.append(item)

    return dod_items


# =============================================================================
# Test Matrix Extraction
# =============================================================================


def _extract_test_matrix(text: str, lines: list[str]) -> tuple[dict[str, list[str]], list[str]]:
    """Extract test matrix mapping requirements to pytest node IDs.

    Returns:
        Tuple of (test_matrix dict, warnings)
    """
    test_matrix: dict[str, list[str]] = {}
    warnings: list[str] = []

    # Look for markdown table with "Requirement" and "pytest" headers
    # Pattern: | Requirement | Pytest | or similar
    table_header_pattern = re.compile(r"^\s*\|\s*(?:Requirement|REQ).*\|.*(?:pytest|test|coverage)", re.MULTILINE | re.IGNORECASE)

    table_start = None
    for i, line in enumerate(lines):
        if table_header_pattern.match(line):
            table_start = i
            break

    if table_start is None:
        return {}, []

    # Skip header and separator rows
    data_start = table_start + 2
    if data_start >= len(lines):
        return {}, []

    # Parse table rows
    for line in lines[data_start:]:
        line = line.strip()
        if not line.startswith("|"):
            break

        # Split by | and get columns
        cols = [c.strip() for c in line.strip("|").split("|")]
        if len(cols) < 2:
            continue

        req_col = cols[0].strip()
        tests_col = cols[1].strip() if len(cols) > 1 else ""

        # Extract requirement ID from first column
        req_match = REQ_ID_PATTERN.search(req_col)
        if not req_match:
            continue

        req_id = req_match.group(1)

        # Parse test node IDs (comma or semicolon separated)
        test_ids = [t.strip() for t in re.split(r"[,;]", tests_col) if t.strip()]
        # Filter to likely pytest node IDs
        test_ids = [t for t in test_ids if "::" in t or t.startswith("tests/") or t.startswith("test_")]

        if test_ids:
            if req_id in test_matrix:
                warnings.append(f"Duplicate test matrix entry for {req_id}")
                test_matrix[req_id].extend(test_ids)
            else:
                test_matrix[req_id] = test_ids

    return test_matrix, warnings


# =============================================================================
# Contract Validation
# =============================================================================


def _validate_contract(spec: DesignDocSpec, mode: ContractMode) -> None:
    """Validate the spec against the contract mode.

    Adds errors/warnings to the spec in place.
    """
    if mode == "off":
        return

    if mode == "strict":
        # Strict mode: must have milestone_id, requirements, DoD, and full test matrix
        if not spec.milestone_id:
            spec.errors.append("STRICT: Missing milestone_id (use --milestone-id to override)")

        if not spec.requirements:
            spec.errors.append("STRICT: No requirements found in document")

        if not spec.definition_of_done:
            spec.errors.append("STRICT: No Definition of Done section found")

        if not spec.test_matrix:
            spec.errors.append("STRICT: No test matrix found")
        elif spec.requirements:
            # Check all requirements have test coverage
            missing_coverage = [r.id for r in spec.requirements if r.id not in spec.test_matrix]
            if missing_coverage:
                spec.errors.append(f"STRICT: Requirements missing test coverage: {missing_coverage}")

    elif mode == "loose":
        # Loose mode: must have at least milestone_id OR some requirements
        if not spec.milestone_id and not spec.requirements:
            spec.errors.append("LOOSE: Need at least milestone_id or some requirements")

        # Warn but don't fail on missing sections
        if not spec.milestone_id:
            spec.warnings.append("Could not extract milestone_id")

        if not spec.definition_of_done:
            spec.warnings.append("No Definition of Done section found")

        if not spec.test_matrix:
            spec.warnings.append("No test matrix found")
        elif spec.requirements:
            missing_coverage = [r.id for r in spec.requirements if r.id not in spec.test_matrix]
            if missing_coverage:
                spec.warnings.append(f"Requirements missing test coverage: {missing_coverage}")


# =============================================================================
# Main Parser Function
# =============================================================================


def parse_design_doc(
    path: Path | str,
    contract_mode: ContractMode = "loose",
    milestone_override: str | None = None,
) -> DesignDocSpec:
    """Parse a design document and extract structured data.

    Args:
        path: Path to the design document
        contract_mode: Validation strictness level
            - "strict": Fail if any required field is missing
            - "loose": Warn on missing fields, fail only if nothing found
            - "off": No validation, just extract what's possible
        milestone_override: Override the extracted milestone_id (CLI flag)

    Returns:
        DesignDocSpec with extracted data and any warnings/errors
    """
    path = Path(path)

    # Handle non-existent file
    if not path.exists():
        return DesignDocSpec(
            path=path,
            raw_text="",
            doc_hash="",
            contract_mode_used=contract_mode,
            errors=[f"Design document not found: {path}"],
        )

    # Read the document
    try:
        raw_text = path.read_text(encoding="utf-8")
    except Exception as e:
        return DesignDocSpec(
            path=path,
            raw_text="",
            doc_hash="",
            contract_mode_used=contract_mode,
            errors=[f"Failed to read design document: {e}"],
        )

    # Compute hash
    doc_hash = hashlib.sha256(raw_text.encode("utf-8")).hexdigest()

    # Split into lines for line-number tracking
    lines = raw_text.splitlines()

    # Extract all fields
    milestone_id, _ = _extract_milestone_id(raw_text)
    title = _extract_title(raw_text)
    requirements, req_warnings = _extract_requirements(raw_text, lines)
    definition_of_done = _extract_definition_of_done(raw_text, lines)
    test_matrix, matrix_warnings = _extract_test_matrix(raw_text, lines)

    # Apply milestone override if provided
    if milestone_override:
        milestone_id = milestone_override

    # Build the spec
    spec = DesignDocSpec(
        path=path,
        raw_text=raw_text,
        doc_hash=doc_hash,
        milestone_id=milestone_id,
        title=title,
        requirements=requirements,
        definition_of_done=definition_of_done,
        test_matrix=test_matrix,
        warnings=req_warnings + matrix_warnings,
        errors=[],
        contract_mode_used=contract_mode,
    )

    # Validate against contract
    _validate_contract(spec, contract_mode)

    return spec


def parse_design_doc_text(
    text: str,
    contract_mode: ContractMode = "loose",
    milestone_override: str | None = None,
    path: Path | str = Path("<inline>"),
) -> DesignDocSpec:
    """Parse design document from raw text (for testing).

    Args:
        text: Raw markdown text
        contract_mode: Validation strictness level
        milestone_override: Override the extracted milestone_id
        path: Path to use in the spec (for identification)

    Returns:
        DesignDocSpec with extracted data
    """
    path = Path(path)
    doc_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
    lines = text.splitlines()

    milestone_id, _ = _extract_milestone_id(text)
    title = _extract_title(text)
    requirements, req_warnings = _extract_requirements(text, lines)
    definition_of_done = _extract_definition_of_done(text, lines)
    test_matrix, matrix_warnings = _extract_test_matrix(text, lines)

    if milestone_override:
        milestone_id = milestone_override

    spec = DesignDocSpec(
        path=path,
        raw_text=text,
        doc_hash=doc_hash,
        milestone_id=milestone_id,
        title=title,
        requirements=requirements,
        definition_of_done=definition_of_done,
        test_matrix=test_matrix,
        warnings=req_warnings + matrix_warnings,
        errors=[],
        contract_mode_used=contract_mode,
    )

    _validate_contract(spec, contract_mode)
    return spec


# =============================================================================
# CLI for standalone testing
# =============================================================================


def main() -> int:
    """CLI entry point for testing the parser."""
    import argparse
    import json

    parser = argparse.ArgumentParser(description="Parse a design document")
    parser.add_argument("doc", type=str, help="Path to design document")
    parser.add_argument(
        "--contract",
        choices=["strict", "loose", "off"],
        default="loose",
        help="Contract validation mode",
    )
    parser.add_argument(
        "--milestone-id",
        type=str,
        help="Override milestone ID",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output as JSON",
    )
    args = parser.parse_args()

    spec = parse_design_doc(
        Path(args.doc),
        contract_mode=args.contract,
        milestone_override=args.milestone_id,
    )

    if args.json:
        print(json.dumps(spec.to_dict(), indent=2))
    else:
        print(f"Path: {spec.path}")
        print(f"Hash: {spec.doc_hash[:16]}...")
        print(f"Milestone: {spec.milestone_id or '(not found)'}")
        print(f"Title: {spec.title or '(not found)'}")
        print(f"Requirements: {len(spec.requirements)}")
        for r in spec.requirements[:5]:
            print(f"  - {r.id}: {r.text[:60]}...")
        if len(spec.requirements) > 5:
            print(f"  ... and {len(spec.requirements) - 5} more")
        print(f"DoD items: {len(spec.definition_of_done)}")
        print(f"Test matrix entries: {len(spec.test_matrix)}")
        print(f"Warnings: {len(spec.warnings)}")
        for w in spec.warnings:
            print(f"  - {w}")
        print(f"Errors: {len(spec.errors)}")
        for e in spec.errors:
            print(f"  - {e}")
        print(f"Valid: {spec.is_valid}")

    return 0 if spec.is_valid else 1


if __name__ == "__main__":
    raise SystemExit(main())

================================================================================
 FILE: gemini.py
================================================================================

#!/usr/bin/env python3
"""Gemini wrapper that always emits schema-valid Turn JSON."""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import subprocess
import sys
from collections.abc import Iterable
from pathlib import Path
from typing import Any

try:
    from bridge.turns import build_error_turn
except Exception:  # pragma: no cover - import shim for direct execution
    ROOT = Path(__file__).resolve().parents[1]
    sys.path.insert(0, str(ROOT))
    from bridge.turns import build_error_turn  # type: ignore[import-not-found]

try:
    import jsonschema  # type: ignore[import-untyped]

    HAS_JSONSCHEMA = True
except Exception:
    HAS_JSONSCHEMA = False


MILESTONE_PATTERNS = [
    re.compile(r"\*\*Milestone:\*\*\s*(M\d+)\b", re.IGNORECASE),
    re.compile(r"\*\*Milestone\s+ID:\*\*\s*(M\d+)\b", re.IGNORECASE),
    re.compile(r"^Milestone:\s*(M\d+)\b", re.IGNORECASE | re.MULTILINE),
]

REQUIRED_KEYS = [
    "agent",
    "milestone_id",
    "phase",
    "work_completed",
    "project_complete",
    "summary",
    "gates_passed",
    "requirement_progress",
    "next_agent",
    "next_prompt",
    "delegate_rationale",
    "stats_refs",
    "needs_write_access",
    "artifacts",
]


def main() -> int:
    args = _parse_args()
    prompt_text = _read_text(args.prompt_path)
    schema = _load_schema(args.schema_path)
    allowed_agents = _extract_enum(schema, "agent") or ["codex", "claude"]
    allowed_phases = _extract_enum(schema, "phase") or ["plan", "implement", "verify", "finalize"]

    agent_preferred = os.environ.get("GEMINI_AGENT", "gemini")
    agent_id = _select_from_allowed(agent_preferred, allowed_agents)
    milestone_id = _extract_milestone(prompt_text) or os.environ.get("MILESTONE_ID", "M0")

    stats_id_set = _load_stats_ids(_repo_root())
    stats_refs = _extract_stats_refs(prompt_text)
    needs_write_access = _env_truthy("WRITE_ACCESS") or _env_truthy("ORCH_WRITE_ACCESS")

    try:
        result = _run_gemini(prompt_text)
    except Exception as exc:
        payload = _build_error_payload(
            agent_id=agent_id,
            milestone_id=milestone_id,
            allowed_agents=allowed_agents,
            allowed_phases=allowed_phases,
            summary="Gemini wrapper error: CLI invocation failed",
            error_detail=str(exc),
            stats_refs=stats_refs,
            stats_id_set=stats_id_set,
            needs_write_access=needs_write_access,
        )
    else:
        if result.returncode != 0:
            payload = _build_error_payload(
                agent_id=agent_id,
                milestone_id=milestone_id,
                allowed_agents=allowed_agents,
                allowed_phases=allowed_phases,
                summary="Gemini wrapper error: non-zero exit",
                error_detail=_format_process_error(result),
                stats_refs=stats_refs,
                stats_id_set=stats_id_set,
                needs_write_access=needs_write_access,
            )
        else:
            raw_text = (result.stdout or "").strip()
            fallback_text = (result.stderr or "").strip()
            candidate = _extract_json_payload(raw_text) or _extract_json_payload(fallback_text)
            if candidate is None:
                payload = _build_error_payload(
                    agent_id=agent_id,
                    milestone_id=milestone_id,
                    allowed_agents=allowed_agents,
                    allowed_phases=allowed_phases,
                    summary="Gemini wrapper error: malformed output",
                    error_detail=_format_parse_error(raw_text or fallback_text),
                    stats_refs=stats_refs,
                    stats_id_set=stats_id_set,
                    needs_write_access=needs_write_access,
                )
            else:
                valid, err = _validate_turn(candidate, schema, allowed_agents, allowed_phases)
                if valid:
                    payload = candidate
                else:
                    payload = _build_error_payload(
                        agent_id=agent_id,
                        milestone_id=milestone_id,
                        allowed_agents=allowed_agents,
                        allowed_phases=allowed_phases,
                        summary="Gemini wrapper error: output failed schema validation",
                        error_detail=_format_validation_error(err, raw_text or fallback_text),
                        stats_refs=stats_refs,
                        stats_id_set=stats_id_set,
                        needs_write_access=needs_write_access,
                    )

    _write_json(args.out_path, payload)
    sys.stdout.write(json.dumps(payload, indent=2, ensure_ascii=True))
    sys.stdout.write("\n")
    return 0


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Gemini wrapper")
    parser.add_argument("prompt_path", type=Path)
    parser.add_argument("schema_path", type=Path)
    parser.add_argument("out_path", type=Path)
    return parser.parse_args()


def _read_text(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _load_schema(path: Path) -> dict[str, Any] | None:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def _extract_enum(schema: dict[str, Any] | None, key: str) -> list[str]:
    if not isinstance(schema, dict):
        return []
    props = schema.get("properties", {})
    if not isinstance(props, dict):
        return []
    entry = props.get(key, {})
    if not isinstance(entry, dict):
        return []
    enum = entry.get("enum")
    if not isinstance(enum, list):
        return []
    return [str(item) for item in enum if isinstance(item, str)]


def _select_from_allowed(preferred: str, allowed: list[str]) -> str:
    if preferred in allowed:
        return preferred
    if "gemini" in allowed:
        return "gemini"
    if allowed:
        return allowed[0]
    return "codex"


def _extract_milestone(prompt_text: str) -> str | None:
    for pattern in MILESTONE_PATTERNS:
        match = pattern.search(prompt_text)
        if match:
            return match.group(1)
    return None


def _extract_stats_refs(prompt_text: str) -> list[str]:
    matches = re.findall(r"\b[A-Z]{2,}-\d+\b", prompt_text)
    seen: set[str] = set()
    out: list[str] = []
    for item in matches:
        if item not in seen:
            out.append(item)
            seen.add(item)
    return out


def _repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def _load_stats_ids(root: Path) -> set[str] | None:
    path = root / "STATS.md"
    if not path.exists():
        return None
    text = _read_text(path)
    ids = {m for m in re.findall(r"\b[A-Z]{2,}-\d+\b", text)}
    return ids if ids else None


def _env_truthy(name: str) -> bool:
    val = os.environ.get(name, "").strip().lower()
    return val in {"1", "true", "yes", "on"}


def _run_gemini(prompt_text: str) -> subprocess.CompletedProcess[str]:
    gemini_bin = os.environ.get("GEMINI_BIN", "gemini")
    cmd = [gemini_bin]
    model = os.environ.get("GEMINI_MODEL", "").strip()
    model_flag = os.environ.get("GEMINI_MODEL_FLAG", "--model")
    if model:
        cmd.extend([model_flag, model])

    extra_args = os.environ.get("GEMINI_ARGS", "").strip()
    if extra_args:
        cmd.extend(shlex.split(extra_args))

    prompt_flag = os.environ.get("GEMINI_PROMPT_FLAG", "").strip()
    stdin_text: str | None = prompt_text
    if prompt_flag:
        cmd.extend([prompt_flag, prompt_text])
        if not _env_truthy("GEMINI_PROMPT_VIA_STDIN"):
            stdin_text = None

    timeout_s = int(os.environ.get("GEMINI_TIMEOUT_S", "300"))
    return subprocess.run(
        cmd,
        input=stdin_text,
        capture_output=True,
        text=True,
        timeout=timeout_s,
    )


def _extract_json_payload(text: str) -> dict[str, Any] | None:
    if not text:
        return None
    for candidate in _candidate_json_texts(text):
        obj = _try_parse_json(candidate)
        if obj is None:
            continue
        unwrapped = _unwrap_payload(obj)
        if isinstance(unwrapped, dict):
            return unwrapped
    return None


def _candidate_json_texts(text: str) -> Iterable[str]:
    stripped = text.strip()
    if stripped:
        yield stripped
    if stripped.startswith("```"):
        yield _strip_code_fence(stripped)
    for line in stripped.splitlines():
        line = line.strip()
        if line.startswith("{") and line.endswith("}"):
            yield line
    balanced = _extract_balanced_json(stripped)
    if balanced:
        yield balanced


def _strip_code_fence(text: str) -> str:
    lines = text.splitlines()
    if lines and lines[0].startswith("```"):
        lines = lines[1:]
    if lines and lines[-1].strip() == "```":
        lines = lines[:-1]
    return "\n".join(lines).strip()


def _try_parse_json(text: str) -> dict[str, Any] | None:
    try:
        obj = json.loads(text)
    except json.JSONDecodeError:
        return None
    if isinstance(obj, dict):
        return obj
    if isinstance(obj, str):
        try:
            obj2 = json.loads(obj)
        except json.JSONDecodeError:
            return None
        return obj2 if isinstance(obj2, dict) else None
    return None


def _extract_balanced_json(text: str) -> str | None:
    start = text.find("{")
    if start == -1:
        return None
    depth = 0
    in_str = False
    esc = False
    for idx in range(start, len(text)):
        ch = text[idx]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return text[start : idx + 1]
    return None


def _unwrap_payload(obj: dict[str, Any]) -> dict[str, Any]:
    if "turn" in obj and isinstance(obj["turn"], dict):
        return obj["turn"]
    if "result" in obj:
        inner = obj["result"]
        if isinstance(inner, dict):
            return inner
        if isinstance(inner, str):
            parsed = _try_parse_json(inner)
            if isinstance(parsed, dict):
                return parsed
    if "response" in obj and isinstance(obj["response"], dict):
        return obj["response"]
    return obj


def _validate_turn(
    obj: dict[str, Any],
    schema: dict[str, Any] | None,
    allowed_agents: list[str],
    allowed_phases: list[str],
) -> tuple[bool, str]:
    if HAS_JSONSCHEMA and schema:
        try:
            jsonschema.validate(instance=obj, schema=schema)
            return True, ""
        except Exception as exc:
            return False, str(exc)
    return _validate_turn_minimal(obj, allowed_agents, allowed_phases)


def _validate_turn_minimal(
    obj: dict[str, Any],
    allowed_agents: list[str],
    allowed_phases: list[str],
) -> tuple[bool, str]:
    if not isinstance(obj, dict):
        return False, "turn is not an object"
    expected_keys = set(REQUIRED_KEYS)
    for key in REQUIRED_KEYS:
        if key not in obj:
            return False, f"missing key: {key}"
    extra_keys = set(obj.keys()) - expected_keys
    if extra_keys:
        return False, f"unexpected keys: {', '.join(sorted(extra_keys))}"
    if allowed_agents:
        if obj.get("agent") not in allowed_agents:
            return False, "invalid agent"
        if obj.get("next_agent") not in allowed_agents:
            return False, "invalid next_agent"
    milestone_id = obj.get("milestone_id")
    if not isinstance(milestone_id, str) or not milestone_id.strip():
        return False, "milestone_id must be non-empty string"
    if allowed_phases and obj.get("phase") not in allowed_phases:
        return False, "invalid phase"
    if not isinstance(obj.get("work_completed"), bool) or not isinstance(obj.get("project_complete"), bool):
        return False, "work_completed/project_complete must be boolean"
    for key in ("summary", "next_prompt", "delegate_rationale"):
        if not isinstance(obj.get(key), str):
            return False, f"{key} must be string"
    if not isinstance(obj.get("needs_write_access"), bool):
        return False, "needs_write_access must be boolean"
    if not isinstance(obj.get("gates_passed"), list) or not all(isinstance(item, str) for item in obj.get("gates_passed", [])):
        return False, "gates_passed must be array of strings"
    stats_refs = obj.get("stats_refs")
    if not isinstance(stats_refs, list) or not stats_refs or not all(isinstance(item, str) for item in stats_refs):
        return False, "stats_refs must be non-empty array of strings"
    rp = obj.get("requirement_progress")
    if not isinstance(rp, dict):
        return False, "requirement_progress must be object"
    rp_keys = set(rp.keys())
    if rp_keys != {"covered_req_ids", "tests_added_or_modified", "commands_run"}:
        return False, "requirement_progress must only contain required keys"
    for key in ("covered_req_ids", "tests_added_or_modified", "commands_run"):
        items = rp.get(key)
        if not isinstance(items, list) or not all(isinstance(x, str) for x in items):
            return False, f"requirement_progress.{key} must be array of strings"
    artifacts = obj.get("artifacts")
    if not isinstance(artifacts, list):
        return False, "artifacts must be array"
    for idx, item in enumerate(artifacts):
        if not isinstance(item, dict):
            return False, f"artifact[{idx}] must be object"
        if set(item.keys()) != {"path", "description"}:
            return False, f"artifact[{idx}] must have path/description"
        if not isinstance(item.get("path"), str) or not isinstance(item.get("description"), str):
            return False, f"artifact[{idx}] path/description must be strings"
    return True, ""


def _build_error_payload(
    *,
    agent_id: str,
    milestone_id: str,
    allowed_agents: list[str],
    allowed_phases: list[str],
    summary: str,
    error_detail: str,
    stats_refs: list[str],
    stats_id_set: set[str] | None,
    needs_write_access: bool,
) -> dict[str, Any]:
    payload = build_error_turn(
        agent=agent_id,
        milestone_id=milestone_id,
        summary=summary,
        error_detail=error_detail,
        stats_refs=stats_refs,
        stats_id_set=stats_id_set,
        needs_write_access=needs_write_access,
    )
    payload["agent"] = _coerce_enum(payload.get("agent"), allowed_agents, agent_id or "codex")
    payload["next_agent"] = _coerce_enum(payload.get("next_agent"), allowed_agents, payload["agent"])
    payload["phase"] = _coerce_enum(payload.get("phase"), allowed_phases, "plan")
    if not payload.get("milestone_id"):
        payload["milestone_id"] = milestone_id or "M0"
    if not payload.get("stats_refs"):
        payload["stats_refs"] = ["CX-1"]
    return payload


def _coerce_enum(value: Any, allowed: list[str], fallback: str) -> str:
    if isinstance(value, str) and value in allowed:
        return value
    if allowed:
        return allowed[0]
    return fallback


def _format_process_error(result: subprocess.CompletedProcess[str]) -> str:
    lines = [f"exit_code={result.returncode}"]
    stdout = _truncate(result.stdout or "")
    stderr = _truncate(result.stderr or "")
    if stderr:
        lines.append(f"stderr={stderr}")
    if stdout:
        lines.append(f"stdout={stdout}")
    return "\n".join(lines)


def _format_parse_error(raw_text: str) -> str:
    snippet = _truncate(raw_text)
    if snippet:
        return f"raw_output={snippet}"
    return "raw_output_empty"


def _format_validation_error(err: str, raw_text: str) -> str:
    snippet = _truncate(raw_text)
    if snippet:
        return f"validation_error={err}\nraw_output={snippet}"
    return f"validation_error={err}"


def _truncate(text: str, limit: int = 800) -> str:
    if len(text) <= limit:
        return text
    return f"{text[:limit]}...(truncated)"


def _write_json(path: Path, payload: dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2, ensure_ascii=True), encoding="utf-8")


if __name__ == "__main__":
    raise SystemExit(main())

================================================================================
 FILE: loop.py
================================================================================

#!/usr/bin/env python3
"""Two-agent orchestration loop (Codex + Claude).

This loop:
- Embeds repo context + verify report + recent summaries into a single prompt.
- Calls one of two agent wrappers (bridge/agents/*.sh) and streams their output.
- Validates the agent's response against the expected turn schema (bridge/turn.schema.json)
  and additional local constraints (agent id, milestone id, stats refs).

Notes:
- This runner is intentionally opinionated: unless project_complete=true, it enforces a
  Codex <-> Claude alternation when both are enabled.
- Verbose streaming output is preserved in live mode: agent stdout/stderr are forwarded
  to your terminal while also being captured to runs/<run_id>/call_XXXX/raw.txt.
"""

from __future__ import annotations

import argparse
import ast
import collections
import concurrent.futures
import contextlib
import dataclasses
import datetime as dt
import hashlib
import json
import os
import re
import shutil
import signal
import subprocess
import sys
import textwrap
import threading
import time
import traceback
from pathlib import Path
from typing import Any

# When run as `python bridge/loop.py`, Python sets sys.path[0] to `bridge/`.
# We want to import sibling packages (e.g. `tools`) from the repo root.
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Atomic I/O for robust file operations
from bridge.atomic_io import atomic_write_json, validate_json_file

# Design document parsing - modular adapter layer
from bridge.design_doc import (
    ContractMode,
    parse_design_doc,
)

# Turn normalization - extracted to submodule for tooling readability
from bridge.loop_pkg.turn_normalizer import (
    normalize_agent_output,
)
from bridge.loop_pkg.turn_normalizer import (
    validate_turn_lenient as _validate_turn_lenient,
)

# Patch integration for commit-free worker operation
from bridge.patch_integration import PatchIntegrator, collect_patch_artifact, save_patch_artifact
from bridge.reports import write_task_report
from bridge.scheduler import BackfillGenerator, FillerTask, LaneConfig, TwoLaneScheduler
from bridge.smoke_route import resolve_smoke_route
from bridge.streaming import run_cmd_with_streaming
from bridge.verify_repair import (
    create_repair_callback,
    run_verify_repair_loop,
    write_repair_report,
)

AGENTS: tuple[str, ...] = ("codex", "claude")
PLANNER_PROFILES: tuple[str, ...] = ("balanced", "throughput", "engineering")
DEFAULT_PLANNER_PROFILE = "balanced"


# -----------------------------
# Config + state
# -----------------------------


@dataclasses.dataclass(frozen=True)
class ParallelSettings:
    max_workers_default: int = 8
    cpu_intensive_threshold_pct: float = 40.0
    mem_intensive_threshold_pct: float = 40.0
    sample_interval_s: float = 1.0
    consecutive_samples: int = 3
    kill_grace_s: float = 8.0
    terminal_max_bytes_per_worker: int = 40000
    terminal_max_line_length: int = 600
    disable_gpu_by_default: bool = True


@dataclasses.dataclass(frozen=True)
class AgentCapabilities:
    """Capabilities for an agent - defines what tools/features are available."""

    supports_tools: bool = True
    supports_fs_read: bool = True
    supports_fs_write: bool = True
    supports_bash: bool = True
    supports_write_access: bool = True


@dataclasses.dataclass(frozen=True)
class RunConfig:
    max_calls_per_agent: int
    quota_retry_attempts: int
    max_total_calls: int
    max_json_correction_attempts: int
    fallback_order: list[str]
    enable_agents: list[str]
    smoke_route: tuple[str, ...]

    agent_scripts: dict[str, str]
    agent_models: dict[str, str]
    quota_error_patterns: dict[str, list[str]]
    supports_write_access: dict[str, bool]
    agent_capabilities: dict[str, AgentCapabilities]
    parallel: ParallelSettings


@dataclasses.dataclass
class RunState:
    run_id: str
    project_root: Path
    runs_dir: Path
    schema_path: Path
    system_prompt_path: Path
    design_doc_path: Path
    smoke_route: tuple[str, ...] = tuple()
    readonly: bool = False

    total_calls: int = 0
    call_counts: dict[str, int] = dataclasses.field(default_factory=lambda: {a: 0 for a in AGENTS})
    quota_failures: dict[str, int] = dataclasses.field(default_factory=lambda: {a: 0 for a in AGENTS})
    disabled_by_quota: dict[str, bool] = dataclasses.field(default_factory=lambda: {a: False for a in AGENTS})
    history: list[dict[str, Any]] = dataclasses.field(default_factory=list)

    # Dynamic write access policy (set by previous turn)
    grant_write_access: bool = False


# -----------------------------
# AgentPolicy: Centralized agent selection enforcement
# -----------------------------


class AgentPolicyViolation(Exception):
    """Raised when code attempts to use an agent that violates the policy."""

    pass


@dataclasses.dataclass
class AgentPolicy:
    """Centralized agent selection policy.

    When forced_agent is set (via --only-codex or --only-claude), ALL agent
    selections must go through this policy and will be overridden to use
    only the forced agent.
    """

    forced_agent: str | None = None  # Set by --only-* flags
    allowed_agents: tuple[str, ...] = AGENTS
    runs_dir: Path | None = None  # For writing violation artifacts

    def enforce(self, requested_agent: str, context: str = "") -> str:
        """Enforce the agent policy, returning the agent to use.

        Args:
            requested_agent: The agent that was requested
            context: Description of where this request originated (for error messages)

        Returns:
            The agent to actually use (forced_agent if set, otherwise requested)

        Raises:
            AgentPolicyViolation: If forced mode is active and code tries to use wrong agent
        """
        if self.forced_agent:
            if requested_agent != self.forced_agent and requested_agent in AGENTS:
                # Log the override
                print(f"[AgentPolicy] OVERRIDE: {requested_agent} -> {self.forced_agent} ({context})")
            return self.forced_agent

        # No forced agent - verify requested is allowed
        if requested_agent not in self.allowed_agents:
            if self.allowed_agents:
                return self.allowed_agents[0]
            return AGENTS[0]

        return requested_agent

    def enforce_strict(self, requested_agent: str, context: str = "") -> str:
        """Strict enforcement - raises exception if wrong agent is requested.

        Use this for code paths that should NEVER attempt to use the wrong agent
        (e.g., fallback logic that might try to switch agents).
        """
        if self.forced_agent and requested_agent != self.forced_agent:
            msg = (
                f"AGENT POLICY VIOLATION: Attempted to use '{requested_agent}' "
                f"when --only-{self.forced_agent} is active. Context: {context}"
            )
            self._write_violation_artifact(msg, requested_agent, context)
            raise AgentPolicyViolation(msg)
        return self.enforce(requested_agent, context)

    def _write_violation_artifact(self, msg: str, requested: str, context: str) -> None:
        """Write an artifact explaining the policy violation."""
        if not self.runs_dir:
            return
        artifact_path = self.runs_dir / "agent_policy_violation.txt"
        content = f"""AGENT POLICY VIOLATION
======================

Timestamp: {dt.datetime.utcnow().isoformat()}Z
Forced Agent: {self.forced_agent}
Requested Agent: {requested}
Context: {context}

Message:
{msg}

This file was created because code attempted to invoke an agent that
violates the --only-{self.forced_agent} flag. This indicates a bug in
the orchestrator's agent selection logic.
"""
        try:
            artifact_path.write_text(content, encoding="utf-8")
            print(f"[AgentPolicy] Violation artifact written to: {artifact_path}")
        except Exception as e:
            print(f"[AgentPolicy] Failed to write violation artifact: {e}")

    def is_forced_mode(self) -> bool:
        """Return True if a forced agent mode is active."""
        return self.forced_agent is not None

    def get_prompt_header(self) -> str:
        """Get a header to inject into prompts when in forced mode.

        This tells the agent it's the only one and must implement, not just review.
        """
        if not self.forced_agent:
            return ""

        return f"""## AGENT POLICY OVERRIDE

**IMPORTANT**: You are running in `--only-{self.forced_agent}` mode.

- You are the ONLY agent allowed in this session.
- You MUST implement all changes yourself. Do NOT suggest handing off to another agent.
- You MUST verify your own changes. Do NOT assume another agent will review.
- Set `next_agent` to `"{self.forced_agent}"` in your response (it will be enforced anyway).
- Focus on both implementation AND verification - you are responsible for the full cycle.

"""


# Global policy instance (set during main() based on CLI flags)
_agent_policy: AgentPolicy | None = None


def get_agent_policy() -> AgentPolicy:
    """Get the global agent policy. Returns a default policy if not set."""
    global _agent_policy
    if _agent_policy is None:
        _agent_policy = AgentPolicy()
    return _agent_policy


def set_agent_policy(policy: AgentPolicy) -> None:
    """Set the global agent policy."""
    global _agent_policy
    _agent_policy = policy


# -----------------------------
# Small helpers
# -----------------------------


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def _read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def _write_text(path: Path, text: str) -> None:
    path.write_text(text, encoding="utf-8")


def _load_json(path: Path) -> Any:
    return json.loads(_read_text(path))


def _normalize_planner_profile(profile: str | None) -> str:
    if not profile:
        return DEFAULT_PLANNER_PROFILE
    normalized = profile.strip().lower()
    if normalized == "default":
        return DEFAULT_PLANNER_PROFILE
    if normalized in PLANNER_PROFILES:
        return normalized
    return DEFAULT_PLANNER_PROFILE


def _load_system_prompt(
    *,
    project_root: Path,
    system_prompt_path: Path,
    planner_profile: str,
) -> tuple[str, Path]:
    """Load the effective system prompt for the selected planner profile.

    Returns a tuple of (prompt_text, effective_path). In engineering mode, this
    forces the engineering system prompt and appends AGENTS.md content.
    """
    effective_path = system_prompt_path
    if planner_profile == "engineering":
        effective_path = project_root / "bridge" / "prompts" / "system_engineering.md"

    prompt_text = _read_text(effective_path) if effective_path.exists() else ""

    if planner_profile == "engineering":
        agents_path = project_root / "AGENTS.md"
        if agents_path.exists():
            agents_text = _read_text(agents_path).strip()
            if agents_text:
                prompt_text = prompt_text.strip() + "\n\n---\n\n# Repo Protocol (AGENTS.md)\n" + agents_text

    return prompt_text, effective_path


def _truncate(text: str, limit: int) -> str:
    if limit <= 0:
        return ""
    if len(text) <= limit:
        return text
    # Keep both ends: helpful for logs that end with stack traces.
    head = max(0, int(limit * 0.7))
    tail = max(0, limit - head - 64)
    return text[:head] + "\n\n[...TRUNCATED... try opening the raw log file for full output ...]\n\n" + text[-tail:]


def _extract_stats_ids(stats_md_text: str) -> list[str]:
    """Extract stable stats identifiers from STATS.md.

    IDs are intentionally simple: CX-* and CL-* only (two-agent mode).
    """

    ids = sorted(set(re.findall(r"\b(?:CX|CL)-\d+\b", stats_md_text)))
    return ids


def _parse_milestone_id(design_doc_text: str) -> str:
    m = re.search(r"\*\*Milestone:\*\*\s*(M\d+)\b", design_doc_text)
    return m.group(1) if m else "M0"


def _parse_all_milestones(design_doc_text: str) -> list[str]:
    """Parse all milestone IDs from a design document that may contain multiple milestone docs.

    Looks for patterns like:
    - **Milestone:** M1
    - # M2 Design Document
    - # M3 Design Document
    """
    milestones = set()

    # Pattern 1: **Milestone:** M1 format
    for m in re.finditer(r"\*\*Milestone:\*\*\s*(M\d+)\b", design_doc_text):
        milestones.add(m.group(1))

    # Pattern 2: # M2 Design Document format
    for m in re.finditer(r"^#\s+(M\d+)\s+Design\s+Document", design_doc_text, re.MULTILINE):
        milestones.add(m.group(1))

    if not milestones:
        return ["M0"]

    # Sort by milestone number
    return sorted(milestones, key=lambda x: int(x[1:]))


def _parse_smoke_route_arg(value: str) -> list[str]:
    route = [tok.strip() for tok in value.split(",") if tok.strip()]
    if not route:
        raise argparse.ArgumentTypeError("smoke-route must include at least one agent name")
    unknown = [tok for tok in route if tok not in AGENTS]
    if unknown:
        allowed = ", ".join(AGENTS)
        raise argparse.ArgumentTypeError(f"smoke-route contains unknown agent(s): {', '.join(unknown)} (allowed: {allowed})")
    return route


def _extract_milestone_from_task_id(task_id: str, fallback: str = "M0") -> str:
    """Extract milestone prefix from task ID (e.g., 'M2-SIM-SCHEMA' -> 'M2')."""
    m = re.match(r"^(M\d+)-", task_id)
    return m.group(1) if m else fallback


def _run_cmd(
    cmd: list[str],
    cwd: Path,
    env: dict[str, str],
    *,
    stream: bool = False,
) -> tuple[int, str, str]:
    """Run a subprocess.

    When stream=True, stdout/stderr are forwarded live while also being captured.
    """

    if not stream:
        proc = subprocess.run(cmd, cwd=str(cwd), env=env, text=True, capture_output=True)
        return proc.returncode, proc.stdout or "", proc.stderr or ""

    proc = subprocess.Popen(
        cmd,
        cwd=str(cwd),
        env=env,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        bufsize=1,
    )

    out_chunks: list[str] = []
    err_chunks: list[str] = []

    def _pump(src, sink, chunks: list[str]) -> None:
        try:
            assert src is not None
            for line in iter(src.readline, ""):
                sink.write(line)
                sink.flush()
                chunks.append(line)
        finally:
            with contextlib.suppress(Exception):
                src.close()

    t_out = threading.Thread(target=_pump, args=(proc.stdout, sys.stdout, out_chunks), daemon=True)
    t_err = threading.Thread(target=_pump, args=(proc.stderr, sys.stderr, err_chunks), daemon=True)
    t_out.start()
    t_err.start()

    rc = proc.wait()
    t_out.join(timeout=1)
    t_err.join(timeout=1)
    return rc, "".join(out_chunks), "".join(err_chunks)


# -----------------------------
# Resource monitoring (parallel runner)
# -----------------------------


def _total_ram_bytes() -> int | None:
    """Best-effort total physical RAM bytes (no external deps)."""
    # POSIX sysconf (Linux + many Unixes)
    try:
        pages = os.sysconf("SC_PHYS_PAGES")
        page_size = os.sysconf("SC_PAGE_SIZE")
        if pages and page_size:
            return int(pages) * int(page_size)
    except Exception:
        pass

    # Linux /proc fallback
    try:
        meminfo = Path("/proc/meminfo")
        if meminfo.exists():
            for line in meminfo.read_text(encoding="utf-8").splitlines():
                if line.startswith("MemTotal:"):
                    parts = line.split()
                    # kB
                    kb = int(parts[1])
                    return kb * 1024
    except Exception:
        pass

    return None


def _parse_ps_time_to_seconds(s: str) -> float:
    """Parse ps TIME / CPUTIME strings into seconds.

    Accepts: [[dd-]hh:]mm:ss, mm:ss, or ss.
    """
    s = s.strip()
    if not s:
        return 0.0

    days = 0
    if "-" in s:
        d, s = s.split("-", 1)
        try:
            days = int(d)
        except ValueError:
            days = 0

    parts = s.split(":")
    try:
        nums = [int(p) for p in parts]
    except ValueError:
        return 0.0

    h = 0
    m = 0
    sec = 0
    if len(nums) == 3:
        h, m, sec = nums
    elif len(nums) == 2:
        m, sec = nums
    elif len(nums) == 1:
        sec = nums[0]
    else:
        # Unexpected format
        return 0.0

    return float(days * 86400 + h * 3600 + m * 60 + sec)


def _ps_list_pids_in_pgid(pgid: int) -> list[int]:
    """Return pids in a process group via ps (portable-ish)."""
    candidates = [
        ["ps", "-o", "pid=", "-g", str(pgid)],
        ["ps", "-o", "pid=", "--pgid", str(pgid)],
    ]
    for cmd in candidates:
        try:
            out = subprocess.check_output(cmd, text=True, stderr=subprocess.DEVNULL)
            pids: list[int] = []
            for tok in out.split():
                tok = tok.strip()
                if tok.isdigit():
                    pids.append(int(tok))
            if pids:
                return sorted(set(pids))
        except Exception:
            continue
    return []


def _ps_sample_pids(pids: list[int]) -> tuple[float, int]:
    """Return (total_cpu_time_seconds, total_rss_bytes) for pids via ps."""
    if not pids:
        return 0.0, 0

    pid_arg = ",".join(str(p) for p in pids)
    candidates = [
        ["ps", "-o", "pid=", "-o", "time=", "-o", "rss=", "-p", pid_arg],
    ]
    for cmd in candidates:
        try:
            out = subprocess.check_output(cmd, text=True, stderr=subprocess.DEVNULL)
            total_cpu = 0.0
            total_rss_kb = 0
            for line in out.splitlines():
                parts = line.strip().split()
                if len(parts) < 3:
                    continue
                # pid = parts[0]
                cpu_s = _parse_ps_time_to_seconds(parts[1])
                try:
                    rss_kb = int(parts[2])
                except ValueError:
                    rss_kb = 0
                total_cpu += cpu_s
                total_rss_kb += rss_kb
            return total_cpu, total_rss_kb * 1024
        except Exception:
            continue

    return 0.0, 0


@dataclasses.dataclass
class MonitoredResult:
    returncode: int
    killed_for_resources: bool
    kill_reason: str | None
    max_cpu_pct_total: float
    max_mem_pct_total: float
    tail_stdout: str
    tail_stderr: str


def _terminate_process_group(proc: subprocess.Popen, *, grace_s: float) -> None:
    """Try hard to stop proc + its process group without leaving orphans."""
    if proc.poll() is not None:
        return

    # First: SIGINT
    try:
        os.killpg(proc.pid, signal.SIGINT)
    except Exception:
        with contextlib.suppress(Exception):
            proc.send_signal(signal.SIGINT)

    t0 = time.monotonic()
    while time.monotonic() - t0 < max(0.5, grace_s * 0.5):
        if proc.poll() is not None:
            return
        time.sleep(0.2)

    # Second: SIGTERM
    try:
        os.killpg(proc.pid, signal.SIGTERM)
    except Exception:
        with contextlib.suppress(Exception):
            proc.terminate()

    t1 = time.monotonic()
    while time.monotonic() - t1 < max(0.5, grace_s * 0.4):
        if proc.poll() is not None:
            return
        time.sleep(0.2)

    # Last resort: SIGKILL
    try:
        os.killpg(proc.pid, signal.SIGKILL)
    except Exception:
        with contextlib.suppress(Exception):
            proc.kill()


def _run_cmd_monitored(
    cmd: list[str],
    cwd: Path,
    env: dict[str, str],
    *,
    prefix: str,
    raw_log_path: Path,
    stream_to_terminal: bool,
    terminal_max_bytes: int,
    terminal_max_line_length: int,
    cpu_threshold_pct_total: float,
    mem_threshold_pct_total: float,
    sample_interval_s: float,
    consecutive_samples: int,
    kill_grace_s: float,
    allow_resource_intensive: bool,
) -> MonitoredResult:
    """Run a subprocess with live streaming + best-effort resource monitoring.

    - Always writes full combined stdout+stderr to raw_log_path.
    - Optionally forwards output to the terminal with per-process truncation.
    - If allow_resource_intensive is False, terminates the process group when its
      CPU or memory exceeds the configured thresholds for consecutive_samples.

    Returns a MonitoredResult containing a small tail buffer for debugging.
    """

    raw_log_path.parent.mkdir(parents=True, exist_ok=True)

    # Tail buffers (keep last ~64KB total)
    tail_limit = 64 * 1024
    tail_out = collections.deque()  # type: ignore[var-annotated]
    tail_err = collections.deque()  # type: ignore[var-annotated]
    tail_out_size = 0
    tail_err_size = 0

    def _append_tail(dq: collections.deque, current_size: int, chunk: str) -> int:
        nonlocal tail_limit
        dq.append(chunk)
        current_size += len(chunk)
        while current_size > tail_limit and dq:
            removed = dq.popleft()
            current_size -= len(removed)
        return current_size

    printed_bytes = 0
    printed_truncated = False

    total_ram = _total_ram_bytes()
    num_cores = os.cpu_count() or 1

    killed_for_resources = False
    kill_reason: str | None = None
    max_cpu_pct = 0.0
    max_mem_pct = 0.0

    with raw_log_path.open("w", encoding="utf-8", errors="replace") as raw_f:
        proc = subprocess.Popen(
            cmd,
            cwd=str(cwd),
            env=env,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            bufsize=1,
            start_new_session=True,
        )

        # Resource monitor state
        prev_cpu_time: float | None = None
        prev_t = time.monotonic()
        consecutive_hits = 0

        def _emit(line: str, *, is_err: bool) -> None:
            nonlocal printed_bytes, printed_truncated, tail_out_size, tail_err_size
            # Raw log: always write full line
            raw_f.write(line)
            raw_f.flush()

            # Tail buffers
            if is_err:
                tail_err_size = _append_tail(tail_err, tail_err_size, line)
            else:
                tail_out_size = _append_tail(tail_out, tail_out_size, line)

            # Terminal streaming (possibly truncated)
            if not stream_to_terminal:
                return
            if terminal_max_bytes > 0 and printed_bytes >= terminal_max_bytes:
                if not printed_truncated:
                    msg = f"{prefix} [terminal output truncated; see {raw_log_path}]\n"
                    sys.stdout.write(msg)
                    sys.stdout.flush()
                    printed_truncated = True
                return

            # Line truncation
            out_line = line
            if terminal_max_line_length > 0 and len(out_line) > terminal_max_line_length:
                out_line = out_line[:terminal_max_line_length] + "...\n"

            # Prefix each physical line
            # Preserve newlines by splitting; cheaper than regex.
            for seg in out_line.splitlines(True):
                sys.stdout.write(f"{prefix} {seg}")
            sys.stdout.flush()
            printed_bytes += len(out_line)

        def _pump(src, *, is_err: bool) -> None:
            try:
                assert src is not None
                for line in iter(src.readline, ""):
                    _emit(line, is_err=is_err)
            finally:
                with contextlib.suppress(Exception):
                    src.close()

        t_out = threading.Thread(target=_pump, args=(proc.stdout,), kwargs={"is_err": False}, daemon=True)
        t_err = threading.Thread(target=_pump, args=(proc.stderr,), kwargs={"is_err": True}, daemon=True)
        t_out.start()
        t_err.start()

        # Monitoring loop
        while proc.poll() is None:
            time.sleep(max(0.25, sample_interval_s))

            if allow_resource_intensive:
                continue

            try:
                pgid = proc.pid
                pids = _ps_list_pids_in_pgid(pgid) or [pgid]
                cpu_time_s, rss_bytes = _ps_sample_pids(pids)
                now = time.monotonic()
                dt_s = max(1e-3, now - prev_t)

                if prev_cpu_time is None:
                    prev_cpu_time = cpu_time_s
                    prev_t = now
                    continue

                delta_cpu = max(0.0, cpu_time_s - prev_cpu_time)
                prev_cpu_time = cpu_time_s
                prev_t = now

                # CPU as pct of total machine
                cpu_pct_total = (delta_cpu / dt_s) * 100.0 / float(num_cores)
                max_cpu_pct = max(max_cpu_pct, cpu_pct_total)

                mem_pct_total = 0.0
                if total_ram and total_ram > 0:
                    mem_pct_total = (rss_bytes / float(total_ram)) * 100.0
                    max_mem_pct = max(max_mem_pct, mem_pct_total)

                hit = False
                if cpu_threshold_pct_total > 0 and cpu_pct_total > cpu_threshold_pct_total:
                    hit = True
                    kill_reason = f"cpu>{cpu_threshold_pct_total:.1f}% (saw {cpu_pct_total:.1f}%)"
                if mem_threshold_pct_total > 0 and total_ram and mem_pct_total > mem_threshold_pct_total:
                    hit = True
                    kill_reason = f"mem>{mem_threshold_pct_total:.1f}% (saw {mem_pct_total:.1f}%)"

                if hit:
                    consecutive_hits += 1
                else:
                    consecutive_hits = 0

                if consecutive_hits >= max(1, consecutive_samples):
                    killed_for_resources = True
                    _emit(f"{prefix} RESOURCE_INTENSIVE: {kill_reason} -> terminating process group\n", is_err=True)
                    _terminate_process_group(proc, grace_s=kill_grace_s)
                    break

            except Exception:
                # Monitoring is best-effort; don't crash the run.
                continue

        rc = proc.wait()
        t_out.join(timeout=1)
        t_err.join(timeout=1)

    return MonitoredResult(
        returncode=rc,
        killed_for_resources=killed_for_resources,
        kill_reason=kill_reason,
        max_cpu_pct_total=max_cpu_pct,
        max_mem_pct_total=max_mem_pct,
        tail_stdout="".join(tail_out),
        tail_stderr="".join(tail_err),
    )


def _git_snapshot(project_root: Path) -> str:
    """A compact repo snapshot embedded into the prompt."""

    env = os.environ.copy()

    def safe(cmd: list[str]) -> str:
        rc, out, err = _run_cmd(cmd, cwd=project_root, env=env, stream=False)
        if rc != 0:
            return f"$ {' '.join(cmd)}\n(rc={rc})\n{(out + err).strip()}"
        return out.strip()

    if not (project_root / ".git").exists():
        return "(no git repo detected)"

    head = safe(["git", "rev-parse", "HEAD"])
    branch = safe(["git", "rev-parse", "--abbrev-ref", "HEAD"])
    status = safe(["git", "status", "--porcelain=v1"])
    diff_stat = safe(["git", "diff", "--stat"])
    log = safe(["git", "log", "-5", "--oneline", "--decorate"])

    parts = [
        f"branch: {branch}\nHEAD: {head}",
        "\n# git status --porcelain\n" + (status or "(clean)"),
        "\n# git diff --stat\n" + (diff_stat or "(no diff)"),
        "\n# git log -5 --oneline --decorate\n" + (log or "(no commits?)"),
    ]
    return "\n".join(parts).strip()


# -----------------------------
# Config loading
# -----------------------------


def load_config(config_path: Path) -> RunConfig:
    data = _load_json(config_path)
    limits = data.get("limits", {})

    max_calls_per_agent = int(limits.get("max_calls_per_agent", 15))
    quota_retry_attempts = int(limits.get("quota_retry_attempts", 3))
    max_total_calls = int(limits.get("max_total_calls", 150))
    max_json_correction_attempts = int(limits.get("max_json_correction_attempts", 3))

    fallback_order = list(data.get("fallback_order", ["codex", "claude"]))
    enable_agents = list(data.get("enable_agents", ["codex", "claude"]))

    agents_cfg = data.get("agents", {})

    agent_scripts: dict[str, str] = {}
    agent_models: dict[str, str] = {}
    quota_pats: dict[str, list[str]] = {}
    supports_write: dict[str, bool] = {}
    agent_capabilities: dict[str, AgentCapabilities] = {}

    for a in AGENTS:
        if a not in agents_cfg:
            raise ValueError(f"Missing agent in config: {a}")
        agent_scripts[a] = str(agents_cfg[a].get("script", ""))
        agent_models[a] = str(agents_cfg[a].get("model", "(default)"))
        quota_pats[a] = list(agents_cfg[a].get("quota_error_patterns", []))
        supports_write[a] = bool(agents_cfg[a].get("supports_write_access", False))
        # Parse agent capabilities
        agent_capabilities[a] = AgentCapabilities(
            supports_tools=bool(agents_cfg[a].get("supports_tools", True)),
            supports_fs_read=bool(agents_cfg[a].get("supports_fs_read", True)),
            supports_fs_write=bool(agents_cfg[a].get("supports_fs_write", True)),
            supports_bash=bool(agents_cfg[a].get("supports_bash", True)),
            supports_write_access=supports_write[a],
        )

    parallel_cfg = data.get("parallel", {}) or {}
    parallel = ParallelSettings(
        max_workers_default=int(parallel_cfg.get("max_workers_default", 8)),
        cpu_intensive_threshold_pct=float(parallel_cfg.get("cpu_intensive_threshold_pct", 40.0)),
        mem_intensive_threshold_pct=float(parallel_cfg.get("mem_intensive_threshold_pct", 40.0)),
        sample_interval_s=float(parallel_cfg.get("sample_interval_s", 1.0)),
        consecutive_samples=int(parallel_cfg.get("consecutive_samples", 3)),
        kill_grace_s=float(parallel_cfg.get("kill_grace_s", 8.0)),
        terminal_max_bytes_per_worker=int(parallel_cfg.get("terminal_max_bytes_per_worker", 40000)),
        terminal_max_line_length=int(parallel_cfg.get("terminal_max_line_length", 600)),
        disable_gpu_by_default=bool(parallel_cfg.get("disable_gpu_by_default", True)),
    )

    return RunConfig(
        max_calls_per_agent=max_calls_per_agent,
        quota_retry_attempts=quota_retry_attempts,
        max_total_calls=max_total_calls,
        max_json_correction_attempts=max_json_correction_attempts,
        fallback_order=fallback_order,
        enable_agents=enable_agents,
        smoke_route=tuple(),
        agent_scripts=agent_scripts,
        agent_models=agent_models,
        quota_error_patterns=quota_pats,
        supports_write_access=supports_write,
        agent_capabilities=agent_capabilities,
        parallel=parallel,
    )


# -----------------------------
# Prompt building
# -----------------------------


def build_prompt(
    *,
    agent: str,
    system_prompt: str,
    design_doc_text: str,
    milestone_id: str,
    repo_info: str,
    verify_report_text: str,
    history: list[dict[str, Any]],
    next_prompt: str,
    call_counts: dict[str, int],
    disabled_by_quota: dict[str, bool],
    stats_ids: list[str],
    readonly: bool,
    planner_profile: str,
) -> str:
    last_summaries = "\n".join([f"- ({h['agent']}) {h['summary']}" for h in history[-4:]])

    state_blob = json.dumps(
        {
            "agent": agent,
            "milestone_id": milestone_id,
            "planner_profile": planner_profile,
            "call_counts": call_counts,
            "disabled_by_quota": disabled_by_quota,
            "known_stats_ids": stats_ids,
            "readonly": readonly,
        },
        indent=2,
        sort_keys=True,
    )

    # Get agent policy header if in forced mode
    policy_header = get_agent_policy().get_prompt_header()

    parts: list[str] = [
        system_prompt.strip(),
    ]

    # Inject policy header right after system prompt if in forced mode
    if policy_header:
        parts.append("\n\n")
        parts.append(policy_header.strip())

    parts.extend(
        [
            "\n\n---\n\n# Orchestrator State (read-only)\n",
            state_blob,
            "\n\n---\n\n# Milestone Spec (DESIGN_DOCUMENT.md)\n",
            _truncate(design_doc_text.strip(), 20000),
            "\n\n---\n\n# Verification Report (tools.verify)\n",
            _truncate(verify_report_text.strip(), 20000),
            "\n\n---\n\n# Repo Snapshot\n",
            _truncate(repo_info.strip(), 20000),
            "\n\n---\n\n",
        ]
    )

    if last_summaries.strip():
        parts += ["# Recent Turn Summaries\n", _truncate(last_summaries.strip(), 5000), "\n\n---\n\n"]

    if planner_profile == "engineering":
        task_packet = _format_task_packet_for_sequential(next_prompt)
        parts += ["# Task Packet\n", task_packet, "\n"]
    else:
        parts += ["# Your Work Item\n", next_prompt.strip() or "(Decide the next best step.)", "\n"]
    return "".join(parts)


# -----------------------------
# Turn parsing/validation
# -----------------------------


def _try_parse_json(text: str) -> Any:
    stripped = text.strip()
    if not stripped:
        raise ValueError("empty output")

    # Fast path.
    try:
        return json.loads(stripped)
    except json.JSONDecodeError:
        pass

    # Extract first balanced JSON object.
    start = stripped.find("{")
    if start == -1:
        raise ValueError("no '{' found in output")

    depth = 0
    in_str = False
    esc = False

    for i in range(start, len(stripped)):
        ch = stripped[i]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
            continue
        if ch == '"':
            in_str = True
            continue
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                candidate = stripped[start : i + 1]
                return json.loads(candidate)

    raise ValueError("unbalanced braces in output")


def _validate_turn(
    obj: Any, *, expected_agent: str, expected_milestone_id: str | None = None, stats_id_set: set[str]
) -> tuple[bool, str]:
    if not isinstance(obj, dict):
        return False, "turn is not an object"

    required_keys = [
        "agent",
        "milestone_id",
        "phase",
        "work_completed",
        "project_complete",
        "summary",
        "gates_passed",
        "requirement_progress",
        "next_agent",
        "next_prompt",
        "delegate_rationale",
        "stats_refs",
        "needs_write_access",
        "artifacts",
    ]

    for k in required_keys:
        if k not in obj:
            return False, f"missing key: {k}"

    if obj.get("agent") != expected_agent:
        return False, f"agent mismatch: expected {expected_agent}, got {obj.get('agent')}"

    # Validate milestone_id if expected_milestone_id is provided
    if expected_milestone_id is not None and str(obj.get("milestone_id")) != expected_milestone_id:
        return False, f"milestone_id mismatch: expected {expected_milestone_id}, got {obj.get('milestone_id')}"

    if obj["agent"] not in AGENTS or obj["next_agent"] not in AGENTS:
        return False, "invalid agent id in agent/next_agent"

    if obj.get("phase") not in ("plan", "implement", "verify", "finalize"):
        return False, "invalid phase"

    if not isinstance(obj["work_completed"], bool) or not isinstance(obj["project_complete"], bool):
        return False, "work_completed/project_complete must be boolean"

    for k in ("summary", "next_prompt", "delegate_rationale"):
        if not isinstance(obj.get(k), str):
            return False, f"{k} must be a string"

    if not isinstance(obj["needs_write_access"], bool):
        return False, "needs_write_access must be boolean"

    if not isinstance(obj["gates_passed"], list) or not all(isinstance(x, str) for x in obj["gates_passed"]):
        return False, "gates_passed must be array of strings"

    if not isinstance(obj["stats_refs"], list) or not all(isinstance(x, str) for x in obj["stats_refs"]):
        return False, "stats_refs must be array of strings"
    if not obj["stats_refs"]:
        return False, "stats_refs is empty"
    unknown = [x for x in obj["stats_refs"] if x not in stats_id_set]
    if unknown:
        return False, f"unknown stats_refs: {unknown}"

    rp = obj.get("requirement_progress")
    if not isinstance(rp, dict):
        return False, "requirement_progress must be object"
    for k in ("covered_req_ids", "tests_added_or_modified", "commands_run"):
        if k not in rp:
            return False, f"requirement_progress missing key: {k}"
        if not isinstance(rp[k], list) or not all(isinstance(x, str) for x in rp[k]):
            return False, f"requirement_progress.{k} must be array of strings"

    if not isinstance(obj["artifacts"], list):
        return False, "artifacts must be an array"
    for i, a in enumerate(obj["artifacts"]):
        if not isinstance(a, dict):
            return False, f"artifact[{i}] must be object"
        if set(a.keys()) != {"path", "description"}:
            return False, f"artifact[{i}] must have exactly keys: path, description"
        if not isinstance(a.get("path"), str) or not isinstance(a.get("description"), str):
            return False, f"artifact[{i}] path/description must be strings"

    extra = set(obj.keys()) - set(required_keys)
    if extra:
        return False, f"unexpected keys present: {sorted(extra)}"

    return True, "ok"


# -----------------------------
# Contract hardening for agent outputs
# -----------------------------

# Patterns that indicate non-compliant agent output requiring strict reprompt
# CRITICAL: These patterns detect when agents incorrectly claim they can't use tools
NONCOMPLIANT_OUTPUT_PATTERNS: list[tuple[str, str]] = [
    # Tool availability claims - these are CRITICAL violations in execution mode
    (r"tools?\s+(are\s+)?(disabled|unavailable|not\s+available)", "tools_disabled_claim"),
    (r"cannot\s+(use|access|execute)\s+tools?", "tools_access_claim"),
    (r"don'?t\s+have\s+(access|permission)\s+to\s+tools?", "tools_permission_claim"),
    (r"tool\s+(execution|use|usage)\s+(is\s+)?(blocked|disabled)", "tools_execution_claim"),
    (r"i\s+(am\s+)?(unable|not\s+able)\s+to\s+(use|access|execute)\s+tools?", "tools_unable_claim"),
    (r"no\s+(access|permission)\s+to\s+(read|write|edit|execute)", "no_access_claim"),
    (r"(cannot|can't|unable)\s+(read|write|edit|modify)\s+(files?|code)", "file_access_claim"),
    (r"(cannot|can't|unable)\s+(run|execute)\s+(commands?|bash|shell)", "command_access_claim"),
    # Formatting violations
    (r"```(json|python|bash|)", "markdown_code_block"),
    (r"^(Here'?s?|I'?ll|Let me|Sure,)", "prose_prefix"),
    (r"^\s*#(?!\s*\{)", "comment_line_as_output"),  # Exclude lines that might be starting JSON with #
]

# Patterns that are CRITICAL violations requiring immediate retry with tools-enabled prompt
CRITICAL_TOOL_VIOLATIONS = {
    "tools_disabled_claim",
    "tools_access_claim",
    "tools_permission_claim",
    "tools_execution_claim",
    "tools_unable_claim",
    "no_access_claim",
    "file_access_claim",
    "command_access_claim",
}

# Hot files: critical integration points that should not be concurrently edited
# by multiple workers. When multiple tasks list these files in touched_paths,
# a shared lock is automatically injected to prevent merge conflicts.
HOT_FILES: tuple[str, ...] = (
    "**/api.py",
    "**/board_writer.py",
    "**/cli_main.py",
    "**/pipeline.py",
    "**/resolve.py",
    "**/manifest.py",
    "**/coupon_runner.py",
)


def _inject_hot_file_locks(tasks: list) -> list:
    """Inject shared locks for hot files touched by multiple tasks.

    When multiple tasks declare the same hot file in their touched_paths,
    this function adds a shared lock (e.g., 'hot:api.py') to serialize
    access and prevent merge conflicts.

    Args:
        tasks: List of ParallelTask objects

    Returns:
        Modified list with locks injected (same list, modified in place)
    """
    import fnmatch

    # Map hot file base names to task IDs that touch them
    hot_file_tasks: dict[str, list[str]] = {}

    for task in tasks:
        for path in task.touched_paths:
            path_str = str(path)
            for hot_pattern in HOT_FILES:
                if fnmatch.fnmatch(path_str, hot_pattern):
                    # Extract base name for lock key
                    base_name = path_str.split("/")[-1] if "/" in path_str else path_str
                    if base_name not in hot_file_tasks:
                        hot_file_tasks[base_name] = []
                    if task.id not in hot_file_tasks[base_name]:
                        hot_file_tasks[base_name].append(task.id)
                    break  # Don't match multiple patterns for same path

    # Inject locks where multiple tasks touch the same hot file
    tasks_by_id = {t.id: t for t in tasks}
    injected_count = 0

    for base_name, task_ids in hot_file_tasks.items():
        if len(task_ids) > 1:
            lock_name = f"hot:{base_name}"
            for tid in task_ids:
                task = tasks_by_id.get(tid)
                if task and lock_name not in task.locks:
                    task.locks.append(lock_name)
                    injected_count += 1

    if injected_count > 0:
        print(f"[orchestrator] hot-file guardrail: injected {injected_count} lock(s) for concurrent hot file access")

    return tasks


def _inject_overlap_locks(tasks: list) -> list:
    """Inject file-based locks for any files touched by multiple tasks.

    This provides generalized collision prevention beyond just hot files.
    When multiple tasks declare the same file in their touched_paths,
    this function adds a file-specific lock to serialize access.

    This is "as narrow as possible, as strong as necessary" locking:
    - Only files actually touched by >1 task get locks
    - Tasks with different touched_paths can run in parallel
    - Tasks missing touched_paths get a conservative directory-based lock

    Args:
        tasks: List of ParallelTask objects

    Returns:
        Modified list with locks injected (same list, modified in place)
    """
    import hashlib

    # Map file paths to task IDs that touch them
    file_to_tasks: dict[str, list[str]] = {}

    for task in tasks:
        if task.status != "pending":
            continue

        if task.touched_paths:
            for path in task.touched_paths:
                path_str = str(path).replace("\\", "/")
                if path_str not in file_to_tasks:
                    file_to_tasks[path_str] = []
                if task.id not in file_to_tasks[path_str]:
                    file_to_tasks[path_str].append(task.id)
        else:
            # For tasks missing touched_paths, derive a conservative lock from task title/description
            # This ensures tasks without declared paths don't silently collide
            # Use a hash of title to create a "bucket" lock
            title_hash = hashlib.md5(task.title.encode()).hexdigest()[:8]
            # Derive directory from task ID (e.g., M1-IMPLEMENT-FOO -> dir:M1)
            dir_lock = f"dir:unknown:{title_hash}"
            if dir_lock not in task.locks:
                task.locks.append(dir_lock)

    # Inject locks where multiple tasks touch the same file
    tasks_by_id = {t.id: t for t in tasks}
    injected_count = 0

    for file_path, task_ids in file_to_tasks.items():
        if len(task_ids) > 1:
            # Create a hash-based lock for the file path
            path_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
            base_name = file_path.split("/")[-1] if "/" in file_path else file_path
            lock_name = f"file:{base_name}:{path_hash}"

            for tid in task_ids:
                task = tasks_by_id.get(tid)
                if task and lock_name not in task.locks:
                    task.locks.append(lock_name)
                    injected_count += 1

    if injected_count > 0:
        print(f"[orchestrator] overlap-lock: injected {injected_count} lock(s) for shared file access")

    return tasks


def _detect_noncompliant_output(text: str) -> tuple[bool, list[str], bool]:
    """Detect patterns in agent output that indicate non-compliance.

    Returns:
        (is_noncompliant, list_of_violations, has_critical_tool_violation)

    has_critical_tool_violation is True if the agent incorrectly claims tools
    are disabled - this requires immediate retry with explicit tools-enabled prompt.
    """
    violations: list[str] = []
    text_lower = text.lower()
    has_critical_tool_violation = False

    for pattern, description in NONCOMPLIANT_OUTPUT_PATTERNS:
        if re.search(pattern, text_lower, flags=re.IGNORECASE | re.MULTILINE):
            violations.append(description)
            # Check if this is a critical tool availability violation
            if description in CRITICAL_TOOL_VIOLATIONS:
                has_critical_tool_violation = True

    return bool(violations), violations, has_critical_tool_violation


def _build_strict_correction_prompt(
    agent: str,
    milestone_id: str,
    validation_error: str,
    task_title: str,
    task_description: str,
    noncompliant_violations: list[str] | None = None,
    attempt_number: int = 1,
    has_critical_tool_violation: bool = False,
) -> str:
    """Build an increasingly strict correction prompt based on attempt number.

    Attempt 1: Standard correction with clear requirements
    Attempt 2+: Stricter template with explicit prohibitions

    If has_critical_tool_violation is True, adds explicit "TOOLS ARE ENABLED" messaging
    to counteract any mistaken belief that tools are disabled.
    """
    stats_ref = "CL-1" if agent == "claude" else "CX-1"

    # Base violations text
    violations_text = ""
    if noncompliant_violations:
        violations_text = f"""
VIOLATIONS DETECTED: {", ".join(noncompliant_violations)}
These patterns are STRICTLY FORBIDDEN in your response."""

    # CRITICAL: If agent claimed tools were disabled, explicitly correct this
    tools_enabled_message = ""
    if has_critical_tool_violation:
        tools_enabled_message = """

## CRITICAL CORRECTION: TOOLS ARE ENABLED

Your previous response incorrectly claimed that tools are disabled.
THIS IS FALSE. You have FULL ACCESS to the following tools:
- Read: Read files from the filesystem
- Edit: Edit files with precise replacements
- Write: Create or overwrite files
- Bash: Execute shell commands
- Grep: Search file contents
- Glob: Find files by pattern

You CAN and SHOULD use these tools to complete your task.
DO NOT claim tools are disabled. Use them to implement the required changes.
"""

    if attempt_number == 1:
        return textwrap.dedent(f"""
            STRICT JSON CORRECTION REQUIRED
            {tools_enabled_message}
            Your previous response was INVALID: {validation_error}
            {violations_text}

            You MUST output ONLY a valid JSON object with these EXACT fields:
            - agent: "{agent}" (EXACTLY this string)
            - milestone_id: "{milestone_id}" (EXACTLY this string)
            - phase: "implement"
            - work_completed: true or false
            - project_complete: false
            - summary: "description of work"
            - gates_passed: []
            - requirement_progress: {{"covered_req_ids": [], "tests_added_or_modified": [], "commands_run": []}}
            - next_agent: "{agent}"
            - next_prompt: ""
            - delegate_rationale: ""
            - stats_refs: ["{stats_ref}"]
            - needs_write_access: true
            - artifacts: []

            CRITICAL RULES:
            - Output ONLY the JSON object, nothing else
            - NO markdown (no ```)
            - NO explanatory text before or after
            - NO comments
            - agent must be EXACTLY "{agent}"
            - milestone_id must be EXACTLY "{milestone_id}"

            FORBIDDEN PHRASES (do NOT output these):
            - "tools disabled" or "tools are disabled"
            - "cannot use tools" or "cannot access tools"
            - "don't have access to tools"
            - Any prose before/after the JSON

            Your task: {task_title}
            {task_description}
        """).strip()
    else:
        # Stricter template for attempt 2+
        return textwrap.dedent(f'''
            FINAL CORRECTION ATTEMPT - STRICT JSON ONLY
            {tools_enabled_message}
            ERROR: {validation_error}
            {violations_text}

            OUTPUT THIS EXACT JSON (modify values as needed):

            {{"agent": "{agent}", "milestone_id": "{milestone_id}", "phase": "implement", "work_completed": true, "project_complete": false, "summary": "Completed task", "gates_passed": [], "requirement_progress": {{"covered_req_ids": [], "tests_added_or_modified": [], "commands_run": []}}, "next_agent": "{agent}", "next_prompt": "", "delegate_rationale": "Task completed", "stats_refs": ["{stats_ref}"], "needs_write_access": true, "artifacts": []}}

            ABSOLUTE REQUIREMENTS:
            1. agent = "{agent}" (exact match required)
            2. milestone_id = "{milestone_id}" (exact match required)
            3. Output ONLY the JSON - no markdown, no prose, no explanation
            4. Do NOT claim tools are disabled - TOOLS ARE ENABLED
            5. Do NOT use ``` code blocks

            Task: {task_title}
        ''').strip()


def _is_noncompliant_and_should_use_stricter_prompt(text: str) -> tuple[bool, list[str], bool]:
    """Check if output warrants a stricter reprompt.

    Returns (should_use_stricter, violations, has_critical_tool_violation).

    has_critical_tool_violation indicates the agent incorrectly claimed tools are
    disabled, requiring explicit correction in the retry prompt.
    """
    is_noncompliant, violations, has_critical_tool_violation = _detect_noncompliant_output(text)

    # Also check if it looks like the agent dumped prose instead of JSON
    stripped = text.strip()
    if not stripped.startswith("{"):
        violations.append("output does not start with JSON")
        is_noncompliant = True

    return is_noncompliant, violations, has_critical_tool_violation


# NOTE: TurnNormalizer has been moved to bridge/loop_pkg/turn_normalizer.py
# for better tooling readability. Import is at the top of this file.


# -----------------------------
# Agent selection + quota (TurnNormalizer moved to loop_pkg/turn_normalizer.py)
# -----------------------------


# -----------------------------
# Agent selection + quota
# -----------------------------


def _is_quota_error(agent: str, text: str, config: RunConfig) -> bool:
    pats = config.quota_error_patterns.get(agent, [])
    return any(re.search(p, text, flags=re.IGNORECASE) for p in pats)


def _pick_fallback(config: RunConfig, state: RunState, current_agent: str | None) -> str:
    policy = get_agent_policy()

    # In forced mode, always return the forced agent (no fallback to other agent)
    if policy.forced_agent:
        return policy.enforce_strict(policy.forced_agent, "fallback selection")

    enabled = [
        a
        for a in config.enable_agents
        if (a in AGENTS)
        and (not state.disabled_by_quota.get(a, False))
        and (state.call_counts.get(a, 0) < config.max_calls_per_agent)
    ]

    if not enabled:
        return current_agent or AGENTS[0]

    enabled_others = [a for a in enabled if a != current_agent] or enabled

    for a in config.fallback_order:
        if a in enabled_others:
            return a

    return enabled_others[0]


def _other_agent(agent: str) -> str | None:
    if agent not in AGENTS:
        return None
    return "claude" if agent == "codex" else "codex"


def _override_next_agent(requested: str, config: RunConfig, state: RunState) -> tuple[str, str | None]:
    policy = get_agent_policy()

    # In forced mode, always return the forced agent
    if policy.forced_agent:
        forced = policy.enforce(requested, "next_agent override")
        if forced != requested:
            return forced, f"agent policy (--only-{policy.forced_agent})"
        return forced, None

    if config.smoke_route:
        idx = state.total_calls % len(config.smoke_route)
        routed, reason = resolve_smoke_route(requested=requested, route=config.smoke_route, index=idx)
        return routed, reason

    enabled = [
        a
        for a in config.enable_agents
        if (a in AGENTS)
        and (not state.disabled_by_quota.get(a, False))
        and (state.call_counts.get(a, 0) < config.max_calls_per_agent)
    ]

    if not enabled:
        return requested, None

    current = state.history[-1]["agent"] if state.history else None
    if current:
        other = _other_agent(current)
        if other and other in enabled:
            if requested != other:
                return other, "two-agent alternation (codex <-> claude)"
            return other, None

    if requested in enabled:
        return requested, None

    return enabled[0], f"requested agent '{requested}' disabled or unknown"


# -----------------------------
# Git helpers + verify
# -----------------------------


def _git_init_if_needed(project_root: Path) -> None:
    if not (project_root / ".git").exists():
        _run_cmd(["git", "init"], cwd=project_root, env=os.environ.copy())
        _run_cmd(["git", "config", "user.email", "agent@example.com"], cwd=project_root, env=os.environ.copy())
        _run_cmd(["git", "config", "user.name", "Agent Runner"], cwd=project_root, env=os.environ.copy())
        _run_cmd(["git", "add", "-A"], cwd=project_root, env=os.environ.copy())
        _run_cmd(["git", "commit", "-m", "chore: initial state"], cwd=project_root, env=os.environ.copy())


# Protected file that must never be reverted by auto-stash or checkout
_PROTECTED_FILE = "DESIGN_DOCUMENT.md"


def _capture_protected_file(project_root: Path) -> tuple[bytes | None, str | None]:
    """Capture current bytes and SHA256 of DESIGN_DOCUMENT.md before git operations.

    Returns (content_bytes, sha256_hex) or (None, None) if file doesn't exist.
    """
    protected_path = project_root / _PROTECTED_FILE
    if not protected_path.exists():
        return None, None
    try:
        content = protected_path.read_bytes()
        sha256_hex = hashlib.sha256(content).hexdigest()
        return content, sha256_hex
    except OSError:
        return None, None


def _restore_protected_file_if_changed(
    project_root: Path,
    original_content: bytes | None,
    original_hash: str | None,
    operation_name: str,
) -> bool:
    """Restore DESIGN_DOCUMENT.md if it was changed by a git operation.

    Returns True if file was restored, False otherwise.
    """
    if original_content is None:
        # File didn't exist before, nothing to restore
        return False

    protected_path = project_root / _PROTECTED_FILE

    # Check current state
    if not protected_path.exists():
        # File was deleted by git operation - restore it
        try:
            protected_path.write_bytes(original_content)
            print(f"[orchestrator] Restored {_PROTECTED_FILE} after {operation_name} (file was deleted)")
            return True
        except OSError as e:
            print(f"[orchestrator] WARNING: Failed to restore {_PROTECTED_FILE}: {e}")
            return False

    try:
        current_content = protected_path.read_bytes()
        current_hash = hashlib.sha256(current_content).hexdigest()
    except OSError:
        current_hash = None

    if current_hash != original_hash:
        # Content changed - restore original
        try:
            protected_path.write_bytes(original_content)
            print(f"[orchestrator] Restored {_PROTECTED_FILE} after {operation_name} to preserve local edits")
            return True
        except OSError as e:
            print(f"[orchestrator] WARNING: Failed to restore {_PROTECTED_FILE}: {e}")
            return False

    return False


def _checkout_agent_branch(project_root: Path, run_id: str) -> None:
    branch = f"agent-run/{run_id}"

    # Capture protected file state before checkout
    original_content, original_hash = _capture_protected_file(project_root)

    rc, _, _ = _run_cmd(["git", "checkout", "-b", branch], cwd=project_root, env=os.environ.copy())
    if rc != 0:
        _run_cmd(["git", "checkout", branch], cwd=project_root, env=os.environ.copy())

    # Restore protected file if it was changed by checkout
    _restore_protected_file_if_changed(project_root, original_content, original_hash, "checkout")


def _run_verify(project_root: Path, out_json: Path, strict_git: bool) -> tuple[int, str, str]:
    cmd = [sys.executable, "-m", "tools.verify", "--json", str(out_json)]
    if strict_git:
        cmd.append("--strict-git")
    return _run_cmd(cmd, cwd=project_root, env=os.environ.copy())


def _completion_gates_ok(project_root: Path) -> tuple[bool, str]:
    """The repo is 'complete' when strict verify passes and git status is clean."""

    if os.environ.get("FF_SKIP_VERIFY") == "1":
        return True, "ok (FF_SKIP_VERIFY=1)"

    env = os.environ.copy()
    rc, out, err = _run_cmd([sys.executable, "-m", "tools.verify", "--strict-git", "--include-m0"], cwd=project_root, env=env)
    if rc != 0:
        return False, (out + "\n" + err).strip()

    rc2, porcelain, err2 = _run_cmd(["git", "status", "--porcelain=v1"], cwd=project_root, env=env)
    if rc2 != 0:
        return False, (porcelain + "\n" + err2).strip()
    if porcelain.strip():
        return False, "git status not clean"

    return True, "ok"


def _preflight_check_repo(
    project_root: Path,
    *,
    auto_stash: bool = False,
    force_dirty: bool = False,
    runs_dir: Path | None = None,
) -> tuple[bool, str, str | None]:
    """Preflight check for dirty repo before starting parallel runs.

    Returns:
        (ok, message, stash_ref) where:
        - ok: True if repo is clean or was auto-stashed
        - message: Human-readable status message
        - stash_ref: The stash reference if auto-stash was used, else None
    """
    env = os.environ.copy()

    # Check for uncommitted changes
    rc, porcelain, _ = _run_cmd(["git", "status", "--porcelain=v1"], cwd=project_root, env=env)
    if rc != 0:
        return False, "Failed to run git status", None

    is_dirty = bool(porcelain.strip())

    # Also check for unmerged files (merge conflict state)
    rc2, unmerged, _ = _run_cmd(
        ["git", "diff", "--name-only", "--diff-filter=U"],
        cwd=project_root,
        env=env,
    )
    has_conflicts = rc2 == 0 and bool(unmerged.strip())

    if has_conflicts:
        conflict_files = unmerged.strip().split("\n")
        return False, f"PREFLIGHT FAILED: Unresolved merge conflicts in: {conflict_files}", None

    if not is_dirty:
        return True, "Repository is clean", None

    # Repo is dirty
    if force_dirty:
        return True, "WARNING: Proceeding with dirty repo (--force-dirty)", None

    if auto_stash:
        # Capture protected file state BEFORE stash (pathspec excludes are unreliable)
        original_content, original_hash = _capture_protected_file(project_root)

        # Stash all changes including untracked files
        # Note: pathspec exclude kept for belt-and-suspenders, but we don't rely on it
        stash_msg = f"orchestrator-auto-stash-{dt.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}"
        rc_stash, out_stash, err_stash = _run_cmd(
            ["git", "stash", "push", "-u", "-m", stash_msg, "--", ":/", ":(exclude):/DESIGN_DOCUMENT.md"],
            cwd=project_root,
            env=env,
        )
        if rc_stash != 0:
            return False, f"Failed to auto-stash: {err_stash}", None

        # CRITICAL: Restore protected file if stash reverted it (pathspec exclude unreliable)
        _restore_protected_file_if_changed(project_root, original_content, original_hash, "auto-stash")

        # Get the stash ref
        rc_ref, stash_list, _ = _run_cmd(
            ["git", "stash", "list", "--format=%gd %s"],
            cwd=project_root,
            env=env,
        )
        stash_ref = None
        if rc_ref == 0:
            for line in stash_list.strip().split("\n"):
                if stash_msg in line:
                    stash_ref = line.split()[0] if line else None
                    break

        # Write stash info to runs_dir if provided
        if runs_dir and stash_ref:
            _ensure_dir(runs_dir)
            stash_info_path = runs_dir / "stash_info.txt"
            stash_info_path.write_text(
                f"Auto-stash created: {stash_ref}\n"
                f"Message: {stash_msg}\n\n"
                f"To restore:\n  git stash pop {stash_ref}\n\n"
                f"Changes stashed:\n{porcelain}\n",
                encoding="utf-8",
            )

        return True, f"Auto-stashed uncommitted changes to {stash_ref}", stash_ref

    # Fail fast with clear error
    dirty_files = porcelain.strip().split("\n")[:10]  # First 10 files
    msg = (
        f"PREFLIGHT FAILED: Repository has uncommitted changes.\n"
        f"  Dirty files ({len(porcelain.strip().split(chr(10)))} total):\n" + "\n".join(f"    {f}" for f in dirty_files) + "\n\n"
        "  This will cause tools.verify --strict-git to fail during the run.\n"
        "  Options:\n"
        "    1. Commit or stash changes before running\n"
        "    2. Use --auto-stash to automatically stash changes\n"
        "    3. Use --force-dirty --verify-mode=skip-git to proceed anyway (not recommended)\n"
    )
    return False, msg, None


def _ast_extract_init_components(source: str) -> tuple[bool, str | None, list[str], list[str], list[tuple[str, str]]]:
    """Extract components from __init__.py source using AST for safety.

    Returns:
        (valid, docstring, imports, all_items, safe_assignments)

    safe_assignments is a list of (name, source_line) for simple string/int/version assignments.
    """
    imports: list[str] = []
    all_items: list[str] = []
    safe_assignments: list[tuple[str, str]] = []
    docstring: str | None = None

    try:
        tree = ast.parse(source)
    except SyntaxError:
        return False, None, [], [], []

    # Get docstring
    docstring = ast.get_docstring(tree)

    source_lines = source.split("\n")

    for node in ast.iter_child_nodes(tree):
        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Constant):
            # Module docstring - already handled
            continue

        elif isinstance(node, (ast.Import, ast.ImportFrom)):
            # Reconstruct import statement from source lines
            start = node.lineno - 1
            end = getattr(node, "end_lineno", node.lineno)
            import_lines = source_lines[start:end]
            import_str = "\n".join(import_lines).strip()
            if import_str:
                imports.append(import_str)

        elif isinstance(node, ast.Assign):
            # Check for __all__ or safe constant assignments
            for target in node.targets:
                if isinstance(target, ast.Name):
                    if target.id == "__all__" and isinstance(node.value, ast.List):
                        # Extract __all__ items
                        for elt in node.value.elts:
                            if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                                all_items.append(elt.value)
                    elif target.id.startswith("_") or target.id.isupper() or target.id == "__version__":
                        # Safe assignments: private vars, constants (ALL_CAPS), __version__
                        if isinstance(node.value, ast.Constant):
                            value = node.value.value
                            if isinstance(value, str):
                                safe_assignments.append((target.id, f'{target.id} = "{value}"'))
                            elif isinstance(value, (int, float)):
                                safe_assignments.append((target.id, f"{target.id} = {value}"))

    return True, docstring, imports, all_items, safe_assignments


def _text_extract_init_components(source: str) -> tuple[str | None, set[str], set[str]]:
    """Fallback text-based extraction for malformed Python.

    Returns:
        (docstring, imports, all_items)
    """
    imports: set[str] = set()
    all_items: set[str] = set()
    docstring: str | None = None

    lines = source.split("\n")
    in_docstring = False
    docstring_lines: list[str] = []
    in_multiline_all = False
    all_buffer: list[str] = []
    in_multiline_import = False
    import_buffer: list[str] = []

    for line in lines:
        stripped = line.strip()

        # Handle docstrings
        if not docstring:
            if '"""' in stripped or "'''" in stripped:
                quote = '"""' if '"""' in stripped else "'''"
                if not in_docstring:
                    in_docstring = True
                    docstring_lines.append(line)
                    # Check for single-line docstring
                    if stripped.count(quote) >= 2 and not stripped.endswith(quote + quote):
                        in_docstring = False
                        docstring = "\n".join(docstring_lines)
                else:
                    docstring_lines.append(line)
                    in_docstring = False
                    docstring = "\n".join(docstring_lines)
                continue
            elif in_docstring:
                docstring_lines.append(line)
                continue

        # Handle multiline imports (with parentheses)
        if in_multiline_import:
            # Check if this line starts a new import (abort the incomplete one)
            if stripped.startswith("from ") or stripped.startswith("import "):
                in_multiline_import = False
                import_buffer = []
                # Don't continue - fall through to process as new import
            elif ")" in stripped:
                import_buffer.append(stripped)
                in_multiline_import = False
                # Validate the import is complete
                full_import = " ".join(import_buffer)
                if full_import.count("(") == full_import.count(")"):
                    imports.add(full_import)
                import_buffer = []
                continue
            else:
                import_buffer.append(stripped)
                continue

        # Handle multiline __all__
        if in_multiline_all:
            all_buffer.append(stripped)
            if "]" in stripped:
                in_multiline_all = False
                full_all = " ".join(all_buffer)
                for item in re.findall(r'"([^"]+)"|\'([^\']+)\'', full_all):
                    all_items.add(item[0] or item[1])
                all_buffer = []
            continue

        # Handle imports
        if stripped.startswith("from ") and " import " in stripped:
            # Check for multiline import (unclosed paren)
            if "(" in stripped and ")" not in stripped:
                in_multiline_import = True
                import_buffer = [stripped]
            elif "(" in stripped and ")" in stripped:
                # Complete single-line import with parens
                imports.add(stripped)
            elif "(" not in stripped:
                # Simple import without parens
                imports.add(stripped)
            # Skip incomplete imports (has open paren but no close)
        elif stripped.startswith("import "):
            imports.add(stripped)

        # Handle __all__ (single line or start of multiline)
        if "__all__" in stripped and "=" in stripped:
            if "[" in stripped and "]" in stripped:
                # Single line __all__
                for item in re.findall(r'"([^"]+)"|\'([^\']+)\'', stripped):
                    all_items.add(item[0] or item[1])
            elif "[" in stripped:
                # Start of multiline __all__
                in_multiline_all = True
                all_buffer.append(stripped)

    return docstring, imports, all_items


def _parse_conflict_blocks(content: str) -> list[tuple[str, str]]:
    """Parse all conflict blocks from content.

    Returns list of (ours_content, theirs_content) tuples for each conflict block.
    """
    conflicts: list[tuple[str, str]] = []
    lines = content.split("\n")

    i = 0
    while i < len(lines):
        if lines[i].startswith("<<<<<<< "):
            ours_lines: list[str] = []
            theirs_lines: list[str] = []
            i += 1

            # Collect "ours" side
            while i < len(lines) and not lines[i].startswith("======="):
                ours_lines.append(lines[i])
                i += 1

            i += 1  # Skip =======

            # Collect "theirs" side
            while i < len(lines) and not lines[i].startswith(">>>>>>> "):
                theirs_lines.append(lines[i])
                i += 1

            conflicts.append(("\n".join(ours_lines), "\n".join(theirs_lines)))
        i += 1

    return conflicts


def _remove_conflict_markers(content: str) -> str:
    """Remove conflict markers while keeping common content."""
    lines = content.split("\n")
    result: list[str] = []
    state = "normal"

    for line in lines:
        if line.startswith("<<<<<<< "):
            state = "ours"
        elif line.startswith("=======") and state == "ours":
            state = "theirs"
        elif line.startswith(">>>>>>> ") and state == "theirs":
            state = "normal"
        elif state == "normal":
            result.append(line)
        # In conflict state, we skip lines (they'll be re-added by merge)

    return "\n".join(result)


def _write_manual_resolution_artifact(file_path: Path, reason: str, ours: str, theirs: str) -> None:
    """Write a .manual_resolution file when auto-resolution fails."""
    artifact_path = file_path.parent / f"{file_path.name}.manual_resolution"
    content = f"""# MANUAL RESOLUTION REQUIRED
# File: {file_path}
# Reason: {reason}
# Timestamp: {dt.datetime.now(dt.timezone.utc).isoformat()}

# === OURS (HEAD) ===
{ours}

# === THEIRS (incoming) ===
{theirs}

# === ACTION REQUIRED ===
# 1. Edit {file_path} to resolve the conflict manually
# 2. Run: python3 -m py_compile {file_path}
# 3. Run: git add {file_path}
# 4. Delete this file: rm {artifact_path}
"""
    try:
        artifact_path.write_text(content, encoding="utf-8")
    except Exception:
        pass  # Best effort


def _auto_resolve_init_py_conflict(file_path: Path, project_root: Path) -> tuple[bool, str]:
    """Auto-resolve merge conflict in __init__.py files using AST-safe union merge.

    Strategy:
    1. Parse all conflict blocks
    2. Try AST-based extraction on each side (fall back to text-based for invalid syntax)
    3. Extract: docstring, imports, __all__ items, safe assignments
    4. Union-merge: deduplicate imports, union __all__, prefer longer docstring
    5. Validate result with py_compile
    6. If unresolvable, write manual resolution artifact

    Returns:
        (success, message)
    """
    try:
        content = file_path.read_text(encoding="utf-8")
    except Exception as e:
        return False, f"Cannot read conflict file: {e}"

    # Check for conflict markers
    has_markers = "<<<<<<< " in content and "=======" in content and ">>>>>>> " in content

    if not has_markers:
        return False, "No conflict markers found"

    # Parse all conflict blocks
    conflict_blocks = _parse_conflict_blocks(content)
    if not conflict_blocks:
        return False, "Failed to parse conflict blocks"

    # Get common content (before/after conflict blocks, with markers removed)
    common_content = _remove_conflict_markers(content)

    # Collect all components
    all_docstrings: list[str] = []
    all_imports: set[str] = set()
    all_all_items: set[str] = set()
    all_safe_assigns: dict[str, str] = {}

    # Process common content
    common_valid, common_doc, common_imports, common_all, common_assigns = _ast_extract_init_components(common_content)
    if common_valid:
        if common_doc:
            all_docstrings.append(common_doc)
        all_imports.update(common_imports)
        all_all_items.update(common_all)
        for name, line in common_assigns:
            all_safe_assigns[name] = line
    else:
        # Fallback to text extraction
        doc, imps, items = _text_extract_init_components(common_content)
        if doc:
            all_docstrings.append(doc)
        all_imports.update(imps)
        all_all_items.update(items)

    # Process each conflict block
    ours_full: list[str] = []
    theirs_full: list[str] = []

    for ours_content, theirs_content in conflict_blocks:
        ours_full.append(ours_content)
        theirs_full.append(theirs_content)

        # Process "ours" side
        ours_valid, ours_doc, ours_imports, ours_all, ours_assigns = _ast_extract_init_components(ours_content)
        if ours_valid:
            if ours_doc:
                all_docstrings.append(ours_doc)
            all_imports.update(ours_imports)
            all_all_items.update(ours_all)
            for name, line in ours_assigns:
                all_safe_assigns[name] = line
        else:
            # Fallback to text extraction for invalid syntax
            doc, imps, items = _text_extract_init_components(ours_content)
            if doc:
                all_docstrings.append(doc)
            all_imports.update(imps)
            all_all_items.update(items)

        # Process "theirs" side
        theirs_valid, theirs_doc, theirs_imports, theirs_all, theirs_assigns = _ast_extract_init_components(theirs_content)
        if theirs_valid:
            if theirs_doc:
                all_docstrings.append(theirs_doc)
            all_imports.update(theirs_imports)
            all_all_items.update(theirs_all)
            for name, line in theirs_assigns:
                all_safe_assigns[name] = line
        else:
            # Fallback to text extraction for invalid syntax
            doc, imps, items = _text_extract_init_components(theirs_content)
            if doc:
                all_docstrings.append(doc)
            all_imports.update(imps)
            all_all_items.update(items)

    # Choose docstring (prefer longer)
    docstring = '"""Package exports."""'
    if all_docstrings:
        all_docstrings.sort(key=len, reverse=True)
        docstring = all_docstrings[0]
        # Ensure docstring is properly formatted
        if not docstring.startswith(('"""', "'''")):
            docstring = f'"""{docstring}"""'

    # Sort imports for determinism (group from imports and regular imports)
    from_imports = sorted([i for i in all_imports if i.startswith("from ")])
    regular_imports = sorted([i for i in all_imports if i.startswith("import ")])
    sorted_imports = from_imports + regular_imports

    # Sort __all__ items for determinism
    sorted_all_items = sorted(all_all_items)

    # Build result
    result_lines: list[str] = []

    # Add docstring
    result_lines.append(docstring)
    result_lines.append("")

    # Add imports
    for imp in sorted_imports:
        result_lines.append(imp)

    if sorted_imports:
        result_lines.append("")

    # Add safe assignments (like __version__)
    for name in sorted(all_safe_assigns.keys()):
        result_lines.append(all_safe_assigns[name])

    if all_safe_assigns:
        result_lines.append("")

    # Add __all__
    if sorted_all_items:
        result_lines.append("__all__ = [")
        for item in sorted_all_items:
            result_lines.append(f'    "{item}",')
        result_lines.append("]")

    result_content = "\n".join(result_lines)
    if not result_content.endswith("\n"):
        result_content += "\n"

    # Validate syntax with compile()
    try:
        compile(result_content, str(file_path), "exec")
    except SyntaxError as e:
        # Write manual resolution artifact
        _write_manual_resolution_artifact(
            file_path,
            f"Generated code has syntax error: {e}",
            "\n".join(ours_full),
            "\n".join(theirs_full),
        )
        return False, f"Generated code has syntax error: {e}. Manual resolution artifact written."

    # Double-check with py_compile for extra safety
    import py_compile
    import tempfile

    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False, encoding="utf-8") as tf:
        tf.write(result_content)
        tf_path = tf.name

    try:
        py_compile.compile(tf_path, doraise=True)
    except py_compile.PyCompileError as e:
        os.unlink(tf_path)
        _write_manual_resolution_artifact(
            file_path,
            f"py_compile failed: {e}",
            "\n".join(ours_full),
            "\n".join(theirs_full),
        )
        return False, f"py_compile validation failed: {e}. Manual resolution artifact written."
    finally:
        if os.path.exists(tf_path):
            os.unlink(tf_path)

    # Write resolved file
    try:
        file_path.write_text(result_content, encoding="utf-8")
    except Exception as e:
        return False, f"Cannot write resolved file: {e}"

    return True, "Auto-resolved __init__.py conflict by AST-safe union merge"


def _attempt_auto_merge_resolution(
    project_root: Path,
    task_id: str,
    runs_dir: Path,
    task_context: str = "",
    milestone_id: str = "M0",
) -> tuple[bool, str]:
    """Attempt to auto-resolve merge conflicts.

    This function:
    1. Tries built-in resolution for __init__.py files (deterministic merge)
    2. Uses agent-based resolution for other files (Claude-powered intelligent merge)
    3. Falls back to manual only if all automated approaches fail

    Returns:
        (success, message)
    """
    env = os.environ.copy()

    # Get list of conflicted files
    rc, conflict_out, _ = _run_cmd(
        ["git", "diff", "--name-only", "--diff-filter=U"],
        cwd=project_root,
        env=env,
    )
    if rc != 0 or not conflict_out.strip():
        return False, "No conflict files detected or git command failed"

    conflict_files = [f.strip() for f in conflict_out.strip().split("\n") if f.strip()]
    if not conflict_files:
        return False, "No conflict files"

    resolved_files: list[str] = []
    unresolved_files: list[str] = []
    init_py_files: list[str] = []
    other_files: list[str] = []

    # Categorize files
    for cf in conflict_files:
        if cf.endswith("__init__.py"):
            init_py_files.append(cf)
        else:
            other_files.append(cf)

    # Phase 1: Try built-in resolution for __init__.py files (deterministic)
    for cf in init_py_files:
        file_path = project_root / cf
        success, msg = _auto_resolve_init_py_conflict(file_path, project_root)
        if success:
            # Stage the resolved file
            _run_cmd(["git", "add", cf], cwd=project_root, env=env)
            resolved_files.append(cf)
            print(f"[orchestrator] AUTO-RESOLVED (__init__.py): {cf} ({msg})")
        else:
            unresolved_files.append(cf)
            print(f"[orchestrator] CANNOT AUTO-RESOLVE (__init__.py): {cf} ({msg})")

    # Phase 2: Try agent-based resolution for other files
    if other_files:
        print(f"[orchestrator] Trying agent-based merge resolution for {len(other_files)} file(s): {other_files}")
        try:
            from bridge.merge_resolver import attempt_agent_merge_resolution

            merge_result = attempt_agent_merge_resolution(
                project_root=project_root,
                runs_dir=runs_dir,
                task_id=task_id,
                task_context=task_context,
                milestone_id=milestone_id,
                max_attempts=3,
            )

            if merge_result.success:
                resolved_files.extend(merge_result.resolved_files)
                print(f"[orchestrator] AGENT-RESOLVED: {merge_result.resolved_files} after {merge_result.attempt} attempt(s)")
            else:
                unresolved_files.extend(merge_result.unresolved_files)
                print(f"[orchestrator] AGENT RESOLUTION FAILED: {merge_result.error}")
                print(f"[orchestrator] Unresolved: {merge_result.unresolved_files}")

        except Exception as e:
            print(f"[orchestrator] Agent merge resolver exception: {e}")
            unresolved_files.extend(other_files)

    if unresolved_files:
        return False, f"Unresolved conflicts remain: {unresolved_files}"

    # All conflicts resolved - commit
    if resolved_files:
        commit_msg = f"auto-resolve: merge conflict in {', '.join(resolved_files)}"
        rc_commit, _, err_commit = _run_cmd(
            ["git", "commit", "-m", commit_msg],
            cwd=project_root,
            env=env,
        )
        if rc_commit != 0:
            return False, f"Failed to commit resolved conflicts: {err_commit}"

        # Log resolution
        resolution_log_path = runs_dir / "auto_merge_resolutions.txt"
        with resolution_log_path.open("a", encoding="utf-8") as f:
            f.write(f"Task: {task_id}\n")
            f.write(f"Resolved: {resolved_files}\n")
            f.write(f"Commit: {commit_msg}\n")
            f.write("-" * 40 + "\n")

        return True, f"Auto-resolved conflicts in: {resolved_files}"

    return False, "No files were resolved"


# -----------------------------
# Agent runners
# -----------------------------


def _run_agent_live(
    *,
    agent: str,
    prompt_path: Path,
    schema_path: Path,
    out_path: Path,
    config: RunConfig,
    state: RunState,
    stream_agent_output: str = "none",
    call_dir: Path | None = None,
) -> tuple[int, str, str]:
    script_rel = config.agent_scripts.get(agent, "")
    if not script_rel:
        return 1, "", f"No script configured for agent {agent}"

    script_path = state.project_root / script_rel
    if not script_path.exists():
        return 1, "", f"Agent script not found: {script_path}"

    _ensure_dir(out_path.parent)

    cmd = [
        str(script_path),
        str(prompt_path),
        str(schema_path),
        str(out_path),
    ]

    env = os.environ.copy()
    write_access = state.grant_write_access and not state.readonly
    # Support both names (some wrappers read WRITE_ACCESS, older versions used ORCH_WRITE_ACCESS).
    env["WRITE_ACCESS"] = "1" if write_access else "0"
    env["ORCH_WRITE_ACCESS"] = env["WRITE_ACCESS"]
    env["ORCH_READONLY"] = "1" if state.readonly else "0"
    # Signal to the agent wrapper that this is a turn schema (enables turn normalization)
    env["ORCH_SCHEMA_KIND"] = "turn"

    if call_dir:
        return run_cmd_with_streaming(
            cmd=cmd,
            cwd=state.project_root,
            env=env,
            agent=agent,
            stream_mode=stream_agent_output,
            call_dir=call_dir,
        )

    stream_enabled = stream_agent_output != "none"
    return _run_cmd(cmd, cwd=state.project_root, env=env, stream=stream_enabled)


def _run_agent_mock(*, agent: str, scenario: dict[str, Any], mock_indices: dict[str, int]) -> tuple[int, str, str]:
    block = scenario.get("agents", {}).get(agent, [])
    idx = mock_indices.get(agent, 0)
    if idx >= len(block):
        return 1, "", f"mock scenario: no more responses for agent '{agent}' (idx={idx})"
    mock_indices[agent] = idx + 1

    item = block[idx]
    t = item.get("type")
    if t == "quota_error":
        return int(item.get("exit_code", 1)), item.get("stdout", ""), item.get("stderr", "TerminalQuotaError")
    if t == "error":
        return int(item.get("exit_code", 1)), item.get("stdout", ""), item.get("stderr", "mock error")
    if t != "ok":
        return 1, "", f"mock scenario: unknown item type '{t}'"

    resp = item.get("response")
    return 0, json.dumps(resp, indent=2, sort_keys=True), ""


# -----------------------------
# Parallel runner
# -----------------------------


@dataclasses.dataclass
class ParallelTask:
    id: str
    title: str
    description: str
    agent: str
    intensity: str = "low"  # low|medium|high
    locks: list[str] = dataclasses.field(default_factory=list)
    touched_paths: list[str] = dataclasses.field(default_factory=list)
    depends_on: list[str] = dataclasses.field(default_factory=list)
    solo: bool = False

    # runtime
    # Status values:
    #   pending - not yet started
    #   running - currently executing
    #   done - completed successfully
    #   failed - execution failed (root failure)
    #   manual - needs manual intervention (root failure)
    #   blocked - transitively blocked by a root failure
    #   pending_rerun - agent returned work_completed=false (root failure, needs retry)
    #   resource_killed - killed due to resource limits (root failure)
    status: str = "pending"
    worker_id: int | None = None
    branch: str | None = None
    base_sha: str | None = None
    worktree_path: Path | None = None
    task_dir: Path | None = None
    prompt_path: Path | None = None
    out_path: Path | None = None
    raw_log_path: Path | None = None
    manual_path: Path | None = None
    error: str | None = None
    max_cpu_pct_total: float = 0.0
    max_mem_pct_total: float = 0.0
    # Turn output tracking
    work_completed: bool | None = None
    commit_sha: str | None = None
    turn_summary: str | None = None
    turn_obj: dict[str, Any] | None = None
    requirement_progress: dict[str, Any] | None = None
    commands_run: list[str] = dataclasses.field(default_factory=list)
    tests_added_or_modified: list[str] = dataclasses.field(default_factory=list)
    covered_req_ids: list[str] = dataclasses.field(default_factory=list)
    # Patch integration (commit-free worker operation)
    patch_path: Path | None = None
    patch_manifest_path: Path | None = None
    has_patch: bool = False
    # Retry tracking for plan-only responses
    retry_count: int = 0
    max_retries: int = 2


def _sanitize_branch_fragment(text: str) -> str:
    frag = re.sub(r"[^A-Za-z0-9._-]+", "-", text.strip())
    frag = frag.strip("-.")
    return frag or "task"


def _collect_machine_info() -> dict[str, Any]:
    cores = os.cpu_count() or 1
    total_ram = _total_ram_bytes()
    return {
        "cpu_cores": int(cores),
        "ram_bytes": int(total_ram) if total_ram else None,
    }


def _analyze_plan_width(
    plan_obj: dict[str, Any],
    max_workers_limit: int,
) -> dict[str, Any]:
    """Analyze a plan for throughput mode compliance.

    Returns:
        dict with keys:
        - task_count: int
        - root_count: int (tasks with empty depends_on)
        - plan_cap: int (max_parallel_tasks from plan)
        - min_plan_cap: int (required minimum for throughput)
        - min_root: int (required minimum roots for throughput)
        - min_task_count: int (recommended, not required)
        - is_thin: bool (True only if HARD requirements not met)
        - issues: list[str] (human-readable issues - hard failures)
        - warnings: list[str] (soft warnings - don't trigger reprompt)

    Note: is_thin is True only for HARD failures (plan_cap, root_count).
    total_tasks < 2*workers is a soft warning that does NOT trigger reprompt.
    """
    tasks = plan_obj.get("tasks", [])
    task_count = len(tasks) if isinstance(tasks, list) else 0
    root_count = sum(1 for t in (tasks if isinstance(tasks, list) else []) if isinstance(t, dict) and not t.get("depends_on"))
    plan_cap = int(plan_obj.get("max_parallel_tasks", 1) or 1)

    # Throughput mode thresholds
    min_plan_cap = max_workers_limit
    min_root = max_workers_limit
    min_task_count = max_workers_limit * 2  # Recommended (soft), not required

    # HARD failures (must reprompt)
    issues: list[str] = []
    if plan_cap < min_plan_cap:
        issues.append(f"max_parallel_tasks={plan_cap} < required {min_plan_cap}")
    if root_count < min_root:
        issues.append(f"root_tasks={root_count} < required {min_root}")

    # SOFT warnings (do NOT trigger reprompt on their own)
    warnings: list[str] = []
    if task_count < min_task_count:
        warnings.append(f"total_tasks={task_count} < recommended {min_task_count}")

    return {
        "task_count": task_count,
        "root_count": root_count,
        "plan_cap": plan_cap,
        "min_plan_cap": min_plan_cap,
        "min_root": min_root,
        "min_task_count": min_task_count,
        "is_thin": len(issues) > 0,  # Only hard failures trigger is_thin
        "issues": issues,
        "warnings": warnings,
    }


# ------------------------------------
# Plan Quality Scoring (Throughput Mode)
# ------------------------------------


def _compute_lock_pressure(plan_obj: dict[str, Any]) -> dict[str, Any]:
    """Compute lock pressure metrics for the plan.

    Returns dict with:
    - lock_counts: dict mapping lock key -> count of tasks using it
    - max_lock_count: highest count for any single lock
    - high_pressure_locks: list of locks with > 2 tasks (likely serialization)
    - pressure_score: 0.0 (no pressure) to 1.0 (severe serialization)
    """
    tasks = plan_obj.get("tasks", [])
    if not isinstance(tasks, list):
        return {
            "lock_counts": {},
            "max_lock_count": 0,
            "high_pressure_locks": [],
            "pressure_score": 0.0,
        }

    lock_counts: dict[str, int] = {}
    for t in tasks:
        if not isinstance(t, dict):
            continue
        locks = t.get("locks", [])
        if not isinstance(locks, list):
            continue
        for lock in locks:
            if isinstance(lock, str) and lock.strip():
                lock_counts[lock.strip()] = lock_counts.get(lock.strip(), 0) + 1

    max_lock_count = max(lock_counts.values()) if lock_counts else 0
    high_pressure_locks = [k for k, v in lock_counts.items() if v > 2]

    # Pressure score: 0 if max <= 2, scaled up to 1.0 if max >= 8
    pressure_score = 0.0
    if max_lock_count > 2:
        pressure_score = min(1.0, (max_lock_count - 2) / 6)

    return {
        "lock_counts": lock_counts,
        "max_lock_count": max_lock_count,
        "high_pressure_locks": high_pressure_locks,
        "pressure_score": pressure_score,
    }


def _compute_task_focus_score(task: dict[str, Any]) -> float:
    """Compute focus score for a single task (0.0 = vague, 1.0 = highly focused).

    Scoring:
    - +0.3 if description mentions specific files/modules (patterns: .py, .ts, src/, tests/)
    - +0.2 if description mentions specific functions/classes (patterns: def, class, function)
    - +0.2 if description mentions tests or test coverage
    - +0.2 if description is sufficiently detailed (>100 chars)
    - +0.1 if title is specific (not generic like "implement", "do", "setup")
    - -0.3 if description is vague (contains "subsystem", "everything", "all of")
    """
    desc = str(task.get("description", "")).lower()
    title = str(task.get("title", "")).lower()

    score = 0.0

    # +0.3 for file/module references
    file_patterns = [".py", ".ts", ".js", ".json", "src/", "tests/", "bridge/", "docs/"]
    if any(pat in desc for pat in file_patterns):
        score += 0.3

    # +0.2 for function/class references
    code_patterns = ["def ", "class ", "function ", "const ", "import ", "from "]
    if any(pat in desc for pat in code_patterns):
        score += 0.2

    # +0.2 for test mentions
    test_patterns = ["test", "pytest", "assert", "validate", "verify", "coverage"]
    if any(pat in desc for pat in test_patterns):
        score += 0.2

    # +0.2 for detailed description
    if len(desc) > 100:
        score += 0.2

    # +0.1 for specific title (not generic)
    vague_titles = ["implement", "do ", "setup", "handle", "work on", "complete"]
    if not any(vt in title for vt in vague_titles):
        score += 0.1

    # -0.3 for vague descriptions
    vague_patterns = ["subsystem", "everything", "all of ", "entire ", "whole "]
    if any(vp in desc for vp in vague_patterns):
        score -= 0.3

    return max(0.0, min(1.0, score))


def _compute_coverage_intent(plan_obj: dict[str, Any]) -> float:
    """Compute proportion of tasks that mention tests or validation outputs.

    Returns float in [0.0, 1.0] representing the proportion.
    Throughput mode target: >= 0.30 (30%)
    """
    tasks = plan_obj.get("tasks", [])
    if not isinstance(tasks, list) or len(tasks) == 0:
        return 0.0

    test_patterns = ["test", "pytest", "assert", "validate", "verify", "spec", "check"]
    test_count = 0

    for t in tasks:
        if not isinstance(t, dict):
            continue
        desc = str(t.get("description", "")).lower()
        title = str(t.get("title", "")).lower()
        text = desc + " " + title
        if any(pat in text for pat in test_patterns):
            test_count += 1

    return test_count / len(tasks)


def _analyze_risk_flags(plan_obj: dict[str, Any]) -> dict[str, Any]:
    """Analyze risk flags in the plan.

    Returns dict with:
    - solo_tasks: list of task IDs with solo=True
    - high_intensity_tasks: list of task IDs with intensity="high"
    - risk_count: total count of risky tasks
    - risk_ratio: proportion of risky tasks (target: < 0.2 in throughput mode)
    """
    tasks = plan_obj.get("tasks", [])
    if not isinstance(tasks, list):
        return {
            "solo_tasks": [],
            "high_intensity_tasks": [],
            "risk_count": 0,
            "risk_ratio": 0.0,
        }

    solo_tasks = []
    high_intensity_tasks = []

    for t in tasks:
        if not isinstance(t, dict):
            continue
        tid = str(t.get("id", "unknown"))
        if t.get("solo", False):
            solo_tasks.append(tid)
        intensity = str(t.get("estimated_intensity", t.get("intensity", "low"))).lower()
        if intensity == "high":
            high_intensity_tasks.append(tid)

    risk_count = len(set(solo_tasks) | set(high_intensity_tasks))
    risk_ratio = risk_count / len(tasks) if tasks else 0.0

    return {
        "solo_tasks": solo_tasks,
        "high_intensity_tasks": high_intensity_tasks,
        "risk_count": risk_count,
        "risk_ratio": risk_ratio,
    }


def _build_plan_quality_report(
    plan_obj: dict[str, Any],
    max_workers_limit: int,
    planner_profile: str = "balanced",
) -> dict[str, Any]:
    """Build comprehensive plan quality report for throughput mode.

    Returns dict with:
    - metrics: all computed metrics
    - issues: list of actionable issue strings
    - should_reprompt: bool - whether to trigger a reprompt
    - reprompt_message: str - targeted guidance for the reprompt
    - hard_failures: list of issues that are hard failures (must reprompt)
    - soft_warnings: list of issues that are warnings only
    """
    # Only do quality scoring in throughput mode
    if planner_profile != "throughput":
        return {
            "metrics": {},
            "issues": [],
            "should_reprompt": False,
            "reprompt_message": "",
            "hard_failures": [],
            "soft_warnings": [],
        }

    tasks = plan_obj.get("tasks", [])
    task_count = len(tasks) if isinstance(tasks, list) else 0
    root_count = sum(1 for t in (tasks if isinstance(tasks, list) else []) if isinstance(t, dict) and not t.get("depends_on"))
    plan_cap = int(plan_obj.get("max_parallel_tasks", 1) or 1)

    # Compute quality metrics
    lock_pressure = _compute_lock_pressure(plan_obj)
    coverage_intent = _compute_coverage_intent(plan_obj)
    risk_flags = _analyze_risk_flags(plan_obj)

    # Compute average focus score
    focus_scores = []
    for t in tasks if isinstance(tasks, list) else []:
        if isinstance(t, dict):
            focus_scores.append(_compute_task_focus_score(t))
    avg_focus_score = sum(focus_scores) / len(focus_scores) if focus_scores else 0.0

    metrics = {
        "plan_cap": plan_cap,
        "root_ready": root_count,
        "total_tasks": task_count,
        "max_lock_count": lock_pressure["max_lock_count"],
        "high_pressure_locks": lock_pressure["high_pressure_locks"],
        "lock_pressure_score": lock_pressure["pressure_score"],
        "avg_focus_score": round(avg_focus_score, 2),
        "coverage_intent": round(coverage_intent, 2),
        "risk_ratio": round(risk_flags["risk_ratio"], 2),
        "solo_tasks": risk_flags["solo_tasks"],
        "high_intensity_tasks": risk_flags["high_intensity_tasks"],
    }

    # Categorize issues into hard failures vs soft warnings
    hard_failures: list[str] = []
    soft_warnings: list[str] = []

    # HARD FAILURE: plan_cap < max_workers
    if plan_cap < max_workers_limit:
        hard_failures.append(f"max_parallel_tasks={plan_cap} < required {max_workers_limit}")

    # HARD FAILURE: root_ready < max_workers
    if root_count < max_workers_limit:
        hard_failures.append(f"root_tasks={root_count} < required {max_workers_limit}")

    # HARD FAILURE: high lock pressure (serializes workers)
    if lock_pressure["max_lock_count"] > 3:
        locks_str = ", ".join(lock_pressure["high_pressure_locks"][:3])
        hard_failures.append(f"lock_pressure=high (max {lock_pressure['max_lock_count']} tasks share a lock: {locks_str})")

    # SOFT WARNING: total_tasks < 2*workers (only if root_ready is also borderline)
    min_task_count = max_workers_limit * 2
    if task_count < min_task_count:
        if root_count < max_workers_limit + 2:  # borderline roots
            soft_warnings.append(f"total_tasks={task_count} < recommended {min_task_count} (with borderline roots)")
        else:
            soft_warnings.append(f"total_tasks={task_count} < recommended {min_task_count} (acceptable: roots={root_count})")

    # SOFT WARNING: low coverage intent
    if coverage_intent < 0.30:
        soft_warnings.append(f"coverage_intent={coverage_intent:.0%} < recommended 30% (few tasks mention tests)")

    # SOFT WARNING: low focus scores
    if avg_focus_score < 0.4:
        soft_warnings.append(f"avg_focus_score={avg_focus_score:.2f} < recommended 0.40 (tasks may be vague)")

    # SOFT WARNING: high risk ratio
    if risk_flags["risk_ratio"] > 0.2:
        soft_warnings.append(f"risk_ratio={risk_flags['risk_ratio']:.0%} > recommended 20% (too many solo/high tasks)")

    # Build reprompt message
    all_issues = hard_failures + soft_warnings
    should_reprompt = len(hard_failures) > 0

    reprompt_message = ""
    if should_reprompt:
        issues_text = "\n".join(f"  - {issue}" for issue in all_issues)
        reprompt_message = f"Issues detected:\n{issues_text}"

        # Add targeted guidance based on failure type
        if any("lock_pressure" in hf for hf in hard_failures):
            reprompt_message += "\n\nYour locks are serializing workers. Use distinct locks per file/module or separate files to enable true parallelism."

    return {
        "metrics": metrics,
        "issues": all_issues,
        "should_reprompt": should_reprompt,
        "reprompt_message": reprompt_message,
        "hard_failures": hard_failures,
        "soft_warnings": soft_warnings,
    }


def _build_throughput_correction_prompt(
    analysis: dict[str, Any],
    max_workers_limit: int,
    *,
    quality_report: dict[str, Any] | None = None,
    attempt_number: int = 1,
) -> str:
    """Build a strict correction prompt for thin plans in throughput mode.

    Args:
        analysis: Result from _analyze_plan_width
        max_workers_limit: Target worker count
        quality_report: Optional result from _build_plan_quality_report (for detailed guidance)
        attempt_number: Current retry attempt (1-based); adds targeted guidance on attempt 2+
    """
    # Combine issues from analysis and quality report
    all_issues = list(analysis.get("issues", []))
    all_warnings = list(analysis.get("warnings", []))

    if quality_report:
        # Add quality-specific issues
        for issue in quality_report.get("hard_failures", []):
            if issue not in all_issues:
                all_issues.append(issue)
        for warning in quality_report.get("soft_warnings", []):
            if warning not in all_warnings:
                all_warnings.append(warning)

    issues_text = "\n".join(f"  - {issue}" for issue in all_issues)
    warnings_text = "\n".join(f"  - {warning}" for warning in all_warnings) if all_warnings else ""

    # Build targeted guidance based on failure type and attempt number
    targeted_guidance = ""

    # Check for lock pressure issues
    has_lock_pressure = quality_report and any("lock_pressure" in hf for hf in quality_report.get("hard_failures", []))

    if has_lock_pressure or attempt_number >= 2:
        # On second attempt or if lock pressure detected, add explicit lock guidance
        targeted_guidance = """
        **LOCK SERIALIZATION WARNING:**
        Your locks are serializing workers! This kills parallelism.

        FIX THIS:
        - Use DISTINCT lock keys per file or module (e.g., "m4-types", "m4-io-reader", "m5-eval")
        - Do NOT put multiple tasks behind the same lock unless they truly conflict
        - If a shared integration file must be touched, create ONE "INTEGRATION" task that runs LAST
        - Example: 8 tasks using lock "init-m4" = serial execution = BAD
                   8 tasks using locks "m4-types", "m4-io", "m4-preprocess", etc. = parallel = GOOD
        """

    base_prompt = textwrap.dedent(f"""
        PLAN REJECTED: Your plan does not meet throughput mode requirements.

        **HARD FAILURES (must fix):**
        {issues_text}
        {
        f'''
        **WARNINGS (should address):**
        {warnings_text}
        '''
        if warnings_text
        else ""
    }
        {targeted_guidance}

        **THROUGHPUT MODE REQUIREMENTS (MANDATORY):**
        1. Set max_parallel_tasks = {max_workers_limit}
        2. Create at least {max_workers_limit} root tasks (depends_on = [])
        3. Target {max_workers_limit * 2}+ total tasks (recommended, not strictly required)
        4. Use DISTINCT locks per file/module - avoid lock serialization
        5. Split large tasks into smaller, file-focused tasks (each touching ≤3 files)

        **QUALITY REQUIREMENTS:**
        - Each task description MUST include files/modules touched
        - Each task SHOULD have clear acceptance criteria (what makes it "done")
        - Prefer 10-20 meaningful tasks over 50 micro-edits
        - Avoid tasks that only rename/reformat without functional change

        **If you cannot produce {max_workers_limit}+ root tasks without junk tasks:**
        Produce fewer tasks but ensure root_ready >= {max_workers_limit} by splitting along meaningful seams.

        Please regenerate the plan with these requirements. Output ONLY the corrected JSON.
    """).strip()

    return base_prompt


def _build_task_plan_prompt(
    *,
    design_doc_text: str,
    milestone_id: str,
    max_workers_limit: int,
    cpu_threshold_pct_total: float,
    mem_threshold_pct_total: float,
    machine_info: dict[str, Any],
    planner_profile: str = "balanced",
) -> str:
    design_excerpt = _truncate(design_doc_text, 40000)  # Increased for multi-doc support
    cores = machine_info.get("cpu_cores")
    ram_b = machine_info.get("ram_bytes")
    ram_gb = None
    if isinstance(ram_b, int) and ram_b > 0:
        ram_gb = round(ram_b / (1024**3), 1)

    # Detect all milestones in the document
    all_milestones = _parse_all_milestones(design_doc_text)
    is_multi_milestone = len(all_milestones) > 1

    if is_multi_milestone:
        milestone_instruction = f"""
MULTI-MILESTONE MODE: The design document contains {len(all_milestones)} milestone documents: {", ".join(all_milestones)}.

You MUST:
1. Create tasks for ALL milestones, not just one.
2. Prefix each task ID with its milestone (e.g., M1-DSL-SCHEMA, M2-OPENEMS-SETUP, M3-ARTIFACT-STORE).
3. Include the milestone prefix in lock keys to avoid collisions (e.g., M1-dsl, M2-openems, M3-manifest).
4. Dependencies across milestones are allowed if later milestones explicitly require earlier ones.
5. Set milestone_id to "MULTI" in your output.

For each milestone in the document, identify and create the necessary tasks."""
    else:
        milestone_instruction = f"Use milestone_id exactly: {milestone_id}"

    # Check agent policy for forced mode
    policy = get_agent_policy()
    if policy.forced_agent:
        planner_role = policy.forced_agent.upper()
        agent_assignment_instruction = f"""
        Agent assignment (POLICY OVERRIDE ACTIVE):
        - **--only-{policy.forced_agent}** flag is active.
        - You MUST assign preferred_agent as "{policy.forced_agent}" for ALL tasks.
        - Do NOT assign any tasks to the other agent - they will be overridden anyway.
"""
    else:
        planner_role = "CODEX"
        agent_assignment_instruction = """
        Agent assignment (CRITICAL):
        - You MUST assign preferred_agent as ONLY "codex" or "claude". NEVER use "either".
        - At least 30-40% of tasks MUST be assigned to "claude".
        - Recommended assignment heuristics:
          * Claude is better for: schemas, documentation, code review, edge-case analysis, test writing, refactoring, API design
          * Codex is better for: heavy implementation, low-level code, CLI tools, integration work, file I/O, build systems
        - If unsure, prefer Claude for design/spec tasks and Codex for implementation tasks.
"""

    header = textwrap.dedent(
        f"""
        You are {planner_role} acting as a **task planner** for a parallel agent runner.

        Your job:
        - Read the design document below.
        - Split the work into a set of independent tasks that can be executed in parallel safely.
        - Output ONLY a single JSON object that matches the provided JSON schema.

        Hardware context:
        - CPU cores: {cores}
        - RAM (GB): {ram_gb if ram_gb is not None else "unknown"}

        Parallel safety policy:
        - Tasks that touch the same subsystem or files should share a lock key in `locks`.
        - If a task might require a heavy local command (full test suite, large build, large format/lint, GPU/ML workloads), set `solo: true` and `intensity: "high"`.
        - The runner will automatically stop any single task if it uses > {cpu_threshold_pct_total:.1f}% CPU or > {mem_threshold_pct_total:.1f}% RAM for multiple samples.

        Concurrency:
        - Choose `max_parallel_tasks` <= {max_workers_limit}.
        - Prefer fewer parallel tasks if you believe tasks are likely to conflict or require heavy commands.

        Shared integration files policy (CRITICAL for avoiding merge conflicts):
        - These files are HIGH CONFLICT RISK and should be edited by AT MOST ONE task:
          * src/formula_foundry/*/\\_\\_init\\_\\_.py (package exports)
          * pyproject.toml (dependencies, config)
        - If multiple tasks need to add exports to __init__.py:
          * DO NOT have each task edit __init__.py directly
          * Instead, have ONE dedicated task (e.g., "API-EXPORTS" or "INTEGRATE-EXPORTS") that:
            - Runs LAST (depends_on all implementation tasks)
            - Collects all new exports and updates __init__.py once
          * Implementation tasks should focus on creating modules, not updating package exports
        - Use lock keys to prevent conflicts: e.g., locks: ["init-m4"], locks: ["pyproject"]
{agent_assignment_instruction}
        {milestone_instruction}
        """
    ).strip()

    # Add THROUGHPUT MODE block if enabled
    throughput_block = ""
    if planner_profile == "throughput":
        throughput_block = textwrap.dedent(f"""
            ---

            # THROUGHPUT MODE (ENABLED) - HIGH-OUTPUT, HIGH-QUALITY

            You are in **throughput mode** - maximize parallel worker utilization while maintaining quality.
            This is NOT "microtask spam" mode. Each task must produce meaningful progress.

            **MANDATORY REQUIREMENTS:**

            1. **Set max_parallel_tasks = {max_workers_limit}** (the allowed limit).

            2. **Create at least {max_workers_limit} root tasks** (tasks with `depends_on: []`) that can start immediately.
               - Workers idle when there are no ready tasks. More roots = faster start.

            3. **Target {max_workers_limit * 2}+ total tasks** to keep the worker pipeline busy.
               - This is a soft target. If you cannot produce this many meaningful tasks, produce fewer.

            4. **Use DISTINCT locks per file/module:**
               - BAD: 8 tasks all using `locks: ["init-m4"]` (serial execution = no parallelism)
               - GOOD: Tasks using `locks: ["m4-types"]`, `locks: ["m4-io"]`, `locks: ["m4-preprocess"]` (true parallelism)
               - Do NOT share a lock across more than 2-3 tasks unless truly necessary.

            5. **Prefer 10-20 meaningful tasks over 50 micro-edits:**
               - Each task should produce a testable unit of work (feature, function, module, test file).
               - Avoid tasks that only rename/reformat without functional change.
               - Avoid tasks that are too vague ("implement subsystem") or too tiny ("add import line").

            **TASK QUALITY REQUIREMENTS:**

            Each task description MUST include:

            6. **Files/modules touched** (or lock key):
               - Example: "Files: src/formula_foundry/m4/types.py, tests/test_m4_types.py"
               - This helps validate lock correctness.

            7. **Acceptance criteria** (what makes the task "done"):
               - Example: "Done when: NetworkConfig dataclass exists with fields for host, port, timeout; unit tests pass"
               - Must be concrete and verifiable.

            8. **Why this task is valuable** (1 sentence):
               - Example: "Provides typed configuration for network connections, enabling type-safe API calls"

            **INTEGRATION TASK PATTERN:**

            If a shared integration file (e.g., `__init__.py`) must be touched by multiple features:
            - Create ONE "INTEGRATION" or "API-EXPORTS" task that runs LAST (`depends_on` all impl tasks)
            - Implementation tasks should create modules WITHOUT updating package exports
            - The integration task collects all exports and updates `__init__.py` once

            **ANTI-PATTERNS TO AVOID:**

            - Splitting into tasks that only rename/reformat unless paired with a functional change
            - Creating 50 tiny tasks just to hit a task count target
            - Putting 8+ tasks behind the same lock (kills parallelism)
            - Vague descriptions like "implement subsystem" or "do M4 work"

            **EXAMPLE of a good throughput plan for {max_workers_limit} workers:**
            - {max_workers_limit}+ root tasks (all can start immediately)
            - {max_workers_limit * 2}+ total tasks (with meaningful scope each)
            - max_parallel_tasks = {max_workers_limit}
            - Each task names specific files, has clear acceptance criteria
            - Uses DISTINCT lock keys: "m4-types", "m4-io-reader", "m4-io-writer", "m5-eval", etc.

            **If you cannot produce {max_workers_limit}+ root tasks without junk tasks:**
            Produce fewer tasks but ensure root_ready >= {max_workers_limit} by splitting along meaningful seams (modules, test files, features).
        """).strip()

    parts = [header]
    if throughput_block:
        parts.append(throughput_block)
    parts.append("---\n\n# Design Document\n" + design_excerpt)

    return "\n\n".join(parts)


def _format_task_packet_for_parallel(task: ParallelTask) -> str:
    """Build a structured Task Packet for engineering mode prompts."""

    def _fmt_list(items: list[str]) -> str:
        if not items:
            return "(none specified)"
        return ", ".join(items)

    lines = [
        f"Objective: {task.title}",
        f"Deliverables (from plan): {task.description.strip()}",
        "Done criteria: Implement the task end-to-end, update tests/docs as needed, and set work_completed=true.",
        f"Relevant files (declared): {_fmt_list([str(p) for p in task.touched_paths])}",
        f"Locks: {_fmt_list([str(l) for l in task.locks])}",
        f"Dependencies: {_fmt_list([str(d) for d in task.depends_on])}",
        "Constraints: Follow AGENTS.md. No placeholders/TODO-only. Do not delegate unless explicitly required.",
        "Suggested verification: Run targeted checks for touched files; if heavy, provide the exact command in summary.",
    ]
    return "\n".join(f"- {line}" for line in lines)


def _format_task_packet_for_sequential(next_prompt: str) -> str:
    """Build a Task Packet for sequential engineering mode prompts."""
    objective = next_prompt.strip() or "(decide the next best step)"
    lines = [
        f"Objective: {objective}",
        "Deliverables: Complete the objective end-to-end with code + tests where applicable.",
        "Done criteria: Work is implemented, verification commands are noted, summary includes a Work Report.",
        "Relevant files: Identify and list in the Work Report.",
        "Constraints: Follow AGENTS.md. No placeholders/TODO-only. Do not delegate unless required.",
        "Suggested verification: Run the smallest relevant tests or provide exact commands.",
    ]
    return "\n".join(f"- {line}" for line in lines)


def _build_parallel_task_prompt(
    *,
    system_prompt: str,
    task: ParallelTask,
    worker_id: int,
    milestone_id: str,
    repo_snapshot: str,
    design_doc_text: str,
    resource_policy: dict[str, Any],
    planner_profile: str,
) -> str:
    design_excerpt = _truncate(design_doc_text, 18000)

    state_blob = {
        "runner_mode": "parallel-worker",
        "worker_id": worker_id,
        "milestone_id": milestone_id,
        "planner_profile": planner_profile,
        "task": {
            "id": task.id,
            "title": task.title,
            "intensity": task.intensity,
            "solo": task.solo,
            "locks": task.locks,
            "depends_on": task.depends_on,
        },
        "resource_policy": resource_policy,
    }

    work_report_req = ""
    if planner_profile == "engineering":
        work_report_req = textwrap.dedent(
            """
            - Your summary MUST include a "Work Report" section with:
              * Commands run (or "none")
              * Files changed (or "no changes")
              * Tests run (or "not run")
              * Blockers/next steps if any
            """
        ).strip()

    instructions = textwrap.dedent(
        f"""
        # Parallel Worker Instructions

        You are worker {worker_id} executing task {task.id}: {task.title}

        Do:
        - Implement ONLY this task.
        - Keep changes focused.
        - Make the repo consistent and runnable.
        - DO NOT run git add or git commit - the orchestrator handles commits.

        Resource safety:
        - Avoid running heavy commands while other agents may be running.
        - If the next step requires a heavy command, DO NOT run it.
          Instead, explain in your summary and provide the exact shell command the user can run later.

        Output:
        - Output ONLY a single JSON object matching the schema.
        - In parallel-worker mode you may set next_agent to yourself and next_prompt to an empty string.
        {work_report_req}
        """
    ).strip()

    # Get agent policy header if in forced mode
    policy_header = get_agent_policy().get_prompt_header()

    parts = [system_prompt.strip()]

    # Inject policy header right after system prompt if in forced mode
    if policy_header:
        parts.append(policy_header.strip())

    parts.extend(
        [
            "---\n\n# Orchestrator State\n" + json.dumps(state_blob, indent=2),
        ]
    )

    if planner_profile == "engineering":
        task_packet = _format_task_packet_for_parallel(task)
        parts.append("---\n\n# Task Packet\n" + task_packet)

    parts.extend(
        [
            "---\n\n# Task\n" + task.description.strip(),
            "---\n\n# Repo Snapshot\n" + repo_snapshot.strip(),
            "---\n\n# Design Doc (truncated)\n" + design_excerpt.strip(),
            "---\n\n" + instructions,
        ]
    )

    return "\n\n".join(parts)


NON_PROGRESS_SUMMARY_PATTERNS = [
    "wrapper_status",
    "auth",
    "authentication",
    "api key",
    "quota",
    "rate limit",
    "tools are disabled",
    "tools disabled",
    "cannot use tools",
    "no changes",
    "nothing to commit",
    "planning only",
    "plan only",
    "no-op",
]

NON_PROGRESS_ACTION_KEYWORDS = [
    "implement",
    "implemented",
    "add",
    "added",
    "update",
    "updated",
    "fix",
    "fixed",
    "remove",
    "removed",
    "create",
    "created",
    "refactor",
    "refactored",
    "modify",
    "modified",
    "test",
    "tests",
    "ran",
    "run",
    "executed",
    "command",
    "patch",
    "diff",
    "changed",
]


def _normalize_commands(commands: list[str]) -> list[str]:
    cleaned = []
    for cmd in commands:
        if cmd is None:
            continue
        text = str(cmd).strip()
        if not text:
            continue
        if text.lower() in {"n/a", "none", "no commands", "noop", "-", "null"}:
            continue
        cleaned.append(text)
    return cleaned


def _summary_is_generic(summary: str) -> bool:
    text = summary.strip()
    if not text:
        return True
    lower = " ".join(text.lower().split())
    has_action = any(word in lower for word in NON_PROGRESS_ACTION_KEYWORDS)
    if any(pat in lower for pat in NON_PROGRESS_SUMMARY_PATTERNS) and not has_action:
        return True
    return bool(len(lower) < 80 and not has_action)


def _detect_non_progress_turn(turn_obj: dict[str, Any], has_changes: bool) -> tuple[bool, str]:
    if not isinstance(turn_obj, dict):
        return False, ""
    if bool(turn_obj.get("work_completed", False)):
        return False, ""
    if has_changes:
        return False, ""

    requirement_progress = turn_obj.get("requirement_progress", {}) or {}
    commands = _normalize_commands(requirement_progress.get("commands_run", []) or [])
    summary = str(turn_obj.get("summary", "") or "")

    if not commands and _summary_is_generic(summary):
        return True, "work_completed=false with no commands and generic summary"
    return False, ""


def _max_retries_for_profile(planner_profile: str) -> int:
    return 0 if planner_profile == "engineering" else 2


def _should_self_heal_task(task: ParallelTask, planner_profile: str) -> tuple[bool, str]:
    """Return (allowed, reason) for self-heal reruns."""
    if planner_profile != "engineering":
        return True, "default_profile"
    error_lower = (task.error or "").lower()
    if task.status == "resource_killed" or "stopped for resources" in error_lower:
        return True, "resource_killed"
    if "exit code 137" in error_lower or "killed" in error_lower:
        return True, "process_killed"
    if "timeout" in error_lower or "timed out" in error_lower:
        return True, "timeout"
    if "lock" in error_lower and ("contention" in error_lower or "lock" in error_lower):
        return True, "lock_contention"
    return False, "non_infra_failure"


def _select_only_tasks(all_tasks: list[ParallelTask], only_ids: list[str]) -> list[ParallelTask]:
    if not only_ids:
        return all_tasks

    ids = {s.strip() for s in only_ids if s.strip()}
    by_id = {t.id: t for t in all_tasks}

    # Include dependencies recursively.
    keep = set()

    def dfs(tid: str) -> None:
        if tid in keep:
            return
        keep.add(tid)
        t = by_id.get(tid)
        if not t:
            return
        for dep in t.depends_on:
            dfs(dep)

    for tid in list(ids):
        dfs(tid)

    return [t for t in all_tasks if t.id in keep]


def _is_backfill_task_id(task_id: str) -> bool:
    """Check if a task ID represents an optional backfill task (FILLER-* prefix)."""
    return task_id.startswith("FILLER-")


def _write_manual_task_file(
    *,
    manual_dir: Path,
    task: ParallelTask,
    reason: str,
    agent_cmd: list[str],
    schema_path: Path,
    prompt_path: Path,
    out_path: Path,
    raw_log_path: Path,
) -> Path:
    _ensure_dir(manual_dir)
    path = manual_dir / f"manual_{_sanitize_branch_fragment(task.id)}.md"

    cmd_str = " ".join(agent_cmd)
    content = f"""# Manual Run Required: {task.id} — {task.title}

This task was stopped or needs manual intervention.

Reason:
- {reason}

Worker:
- {task.worker_id}

Artifacts:
- Prompt: {prompt_path}
- Schema: {schema_path}
- Intended output JSON: {out_path}
- Raw log: {raw_log_path}

Suggested manual rerun (run when no other agents are running):

```bash
cd {task.worktree_path}
export WRITE_ACCESS=1
export ORCH_WRITE_ACCESS=1
{cmd_str}
```

If you prefer rerunning through the orchestrator (single-worker):

```bash
./run_parallel.sh --only-task {task.id} --max-workers 1 --allow-resource-intensive
```
"""

    path.write_text(content, encoding="utf-8")
    return path


def _mark_task_manual(
    *,
    task: ParallelTask,
    reason: str,
    manual_dir: Path,
    schema_path: Path,
) -> None:
    """Mark a task as manual and ensure manual_path is written.

    Sets task.status to 'manual', task.error to the reason, and if task.manual_path
    is None, writes a manual file using _write_manual_task_file.
    """
    task.status = "manual"
    task.error = reason
    if task.manual_path is None:
        task.manual_path = _write_manual_task_file(
            manual_dir=manual_dir,
            task=task,
            reason=reason,
            agent_cmd=[],
            schema_path=schema_path,
            prompt_path=task.prompt_path or Path(""),
            out_path=task.out_path or Path(""),
            raw_log_path=task.raw_log_path or Path(""),
        )


def _generate_run_summary(
    *,
    tasks: list[ParallelTask],
    runs_dir: Path,
    verify_exit_code: int,
    planner_profile: str = DEFAULT_PLANNER_PROFILE,
) -> dict[str, Any]:
    """Generate a structured summary of the parallel run.

    Backfill tasks (FILLER-* prefix) are optional and do NOT cause run failure.
    They are tracked in 'optional_tasks' for visibility but excluded from
    root_failures, blocked_tasks, and success calculation.
    """
    root_failures = []
    blocked_tasks = []
    completed_tasks = []
    pending_rerun_tasks = []
    optional_tasks = []

    # Root failure statuses: tasks that failed on their own (not due to dependencies)
    root_failure_statuses = ("failed", "manual", "pending_rerun", "resource_killed")

    for t in tasks:
        task_info = {
            "id": t.id,
            "title": t.title,
            "status": t.status,
            "agent": t.agent,
            "work_completed": t.work_completed,
            "commit_sha": t.commit_sha,
            "error": t.error,
            "manual_path": str(t.manual_path) if t.manual_path else None,
            "raw_log_path": str(t.raw_log_path) if t.raw_log_path else None,
            "turn_summary": t.turn_summary,
        }

        is_backfill = _is_backfill_task_id(t.id)

        # Backfill tasks are optional - track separately for visibility
        if is_backfill:
            optional_tasks.append(task_info)
            # Still count done backfills as completed for stats
            if t.status == "done":
                completed_tasks.append(task_info)
            continue

        # Non-backfill task processing
        if t.status == "done":
            completed_tasks.append(task_info)
        elif t.status == "pending_rerun":
            pending_rerun_tasks.append(task_info)
            root_failures.append(task_info)
        elif t.status in root_failure_statuses:
            root_failures.append(task_info)
        elif t.status in ("blocked", "skipped"):
            # Both "blocked" (new) and "skipped" (legacy) are transitively blocked
            blocked_tasks.append(task_info)

    # Count non-backfill tasks only for failure stats
    non_backfill_tasks = [t for t in tasks if not _is_backfill_task_id(t.id)]
    failed_count = len([t for t in non_backfill_tasks if t.status in ("failed", "manual", "resource_killed")])
    pending_rerun_count = len([t for t in non_backfill_tasks if t.status == "pending_rerun"])
    blocked_count = len([t for t in non_backfill_tasks if t.status in ("blocked", "skipped")])

    summary = {
        "run_dir": str(runs_dir),
        "planner_profile": planner_profile,
        "total_tasks": len(tasks),
        "completed": len(completed_tasks),
        "failed": failed_count,
        "pending_rerun": pending_rerun_count,
        "blocked": blocked_count,
        "verify_exit_code": verify_exit_code,
        "success": verify_exit_code == 0 and len(root_failures) == 0,
        "completed_tasks": completed_tasks,
        "root_failures": root_failures,
        "blocked_tasks": blocked_tasks,
        "optional_tasks": optional_tasks,
    }

    return summary


def _generate_continuation_prompt(
    *,
    summary: dict[str, Any],
    tasks: list[ParallelTask],
    design_doc_text: str,
    runs_dir: Path,
) -> str:
    """Generate a continuation prompt for the next run based on failures."""
    lines = [
        "# Continuation Run - Fix Failures and Complete Remaining Work",
        "",
        "The previous parallel run did not complete all tasks successfully.",
        "",
        "## Previous Run Summary",
        f"- Completed: {summary['completed']}/{summary['total_tasks']}",
        f"- Failed: {summary['failed']}",
        f"- Pending Rerun (planning-only): {summary['pending_rerun']}",
        f"- Blocked: {summary['blocked']}",
        f"- Verify exit code: {summary['verify_exit_code']}",
        "",
    ]

    if summary["root_failures"]:
        lines.append("## Root Failures (must be fixed)")
        lines.append("")
        for t in summary["root_failures"]:
            lines.append(f"### {t['id']}: {t['title']}")
            lines.append(f"- Status: {t['status']}")
            lines.append(f"- Agent: {t['agent']}")
            if t["error"]:
                lines.append(f"- Error: {t['error']}")
            if t["turn_summary"]:
                lines.append(f"- Last summary: {t['turn_summary'][:300]}")
            if t["manual_path"]:
                lines.append(f"- Manual file: {t['manual_path']}")
            if t["raw_log_path"]:
                # Include tail of log
                try:
                    log_path = Path(t["raw_log_path"])
                    if log_path.exists():
                        log_text = log_path.read_text(encoding="utf-8", errors="replace")
                        tail = log_text[-2000:] if len(log_text) > 2000 else log_text
                        lines.append(f"- Log tail:\n```\n{tail}\n```")
                except Exception:
                    pass
            lines.append("")

    if summary["blocked_tasks"]:
        lines.append("## Blocked Tasks (will unblock once failures are fixed)")
        lines.append("")
        for t in summary["blocked_tasks"]:
            lines.append(f"- {t['id']}: {t['error'] or 'blocked by prerequisites'}")
        lines.append("")

    if summary["completed_tasks"]:
        lines.append("## Completed Tasks (DO NOT re-implement)")
        lines.append("")
        for t in summary["completed_tasks"]:
            lines.append(f"- {t['id']}: {t['title']}")
        lines.append("")

    lines.extend(
        [
            "## Instructions for This Run",
            "",
            "1. **Focus ONLY on root failures** - do not re-implement completed tasks",
            "2. For tasks marked 'pending_rerun', the agent only produced a plan - now IMPLEMENT the actual code",
            "3. For failed tasks, analyze the error and fix the issue",
            "4. After fixing root failures, blocked tasks will automatically become runnable",
            "5. Produce a new task plan that includes ONLY:",
            "   - Tasks from root_failures that need to be fixed/implemented",
            "   - Any blocked tasks that will become runnable",
            "",
            "IMPORTANT: Do NOT include completed tasks in the new plan.",
            "",
        ]
    )

    return "\n".join(lines)


def _compute_transitive_blocked(
    tasks: list[ParallelTask],
    root_failure_ids: set,
) -> set:
    """Compute the transitive closure of all tasks blocked by root failures.

    A task is blocked if:
    - It depends directly on a root failure
    - It depends on another blocked task (transitive)

    Returns the set of task IDs that are transitively blocked.
    """
    {t.id: t for t in tasks}
    blocked_ids: set = set()

    # Build reverse dependency graph
    dependents: dict[str, list[str]] = {t.id: [] for t in tasks}
    for t in tasks:
        for dep in t.depends_on:
            if dep in dependents:
                dependents[dep].append(t.id)

    # BFS from root failures to find all transitively blocked tasks
    queue_ids = list(root_failure_ids)
    visited = set(root_failure_ids)

    while queue_ids:
        current_id = queue_ids.pop(0)
        # All tasks that depend on this one are blocked
        for dependent_id in dependents.get(current_id, []):
            if dependent_id not in visited:
                visited.add(dependent_id)
                blocked_ids.add(dependent_id)
                queue_ids.append(dependent_id)

    return blocked_ids


def _build_implement_now_prompt(
    *,
    task: ParallelTask,
    worker_id: int,
    milestone_id: str,
    repo_snapshot: str,
    previous_summary: str,
) -> str:
    """Build a focused "IMPLEMENT NOW" prompt for plan-only retry.

    This is intentionally smaller than the full design doc to avoid context bloat.
    """
    return textwrap.dedent(f"""
        # CRITICAL: IMPLEMENT NOW - DO NOT PLAN

        You are worker {worker_id} retrying task {task.id}: {task.title}

        YOUR PREVIOUS ATTEMPT ONLY PRODUCED A PLAN. THIS IS NOT ACCEPTABLE.

        ## Task Description
        {task.description}

        ## Your Previous Summary
        {previous_summary}

        ## Current Repo State
        {repo_snapshot}

        ## REQUIREMENTS
        1. You MUST write actual code and commit changes
        2. You MUST set work_completed=true in your response
        3. DO NOT just describe what you would do - ACTUALLY DO IT
        4. If you cannot complete the task, explain WHY in your error field

        ## Output Schema Reminder
        Your response MUST be a JSON object with:
        - agent: "{task.agent}"
        - milestone_id: "{milestone_id}"
        - phase: "implement"
        - work_completed: true (REQUIRED - you must complete work!)
        - project_complete: false
        - summary: "what you actually implemented"
        - gates_passed: []
        - requirement_progress: {{"covered_req_ids": [], "tests_added_or_modified": [], "commands_run": []}}
        - next_agent: "{task.agent}"
        - next_prompt: ""
        - delegate_rationale: "completed task"
        - stats_refs: ["CL-1"] or ["CX-1"]
        - needs_write_access: true
        - artifacts: [list of files created/modified]

        NOW IMPLEMENT THE TASK. NO MORE PLANNING.
    """).strip()


def _run_parallel_task(
    *,
    task: ParallelTask,
    worker_id: int,
    state: RunState,
    config: RunConfig,
    milestone_id: str,
    design_doc_text: str,
    system_prompt: str,
    planner_profile: str,
    stats_id_set: set,
    tasks_dir: Path,
    worktrees_dir: Path,
    manual_dir: Path,
    git_lock: threading.Lock,
    cpu_threshold_pct_total: float,
    mem_threshold_pct_total: float,
    terminal_max_bytes: int,
    terminal_max_line_length: int,
    allow_resource_intensive: bool,
) -> None:
    task.worker_id = worker_id
    task.task_dir = tasks_dir / _sanitize_branch_fragment(task.id)
    _ensure_dir(task.task_dir)

    # Create a fresh worktree/branch for this task (with idempotent cleanup).
    with git_lock:
        rc, base_sha_out, err = _run_cmd(["git", "rev-parse", "HEAD"], cwd=state.project_root, env=os.environ.copy())
        if rc != 0:
            task.status = "failed"
            task.error = (base_sha_out + "\n" + err).strip()
            return
        base_sha = base_sha_out.strip()
        task.base_sha = base_sha

        branch = f"task/{state.run_id}/{_sanitize_branch_fragment(task.id)}"
        task.branch = branch

        worktree_path = worktrees_dir / f"w{worker_id:02d}_{_sanitize_branch_fragment(task.id)}"
        task.worktree_path = worktree_path

        # IDEMPOTENT WORKTREE SETUP: Handle branch/worktree collisions robustly
        # This fixes the "branch already exists" error that blocked Jan 26 runs.

        # Step 1: Check if branch already exists
        rc_ref, _, _ = _run_cmd(
            ["git", "show-ref", "--verify", "--quiet", f"refs/heads/{branch}"],
            cwd=state.project_root,
            env=os.environ.copy(),
        )
        branch_exists = rc_ref == 0

        if branch_exists:
            print(f"[orchestrator] CLEANUP: Branch '{branch}' already exists, cleaning up...")

            # Step 2: Check if branch is checked out in a worktree
            rc_wt, wt_out, _ = _run_cmd(
                ["git", "worktree", "list", "--porcelain"],
                cwd=state.project_root,
                env=os.environ.copy(),
            )
            if rc_wt == 0:
                # Parse worktree list to find if our branch is checked out
                current_wt_path = None
                for line in wt_out.strip().split("\n"):
                    if line.startswith("worktree "):
                        current_wt_path = line.split(" ", 1)[1]
                    elif line.startswith("branch ") and line.endswith(f"/{branch}"):
                        # Found worktree with our branch - remove it
                        if current_wt_path:
                            print(f"[orchestrator] CLEANUP: Removing worktree at {current_wt_path}")
                            _run_cmd(
                                ["git", "worktree", "remove", "--force", current_wt_path],
                                cwd=state.project_root,
                                env=os.environ.copy(),
                            )
                        break

            # Step 3: Prune stale worktree references
            _run_cmd(["git", "worktree", "prune"], cwd=state.project_root, env=os.environ.copy())

            # Step 4: Delete the branch (now safe since not checked out)
            rc_del, _, del_err = _run_cmd(
                ["git", "branch", "-D", branch],
                cwd=state.project_root,
                env=os.environ.copy(),
            )
            if rc_del != 0:
                print(f"[orchestrator] CLEANUP: Could not delete branch '{branch}': {del_err}")
                # Try force-deleting by name
                _run_cmd(
                    ["git", "update-ref", "-d", f"refs/heads/{branch}"],
                    cwd=state.project_root,
                    env=os.environ.copy(),
                )

        # Step 5: Handle worktree path collision
        if worktree_path.exists():
            print(f"[orchestrator] CLEANUP: Worktree path {worktree_path} exists, removing...")
            # Try git worktree remove first
            _run_cmd(
                ["git", "worktree", "remove", "--force", str(worktree_path)],
                cwd=state.project_root,
                env=os.environ.copy(),
            )
            # If directory still exists, remove it
            if worktree_path.exists():
                shutil.rmtree(worktree_path, ignore_errors=True)
            # Prune again
            _run_cmd(["git", "worktree", "prune"], cwd=state.project_root, env=os.environ.copy())

        # Step 6: Create fresh worktree with new branch
        rc2, out2, err2 = _run_cmd(
            ["git", "worktree", "add", "-b", branch, str(worktree_path), base_sha],
            cwd=state.project_root,
            env=os.environ.copy(),
        )
        if rc2 != 0:
            task.status = "failed"
            task.error = (out2 + "\n" + err2).strip()
            return

    # Agent execution with retry loop for plan-only responses
    agent = task.agent if task.agent in AGENTS else "codex"
    script_rel = config.agent_scripts.get(agent, "")
    script_path = (task.worktree_path / script_rel) if script_rel else None
    if not script_path or not script_path.exists():
        task.status = "failed"
        task.error = f"Agent script not found: {script_path}"
        return

    schema_path = state.schema_path
    env = os.environ.copy()
    env["WRITE_ACCESS"] = "1"
    env["ORCH_WRITE_ACCESS"] = "1"
    env["FF_WORKER_ID"] = str(worker_id)
    # Signal to the agent wrapper that this is a turn schema (enables turn normalization)
    env["ORCH_SCHEMA_KIND"] = "turn"

    # Disable GPU by default in parallel mode unless explicitly allowed.
    if config.parallel.disable_gpu_by_default and env.get("FF_ALLOW_GPU") != "1":
        env.setdefault("CUDA_VISIBLE_DEVICES", "")

    prefix = f"[w{worker_id:02d} {agent} {task.id}]"

    # Retry loop: try up to max_retries+1 times (initial + retries)
    while True:
        attempt = task.retry_count + 1
        attempt_suffix = f"_attempt{attempt}" if task.retry_count > 0 else ""

        # Build prompt (use "IMPLEMENT NOW" prompt for retries)
        repo_snapshot = _git_snapshot(task.worktree_path)

        if task.retry_count == 0:
            # First attempt: use full design doc prompt
            resource_policy = {
                "cpu_threshold_pct_total": cpu_threshold_pct_total,
                "mem_threshold_pct_total": mem_threshold_pct_total,
                "resource_intensive_definition": "> 40% CPU or RAM",
                "allow_resource_intensive": bool(allow_resource_intensive),
            }
            prompt_text = _build_parallel_task_prompt(
                system_prompt=system_prompt,
                task=task,
                worker_id=worker_id,
                milestone_id=milestone_id,
                repo_snapshot=repo_snapshot,
                design_doc_text=design_doc_text,
                resource_policy=resource_policy,
                planner_profile=planner_profile,
            )
        else:
            # Retry: use focused "IMPLEMENT NOW" prompt
            print(f"[orchestrator] {task.id}: RETRY {task.retry_count}/{task.max_retries} with IMPLEMENT NOW prompt")
            prompt_text = _build_implement_now_prompt(
                task=task,
                worker_id=worker_id,
                milestone_id=milestone_id,
                repo_snapshot=repo_snapshot,
                previous_summary=task.turn_summary or "(no summary)",
            )

        prompt_path = task.task_dir / f"prompt{attempt_suffix}.txt"
        prompt_path.write_text(prompt_text, encoding="utf-8")
        task.prompt_path = prompt_path

        out_path = task.task_dir / f"turn{attempt_suffix}.json"
        raw_log_path = task.task_dir / f"raw{attempt_suffix}.log"
        task.out_path = out_path
        task.raw_log_path = raw_log_path

        cmd = [str(script_path), str(prompt_path), str(schema_path), str(out_path)]

        res = _run_cmd_monitored(
            cmd,
            cwd=task.worktree_path,
            env=env,
            prefix=prefix,
            raw_log_path=raw_log_path,
            stream_to_terminal=True,
            terminal_max_bytes=terminal_max_bytes,
            terminal_max_line_length=terminal_max_line_length,
            cpu_threshold_pct_total=cpu_threshold_pct_total,
            mem_threshold_pct_total=mem_threshold_pct_total,
            sample_interval_s=config.parallel.sample_interval_s,
            consecutive_samples=config.parallel.consecutive_samples,
            kill_grace_s=config.parallel.kill_grace_s,
            allow_resource_intensive=allow_resource_intensive,
        )

        task.max_cpu_pct_total = max(task.max_cpu_pct_total, res.max_cpu_pct_total)
        task.max_mem_pct_total = max(task.max_mem_pct_total, res.max_mem_pct_total)

        if res.killed_for_resources:
            task.status = "resource_killed"
            task.error = f"Stopped for resources: {res.kill_reason}"
            task.manual_path = _write_manual_task_file(
                manual_dir=manual_dir,
                task=task,
                reason=task.error,
                agent_cmd=cmd,
                schema_path=schema_path,
                prompt_path=prompt_path,
                out_path=out_path,
                raw_log_path=raw_log_path,
            )
            return

        if res.returncode != 0:
            task.status = "failed"
            task.error = f"Agent exit code {res.returncode}"
            task.manual_path = _write_manual_task_file(
                manual_dir=manual_dir,
                task=task,
                reason=task.error,
                agent_cmd=cmd,
                schema_path=schema_path,
                prompt_path=prompt_path,
                out_path=out_path,
                raw_log_path=raw_log_path,
            )
            return

        # Validate JSON output with auto-recovery for bad output
        # Track correction attempts for this specific agent call
        if not hasattr(task, "_json_correction_count"):
            task._json_correction_count = 0
        if not hasattr(task, "_agent_fallback_used"):
            task._agent_fallback_used = False

        validation_error: str | None = None
        turn_obj: dict | None = None
        normalization_warnings: list[str] = []

        if not out_path.exists():
            validation_error = "Agent did not produce output file"
        else:
            # Validate file is not 0-bytes (critical robustness check)
            is_valid, turn_data, file_error = validate_json_file(out_path)
            if not is_valid:
                if "empty (0 bytes)" in (file_error or ""):
                    print(f"[orchestrator] {task.id}: CRITICAL: 0-byte turn.json detected, triggering retry")
                    validation_error = "0-byte output file - agent output was not written correctly"
                else:
                    validation_error = f"Invalid turn.json: {file_error}"
            else:
                turn_text = out_path.read_text(encoding="utf-8")

                # Use TurnNormalizer for robust payload extraction and invariant override
                norm_result = normalize_agent_output(
                    turn_text,
                    expected_agent=agent,
                    expected_milestone_id=milestone_id,
                    stats_id_set=stats_id_set,
                    default_phase="implement",
                )

                if norm_result.success and norm_result.turn:
                    turn_obj = norm_result.turn
                    normalization_warnings = norm_result.warnings
                    # Log normalization warnings (auto-corrections)
                    for warning in normalization_warnings:
                        print(f"[orchestrator] {task.id}: NORMALIZED: {warning}")

                    # Use lenient validation (auto-corrects mismatches with warnings)
                    ok, msg, val_warnings = _validate_turn_lenient(
                        turn_obj,
                        expected_agent=agent,
                        expected_milestone_id=milestone_id,
                        stats_id_set=stats_id_set,
                    )
                    for warning in val_warnings:
                        print(f"[orchestrator] {task.id}: VALIDATION: {warning}")
                    if not ok:
                        validation_error = f"Invalid JSON output after normalization: {msg}"
                    else:
                        # Write normalized turn atomically to ensure file integrity
                        try:
                            atomic_write_json(out_path, turn_obj, indent=2)
                            print(f"[orchestrator] {task.id}: turn.json written atomically")
                        except Exception as e:
                            print(f"[orchestrator] {task.id}: WARNING: atomic write failed: {e}")
                else:
                    # Normalization failed - fall back to direct parsing
                    turn_obj = _try_parse_json(turn_text)
                    if turn_obj is None:
                        validation_error = f"Cannot extract JSON payload: {norm_result.error or 'unknown error'}"
                    else:
                        # Got JSON but didn't pass normalizer - try lenient validation directly
                        ok, msg, val_warnings = _validate_turn_lenient(
                            turn_obj,
                            expected_agent=agent,
                            expected_milestone_id=milestone_id,
                            stats_id_set=stats_id_set,
                        )
                        for warning in val_warnings:
                            print(f"[orchestrator] {task.id}: VALIDATION: {warning}")
                        if not ok:
                            validation_error = f"Invalid JSON output: {msg}"

        # Auto-recovery: reprompt on validation error
        if validation_error:
            max_json_corrections = config.max_json_correction_attempts
            task._json_correction_count += 1

            # Check for noncompliant output patterns (tools disabled, markdown, etc.)
            raw_text = ""
            noncompliant_violations: list[str] = []
            has_critical_tool_violation = False
            if out_path.exists():
                try:
                    raw_text = out_path.read_text(encoding="utf-8")
                    _, noncompliant_violations, has_critical_tool_violation = _is_noncompliant_and_should_use_stricter_prompt(
                        raw_text
                    )
                except Exception:
                    pass

            if noncompliant_violations:
                print(f"[orchestrator] {task.id}: Noncompliant output detected: {noncompliant_violations}")
                if has_critical_tool_violation:
                    print(
                        f"[orchestrator] {task.id}: CRITICAL: Agent incorrectly claimed tools are disabled - will explicitly correct"
                    )

            if task._json_correction_count <= max_json_corrections:
                # Reprompt with strict correction prompt
                print(
                    f"[orchestrator] {task.id}: JSON validation failed ({validation_error}), "
                    f"auto-reprompt {task._json_correction_count}/{max_json_corrections}"
                )

                # Build strict correction prompt using contract hardening
                correction_prompt = _build_strict_correction_prompt(
                    agent=agent,
                    milestone_id=milestone_id,
                    validation_error=validation_error,
                    task_title=task.title,
                    task_description=task.description,
                    noncompliant_violations=noncompliant_violations if noncompliant_violations else None,
                    attempt_number=task._json_correction_count,
                    has_critical_tool_violation=has_critical_tool_violation,
                )

                # Save correction prompt and retry
                correction_path = task.task_dir / f"correction_{task._json_correction_count}.txt"
                correction_path.write_text(correction_prompt, encoding="utf-8")
                task.prompt_path = correction_path

                correction_out_path = task.task_dir / f"turn_correction_{task._json_correction_count}.json"
                correction_raw_log = task.task_dir / f"raw_correction_{task._json_correction_count}.log"

                cmd = [str(script_path), str(correction_path), str(schema_path), str(correction_out_path)]
                res = _run_cmd_monitored(
                    cmd,
                    cwd=task.worktree_path,
                    env=env,
                    prefix=f"{prefix} correction",
                    raw_log_path=correction_raw_log,
                    stream_to_terminal=True,
                    terminal_max_bytes=terminal_max_bytes,
                    terminal_max_line_length=terminal_max_line_length,
                    cpu_threshold_pct_total=cpu_threshold_pct_total,
                    mem_threshold_pct_total=mem_threshold_pct_total,
                    sample_interval_s=config.parallel.sample_interval_s,
                    consecutive_samples=config.parallel.consecutive_samples,
                    kill_grace_s=config.parallel.kill_grace_s,
                    allow_resource_intensive=allow_resource_intensive,
                )

                if res.returncode == 0 and correction_out_path.exists():
                    # Re-validate the correction output using TurnNormalizer
                    corr_text = correction_out_path.read_text(encoding="utf-8")

                    # Check for noncompliant patterns in correction output
                    is_still_noncompliant, new_violations, new_has_critical = _is_noncompliant_and_should_use_stricter_prompt(
                        corr_text
                    )
                    if is_still_noncompliant:
                        print(f"[orchestrator] {task.id}: Correction output still noncompliant: {new_violations}")
                        # Force another correction attempt with stricter prompt
                        noncompliant_violations = new_violations
                        has_critical_tool_violation = new_has_critical
                    else:
                        # Use TurnNormalizer for correction output
                        corr_norm = normalize_agent_output(
                            corr_text,
                            expected_agent=agent,
                            expected_milestone_id=milestone_id,
                            stats_id_set=stats_id_set,
                            default_phase="implement",
                        )
                        if corr_norm.success and corr_norm.turn:
                            corr_obj = corr_norm.turn
                            for w in corr_norm.warnings:
                                print(f"[orchestrator] {task.id}: CORRECTION NORMALIZED: {w}")
                            # Use lenient validation
                            ok2, msg2, val_w = _validate_turn_lenient(
                                corr_obj,
                                expected_agent=agent,
                                expected_milestone_id=milestone_id,
                                stats_id_set=stats_id_set,
                            )
                            for w in val_w:
                                print(f"[orchestrator] {task.id}: CORRECTION VALIDATION: {w}")
                            if ok2:
                                # Correction succeeded!
                                print(
                                    f"[orchestrator] {task.id}: JSON correction succeeded on attempt {task._json_correction_count}"
                                )
                                turn_obj = corr_obj
                                out_path = correction_out_path
                                validation_error = None

                # If still invalid, try fallback agent
                if validation_error and not task._agent_fallback_used:
                    other_agent = "claude" if agent == "codex" else "codex"
                    # Check if other agent is enabled
                    enabled = config.enable_agents or []
                    if other_agent in enabled:
                        print(f"[orchestrator] {task.id}: Falling back to {other_agent} after {agent} failed")
                        task._agent_fallback_used = True
                        task._json_correction_count = 0

                        # Update agent and script
                        agent = other_agent
                        script_rel = config.agent_scripts.get(agent, "")
                        script_path = (task.worktree_path / script_rel) if script_rel else None
                        if script_path and script_path.exists():
                            prefix = f"[w{worker_id:02d} {agent} {task.id}]"
                            # Retry with new agent (continue loop)
                            continue

        # If validation still fails after all attempts, mark as failed
        if validation_error:
            task.status = "failed"
            task.error = f"JSON validation failed after recovery attempts: {validation_error}"
            task.manual_path = _write_manual_task_file(
                manual_dir=manual_dir,
                task=task,
                reason=task.error,
                agent_cmd=cmd,
                schema_path=schema_path,
                prompt_path=prompt_path,
                out_path=out_path,
                raw_log_path=raw_log_path,
            )
            return

        # Track turn output metadata
        task.turn_obj = turn_obj
        task.work_completed = bool(turn_obj.get("work_completed", False))
        task.turn_summary = str(turn_obj.get("summary", ""))[:500]
        requirement_progress = turn_obj.get("requirement_progress", {}) or {}
        task.requirement_progress = requirement_progress
        task.commands_run = _normalize_commands(requirement_progress.get("commands_run", []) or [])
        task.tests_added_or_modified = list(requirement_progress.get("tests_added_or_modified", []) or [])
        task.covered_req_ids = list(requirement_progress.get("covered_req_ids", []) or [])

        # Collect changes as patch artifact (no git commit required from worker)
        # This eliminates the ".git/worktrees/*/index.lock permission denied" class of failures
        rc, porcelain, err = _run_cmd(["git", "status", "--porcelain=v1"], cwd=task.worktree_path, env=os.environ.copy())
        has_changes = rc == 0 and bool(porcelain.strip())
        if has_changes:
            # Collect patch artifact from worktree
            patch_artifact = collect_patch_artifact(
                worktree_path=task.worktree_path,
                task_id=task.id,
                base_sha=task.base_sha,
            )
            if patch_artifact.success:
                # Save patch artifacts atomically
                patch_path, manifest_path = save_patch_artifact(patch_artifact, task.task_dir)
                task.patch_path = patch_path
                task.patch_manifest_path = manifest_path
                task.has_patch = True
                print(f"[orchestrator] {task.id}: collected patch with {len(patch_artifact.changes)} changed files")
            else:
                print(f"[orchestrator] WARNING: {task.id}: patch collection failed: {patch_artifact.error}")
                # Fall back to legacy worktree-based commit attempt (may fail in sandbox)
                _run_cmd(["git", "add", "-A"], cwd=task.worktree_path, env=os.environ.copy())
                rc_commit, out_commit, err_commit = _run_cmd(
                    ["git", "commit", "-m", f"task({task.id}): auto commit"],
                    cwd=task.worktree_path,
                    env=os.environ.copy(),
                )
                if rc_commit == 0:
                    rc_sha, sha_out, _ = _run_cmd(["git", "rev-parse", "HEAD"], cwd=task.worktree_path, env=os.environ.copy())
                    if rc_sha == 0:
                        task.commit_sha = sha_out.strip()
                # If commit fails, that's OK - we'll rely on the patch artifact

        # Engineering mode: fail fast on non-progress turns
        non_progress = False
        non_progress_reason = ""
        if planner_profile == "engineering":
            non_progress, non_progress_reason = _detect_non_progress_turn(turn_obj, has_changes or task.has_patch)
            if non_progress:
                task.status = "manual"
                task.error = f"NON_PROGRESS: {non_progress_reason}"
                _mark_task_manual(
                    task=task,
                    reason=task.error,
                    manual_dir=manual_dir,
                    schema_path=schema_path,
                )
                return

        # CRITICAL: Task is only "done" if work_completed==true
        if task.work_completed:
            task.status = "done"
            task.error = None
            return

        # Check if there were actual changes (patch or commit) despite work_completed=false
        if getattr(task, "has_patch", False) or (task.commit_sha and task.commit_sha != task.base_sha):
            # Agent made changes but claimed work_completed=false - trust the changes, mark done
            print(f"[orchestrator] NOTE: {task.id} has work_completed=false but made changes; marking done")
            task.status = "done"
            task.error = None
            return

        # Planning-only turn with no implementation - check if we can retry
        if task.retry_count < task.max_retries:
            task.retry_count += 1
            print(f"[orchestrator] {task.id}: work_completed=false, scheduling retry {task.retry_count}/{task.max_retries}")
            # Continue to next iteration of the retry loop
            continue
        else:
            # All retries exhausted - mark for manual rerun
            if planner_profile == "engineering":
                task.status = "manual"
                task.error = non_progress_reason or "work_completed=false with no changes"
                _mark_task_manual(
                    task=task,
                    reason=f"{task.error}. Summary: {task.turn_summary}",
                    manual_dir=manual_dir,
                    schema_path=schema_path,
                )
                return
            task.status = "pending_rerun"
            task.error = f"Agent returned work_completed=false after {task.retry_count + 1} attempts; needs manual implementation"
            task.manual_path = _write_manual_task_file(
                manual_dir=manual_dir,
                task=task,
                reason=f"{task.error}. Summary: {task.turn_summary}",
                agent_cmd=cmd,
                schema_path=schema_path,
                prompt_path=prompt_path,
                out_path=out_path,
                raw_log_path=raw_log_path,
            )
            return


def _run_selftest_task(
    *,
    task: ParallelTask,
    worker_id: int,
    tasks_dir: Path,
) -> None:
    """Execute a selftest task (no real agent, just a trivial command)."""
    task.worker_id = worker_id
    task.task_dir = tasks_dir / _sanitize_branch_fragment(task.id)
    _ensure_dir(task.task_dir)

    # Write a synthetic prompt
    prompt_path = task.task_dir / "prompt.txt"
    prompt_text = f"[SELFTEST] Task: {task.id}\nTitle: {task.title}\nDescription: {task.description}\n"
    prompt_path.write_text(prompt_text, encoding="utf-8")
    task.prompt_path = prompt_path

    # Raw log
    raw_log_path = task.task_dir / "raw.log"
    task.raw_log_path = raw_log_path

    prefix = f"[w{worker_id:02d} SELFTEST-{task.id}]"

    # Execute a trivial command
    cmd = [
        sys.executable,
        "-c",
        f"import time; print('selftest {task.id} started'); time.sleep(0.5); print('selftest {task.id} done')",
    ]

    try:
        with raw_log_path.open("w", encoding="utf-8") as log_f:
            proc = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
            )
            for line in proc.stdout or []:
                log_f.write(line)
                log_f.flush()
                sys.stdout.write(f"{prefix} {line}")
                sys.stdout.flush()
            proc.wait()

        if proc.returncode != 0:
            task.status = "failed"
            task.error = f"Selftest command exited with code {proc.returncode}"
        else:
            task.status = "done"
    except Exception as e:
        task.status = "failed"
        task.error = str(e)
        # Write exception.txt
        exc_path = task.task_dir / "exception.txt"
        exc_path.write_text(traceback.format_exc(), encoding="utf-8")


def _maybe_write_task_report(
    *,
    task: ParallelTask,
    runs_dir: Path,
    planner_profile: str,
    config: RunConfig,
) -> None:
    if planner_profile != "engineering":
        return
    try:
        agent_model = config.agent_models.get(task.agent)
        report_path = write_task_report(
            runs_dir=runs_dir,
            task=task,
            agent_model=agent_model,
            planner_profile=planner_profile,
        )
        print(f"[orchestrator] wrote task report: {report_path}")
    except Exception as e:
        print(f"[orchestrator] warning: failed to write task report for {task.id}: {e}")


def run_parallel(
    *,
    args: argparse.Namespace,
    config: RunConfig,
    state: RunState,
    stats_ids: list[str],
    stats_id_set: set,
    system_prompt: str,
) -> int:
    machine_info = _collect_machine_info()
    planner_profile = _normalize_planner_profile(getattr(args, "planner_profile", DEFAULT_PLANNER_PROFILE))
    selftest_mode = getattr(args, "selftest_parallel", False)

    # Preflight check: detect dirty repo before spending credits
    # Skip in selftest mode since we're not doing real work
    if not selftest_mode:
        auto_stash = getattr(args, "auto_stash", False)
        force_dirty = getattr(args, "force_dirty", False)
        verify_mode = getattr(args, "verify_mode", "strict")

        preflight_ok, preflight_msg, stash_ref = _preflight_check_repo(
            state.project_root,
            auto_stash=auto_stash,
            force_dirty=force_dirty,
            runs_dir=state.runs_dir,
        )

        if not preflight_ok:
            print(f"\n[orchestrator] {preflight_msg}")
            # Write error to runs_dir for debugging
            error_path = state.runs_dir / "preflight_error.txt"
            error_path.write_text(preflight_msg, encoding="utf-8")
            stderr_path = state.runs_dir / "stderr.log"
            stderr_path.write_text(f"PREFLIGHT FAILURE\n\n{preflight_msg}\n", encoding="utf-8")
            # Write minimal run.json
            run_json_path = state.runs_dir / "run.json"
            run_json_path.write_text(
                json.dumps(
                    {
                        "status": "preflight_failed",
                        "error": preflight_msg,
                        "run_id": state.run_id,
                    },
                    indent=2,
                ),
                encoding="utf-8",
            )
            return 2

        if stash_ref:
            print(f"[orchestrator] {preflight_msg}")
            print(f"[orchestrator] IMPORTANT: Run 'git stash pop {stash_ref}' to restore your changes after the run")
        elif "WARNING" in preflight_msg:
            print(f"[orchestrator] {preflight_msg}")

        # Run bootstrap to ensure environment is consistent before tasks start
        print("[orchestrator] Running environment bootstrap...")
        from bridge.verify_repair.bootstrap import run_bootstrap

        bootstrap_log = state.runs_dir / "bootstrap_start.log"
        bootstrap_result = run_bootstrap(
            state.project_root,
            log_path=bootstrap_log,
            verbose=True,
        )
        if bootstrap_result.success:
            if not bootstrap_result.skipped:
                print(f"[orchestrator] Bootstrap completed in {bootstrap_result.elapsed_s:.1f}s")
        else:
            print(f"[orchestrator] WARNING: Bootstrap failed: {bootstrap_result.stderr[:200]}")
            print("[orchestrator] Continuing anyway - verify will catch any missing dependencies")

    # In selftest mode, we use synthetic tasks
    if selftest_mode:
        print("[orchestrator] SELFTEST MODE: using synthetic tasks (no real agents)")
        tasks: list[ParallelTask] = [
            ParallelTask(
                id="SELFTEST-A",
                title="Selftest task A (no deps)",
                description="First selftest task with no dependencies",
                agent="codex",
                intensity="low",
                locks=["lock-a"],
                depends_on=[],
                solo=False,
            ),
            ParallelTask(
                id="SELFTEST-B",
                title="Selftest task B (depends on A)",
                description="Second selftest task depending on A",
                agent="codex",
                intensity="low",
                locks=["lock-b"],
                depends_on=["SELFTEST-A"],
                solo=False,
            ),
            ParallelTask(
                id="SELFTEST-C",
                title="Selftest task C (depends on A)",
                description="Third selftest task depending on A",
                agent="codex",
                intensity="low",
                locks=["lock-c"],
                depends_on=["SELFTEST-A"],
                solo=False,
            ),
        ]
        milestone_id = "SELFTEST"
        design_doc_text = "# Selftest Design Document\n"
        max_workers = 2
        safe_cap = 2
    else:
        # Parse design document using modular adapter layer
        # This supports arbitrary markdown formats without rigid heading assumptions
        contract_mode: ContractMode = getattr(args, "design_doc_contract", "loose")
        milestone_override = getattr(args, "milestone_id", None)

        design_spec = parse_design_doc(
            state.design_doc_path,
            contract_mode=contract_mode,
            milestone_override=milestone_override,
        )

        # Log design doc parsing results
        print(f"[orchestrator] design_doc: path={state.design_doc_path}, hash={design_spec.doc_hash[:12]}")
        print(
            f"[orchestrator] design_doc: milestone={design_spec.milestone_id or '(not found)'}, "
            f"requirements={len(design_spec.requirements)}, contract_mode={contract_mode}"
        )

        if design_spec.warnings:
            for warning in design_spec.warnings[:5]:
                print(f"[orchestrator] design_doc WARNING: {warning}")
            if len(design_spec.warnings) > 5:
                print(f"[orchestrator] design_doc: ... and {len(design_spec.warnings) - 5} more warnings")

        # Check for errors based on contract mode
        if design_spec.errors:
            for error in design_spec.errors:
                print(f"[orchestrator] design_doc ERROR: {error}")
            if contract_mode != "off":
                print("[orchestrator] ERROR: Design doc validation failed. Use --design-doc-contract=off to bypass.")
                return 2

        # Extract values for use in prompts
        design_doc_text = design_spec.raw_text
        if not design_doc_text.strip():
            print(f"[orchestrator] ERROR: design doc not found or empty: {state.design_doc_path}")
            return 2

        # Use milestone from spec (may be CLI override or extracted)
        milestone_id = design_spec.milestone_id or "M0"

        # Save design spec artifact for debugging
        design_spec_artifact = state.runs_dir / "design_doc_spec.json"
        try:
            design_spec_artifact.write_text(json.dumps(design_spec.to_dict(), indent=2), encoding="utf-8")
        except Exception:
            pass  # Non-fatal if we can't write the artifact

        # Thresholds & stream limits
        cpu_thr = args.cpu_threshold if args.cpu_threshold > 0 else config.parallel.cpu_intensive_threshold_pct
        mem_thr = args.mem_threshold if args.mem_threshold > 0 else config.parallel.mem_intensive_threshold_pct
        term_bytes = args.terminal_max_bytes if args.terminal_max_bytes > 0 else config.parallel.terminal_max_bytes_per_worker
        term_line = args.terminal_max_line_len if args.terminal_max_line_len > 0 else config.parallel.terminal_max_line_length

        allow_resource_intensive = bool(args.allow_resource_intensive)

        # Planner step (CODEX)
        plan_schema_path = (state.project_root / args.task_plan_schema).resolve()
        plan_prompt_path = state.runs_dir / "task_planner_prompt.txt"
        plan_out_path = state.runs_dir / "task_plan.json"

        # Safety cap hint: default to cores - 6 (leaving headroom for OS + orchestrator), capped at 12.
        # This gives 10 on a 16-core machine. Use --max-workers to override if needed.
        cores = machine_info.get("cpu_cores") or 1
        safe_cap = min(12, max(4, int(cores) - 6))

        # If CLI explicitly sets --max-workers, use that as the planner limit too
        cli_cap = int(args.max_workers) if args.max_workers and int(args.max_workers) > 0 else 0
        planner_max_workers_limit = cli_cap if cli_cap > 0 else safe_cap

        # Get planner profile (default: balanced)
        # Log throughput mode settings
        if planner_profile == "throughput":
            print(
                f"[orchestrator] planner_profile=throughput (target_roots={planner_max_workers_limit}, target_plan_cap={planner_max_workers_limit})"
            )
        elif planner_profile == "engineering":
            print("[orchestrator] planner_profile=engineering (no backfill, fail-fast, per-task reports)")

        plan_prompt = _build_task_plan_prompt(
            design_doc_text=design_doc_text,
            milestone_id=milestone_id,
            max_workers_limit=planner_max_workers_limit,
            cpu_threshold_pct_total=cpu_thr,
            mem_threshold_pct_total=mem_thr,
            machine_info=machine_info,
            planner_profile=planner_profile,
        )
        plan_prompt_path.write_text(plan_prompt, encoding="utf-8")

        # Select planner agent through policy (default: codex, but --only-* overrides)
        policy = get_agent_policy()
        planner_agent = policy.enforce("codex", "planner agent selection")
        planner_script = state.project_root / config.agent_scripts.get(planner_agent, "")
        if not planner_script.exists():
            print(f"[orchestrator] ERROR: planner script not found: {planner_script}")
            return 2

        print(f"[orchestrator] parallel: planning tasks via {planner_agent} (schema={plan_schema_path})")
        env = os.environ.copy()
        env["WRITE_ACCESS"] = "0"
        env["ORCH_WRITE_ACCESS"] = "0"
        # Signal to the agent wrapper that this is a task_plan schema (not turn schema)
        # This prevents turn normalization from corrupting the planner output
        env["ORCH_SCHEMA_KIND"] = "task_plan"

        # Planner loop with auto-reprompt for throughput mode
        # Uses comprehensive plan quality scoring to determine reprompt
        max_planner_attempts = 3 if planner_profile == "throughput" else 1
        plan_obj: dict[str, Any] | None = None
        final_quality_report: dict[str, Any] | None = None

        for planner_attempt in range(max_planner_attempts):
            current_prompt_path = (
                plan_prompt_path if planner_attempt == 0 else state.runs_dir / f"task_planner_prompt_retry{planner_attempt}.txt"
            )
            current_out_path = (
                plan_out_path if planner_attempt == 0 else state.runs_dir / f"task_plan_retry{planner_attempt}.json"
            )

            rc, _, err = _run_cmd(
                [str(planner_script), str(current_prompt_path), str(plan_schema_path), str(current_out_path)],
                cwd=state.project_root,
                env=env,
                stream=True,
            )
            if rc != 0 or not current_out_path.exists():
                print("[orchestrator] ERROR: planning step failed")
                if err:
                    print(err)
                return 2

            plan_obj = _try_parse_json(current_out_path.read_text(encoding="utf-8"))
            if not isinstance(plan_obj, dict):
                print("[orchestrator] ERROR: could not parse task_plan.json")
                return 2

            # Analyze plan width (basic metrics)
            analysis = _analyze_plan_width(plan_obj, planner_max_workers_limit)

            # Build comprehensive quality report (throughput mode only)
            quality_report = _build_plan_quality_report(plan_obj, planner_max_workers_limit, planner_profile)
            final_quality_report = quality_report

            # Log plan quality metrics
            metrics = quality_report.get("metrics", {})
            if planner_profile == "throughput" and metrics:
                max_lock = metrics.get("max_lock_count", 0)
                tests_intent = metrics.get("coverage_intent", 0)
                should_reprompt = quality_report.get("should_reprompt", False)
                print(
                    f"[orchestrator] plan_quality: cap={metrics.get('plan_cap', 0)} "
                    f"roots={metrics.get('root_ready', 0)} tasks={metrics.get('total_tasks', 0)} "
                    f"lock_pressure=max(lock)={max_lock} tests_intent={tests_intent:.2f} "
                    f"reprompt={'yes' if should_reprompt else 'no'}"
                )
            else:
                # Balanced mode: simple width log
                print(
                    f"[orchestrator] plan_width: tasks={analysis['task_count']}, roots={analysis['root_count']}, plan_cap={analysis['plan_cap']}"
                )

            # In throughput mode, use quality report to determine reprompt
            if planner_profile == "throughput" and quality_report.get("should_reprompt", False):
                if planner_attempt < max_planner_attempts - 1:
                    print(
                        f"[orchestrator] WARNING: Plan does not meet throughput quality requirements (attempt {planner_attempt + 1}/{max_planner_attempts})"
                    )
                    for issue in quality_report.get("hard_failures", []):
                        print(f"[orchestrator]   FAIL: {issue}")
                    for warning in quality_report.get("soft_warnings", []):
                        print(f"[orchestrator]   WARN: {warning}")
                    print("[orchestrator] Re-prompting planner with targeted correction...")

                    # Build correction prompt with quality report and attempt number
                    correction_prompt = _build_throughput_correction_prompt(
                        analysis,
                        planner_max_workers_limit,
                        quality_report=quality_report,
                        attempt_number=planner_attempt + 1,
                    )
                    # Append correction to original prompt
                    combined_prompt = plan_prompt + "\n\n" + correction_prompt
                    next_prompt_path = state.runs_dir / f"task_planner_prompt_retry{planner_attempt + 1}.txt"
                    next_prompt_path.write_text(combined_prompt, encoding="utf-8")
                    continue  # Retry with correction
                else:
                    # Exhausted retries - warn loudly but proceed (do not fail run)
                    print(
                        f"[orchestrator] WARNING: Plan still has quality issues after {max_planner_attempts} attempts. Proceeding anyway."
                    )
                    for issue in quality_report.get("hard_failures", []):
                        print(f"[orchestrator]   FAIL: {issue}")
                    for warning in quality_report.get("soft_warnings", []):
                        print(f"[orchestrator]   WARN: {warning}")
                    break
            else:
                # Plan is acceptable (or balanced mode)
                if planner_profile == "throughput":
                    # Log any soft warnings even for acceptable plans
                    for warning in quality_report.get("soft_warnings", []):
                        print(f"[orchestrator] note: {warning}")
                break

        # Write plan quality report to runs directory (observability)
        if final_quality_report and planner_profile == "throughput":
            quality_report_path = state.runs_dir / "plan_quality_report.json"
            try:
                quality_report_path.write_text(
                    json.dumps(final_quality_report, indent=2, default=str),
                    encoding="utf-8",
                )
                print("[orchestrator] wrote plan_quality_report.json")
            except Exception as e:
                print(f"[orchestrator] warning: could not write plan_quality_report.json: {e}")

        if plan_obj is None:
            print("[orchestrator] ERROR: no valid plan generated")
            return 2

        raw_tasks = plan_obj.get("tasks", [])
        plan_max_parallel = int(plan_obj.get("max_parallel_tasks", safe_cap) or safe_cap)

        # Helper to select agent when "either" is specified
        agent_round_robin_counter = [0]  # mutable for closure

        def _select_agent_for_task(preferred: str, task_title: str, task_desc: str) -> str:
            """Select agent using heuristics when 'either' or invalid agent is specified.

            NOTE: This returns a heuristic-based selection. The caller must still
            apply AgentPolicy.enforce() to the result for forced mode compliance.
            """
            if preferred in AGENTS:
                return preferred

            # Heuristic: check title/description for keywords
            text = (task_title + " " + task_desc).lower()
            claude_keywords = ["schema", "doc", "review", "test", "edge", "api", "refactor", "spec", "design", "validate"]
            codex_keywords = ["implement", "build", "cli", "integration", "low-level", "pipeline", "backend", "export"]

            claude_score = sum(1 for kw in claude_keywords if kw in text)
            codex_score = sum(1 for kw in codex_keywords if kw in text)

            if claude_score > codex_score:
                return "claude"
            elif codex_score > claude_score:
                return "codex"
            else:
                # Round-robin fallback for ties
                agent_round_robin_counter[0] += 1
                return "claude" if agent_round_robin_counter[0] % 2 == 0 else "codex"

        tasks = []
        for t in raw_tasks if isinstance(raw_tasks, list) else []:
            if not isinstance(t, dict):
                continue
            tid = str(t.get("id", "")).strip()
            if not tid:
                continue
            if planner_profile == "engineering" and tid.startswith("FILLER-"):
                print(f"[orchestrator] engineering profile: skipping filler task {tid}")
                continue
            # The planner schema uses preferred_agent/estimated_intensity, but we also
            # accept legacy keys (agent/intensity) for compatibility.
            agent = str(t.get("preferred_agent", t.get("agent", "either"))).strip().lower()
            task_title = str(t.get("title", tid)).strip()
            task_desc = str(t.get("description", "")).strip()
            if agent == "either" or agent not in AGENTS:
                agent = _select_agent_for_task(agent, task_title, task_desc)
            enabled = getattr(config, "enable_agents", None) or getattr(config, "enabled_agents", None) or []
            if enabled and agent not in enabled:
                agent = enabled[0]
            # Apply agent policy enforcement (--only-* flags)
            agent = policy.enforce(agent, f"task {tid} agent selection")
            tasks.append(
                ParallelTask(
                    id=tid,
                    title=str(t.get("title", tid)).strip(),
                    description=str(t.get("description", "")).strip() or str(t.get("title", tid)).strip(),
                    agent=agent,
                    intensity=str(t.get("estimated_intensity", t.get("intensity", "low"))).strip().lower(),
                    locks=list(t.get("locks", []) or []),
                    touched_paths=list(t.get("touched_paths", []) or []),
                    depends_on=list(t.get("depends_on", []) or []),
                    solo=bool(t.get("solo", False)),
                    max_retries=_max_retries_for_profile(planner_profile),
                )
            )

        tasks = _select_only_tasks(tasks, args.only_task)

        # Inject hot-file locks to prevent concurrent edits to critical files
        tasks = _inject_hot_file_locks(tasks)

        # Inject general overlap locks for any files touched by multiple tasks
        # This provides "as narrow as possible, as strong as necessary" locking
        tasks = _inject_overlap_locks(tasks)

        if not tasks:
            print("[orchestrator] ERROR: no tasks in plan")
            return 2

        # Final worker count (CLI override takes precedence and bypasses safe_cap)
        if args.max_workers and int(args.max_workers) > 0:
            max_workers = min(16, int(args.max_workers))
        else:
            max_workers = min(16, safe_cap, plan_max_parallel)
            max_workers = max(1, max_workers)

    # Common setup for both selftest and real mode
    if not selftest_mode:
        cpu_thr = args.cpu_threshold if args.cpu_threshold > 0 else config.parallel.cpu_intensive_threshold_pct
        mem_thr = args.mem_threshold if args.mem_threshold > 0 else config.parallel.mem_intensive_threshold_pct
        term_bytes = args.terminal_max_bytes if args.terminal_max_bytes > 0 else config.parallel.terminal_max_bytes_per_worker
        term_line = args.terminal_max_line_len if args.terminal_max_line_len > 0 else config.parallel.terminal_max_line_length
        allow_resource_intensive = bool(args.allow_resource_intensive)
    else:
        cpu_thr = 0.0
        mem_thr = 0.0
        term_bytes = 10000
        term_line = 200
        allow_resource_intensive = True

    # Verbose logging of all worker limits
    if not selftest_mode:
        cli_val = int(args.max_workers) if args.max_workers and int(args.max_workers) > 0 else 0
        print(
            f"[orchestrator] parallel: max_workers={max_workers} (safe_cap={safe_cap}, plan_cap={plan_max_parallel}, cli_cap={cli_val if cli_val > 0 else 'auto'})"
        )
    else:
        print(f"[orchestrator] parallel: max_workers={max_workers} (selftest mode)")

    tasks_dir = state.runs_dir / "tasks"
    worktrees_dir = state.runs_dir / "worktrees"
    manual_dir = state.runs_dir / "manual"
    _ensure_dir(tasks_dir)
    _ensure_dir(worktrees_dir)
    _ensure_dir(manual_dir)

    by_id = {t.id: t for t in tasks}

    def deps_satisfied(t: ParallelTask) -> bool:
        """Check if all dependencies are satisfied.

        Only "done" counts as satisfied. "blocked", "failed", "manual",
        "pending_rerun", and "resource_killed" do NOT satisfy dependencies.
        """
        for dep in t.depends_on:
            dt = by_id.get(dep)
            if dt is None:
                # Unknown dependency - treat as satisfied (might be external)
                continue
            if dt.status != "done":
                return False
        return True

    def is_root_failure(t: ParallelTask) -> bool:
        """Check if a task is a root failure (not transitively blocked)."""
        return t.status in ("failed", "manual", "pending_rerun", "resource_killed")

    # Scheduler state
    held_locks: set = set()
    running: dict[str, concurrent.futures.Future] = {}  # task_id -> future
    git_lock = threading.Lock()

    # Patch integrator for commit-free integration
    patch_integrator = PatchIntegrator(state.project_root, state.runs_dir)
    patch_integrator.set_lock(git_lock)

    def locks_available(t: ParallelTask) -> bool:
        # Check named locks
        if set(map(str, t.locks)) & held_locks:
            return False
        # Check touched_paths overlap - auto-generate path-based locks
        task_paths = set(str(p) for p in t.touched_paths)
        return not task_paths & held_locks

    # Create two-lane scheduler for high-utilization execution
    lane_config = LaneConfig.from_max_workers(max_workers)
    two_lane_scheduler = TwoLaneScheduler(
        lane_config=lane_config,
        tasks=tasks,
        deps_satisfied_fn=deps_satisfied,
        locks_available_fn=locks_available,
    )
    print(
        f"[orchestrator] two-lane scheduler: coding_lane={lane_config.coding_lane_size}, executor_lane={lane_config.executor_lane_size}"
    )

    # Create backfill generator to keep workers busy when primary tasks are blocked
    # In throughput profile, we restrict to high-ROI task types (test, lint) and disable docs/type_hints
    backfill_generator = BackfillGenerator(
        project_root=str(state.project_root),
        min_queue_depth=max_workers * 2,  # Keep 2x workers worth of tasks ready
        planner_profile=planner_profile,  # Pass profile for type restrictions
    )
    backfill_tasks_generated = 0
    # REDUCED CAP: max_workers * 2 instead of * 3 to prevent runaway
    max_backfill_tasks = max_workers * 2  # Cap total backfill to avoid runaway

    def can_start(t: ParallelTask) -> bool:
        """Check if task can start using two-lane scheduler."""
        return two_lane_scheduler.can_start(t)

    def get_ready_tasks() -> list[ParallelTask]:
        """Get ready tasks sorted by priority."""
        return two_lane_scheduler.get_ready_tasks()

    # Allowed directories for backfill tasks (to prevent merge conflicts)
    # NOTE: bridge/ removed from default to protect orchestrator core
    BACKFILL_ALLOWED_DIRS = ["tests/", "docs/", ".github/"]

    # Track recently rejected backfill paths for cooldown (prevents spam loops)
    # Maps path -> cycle_count when rejected
    backfill_rejection_cooldown: dict[str, int] = {}
    BACKFILL_COOLDOWN_CYCLES = 5  # Number of cycles before retry after rejection

    def derive_backfill_lock(filler: FillerTask) -> list[str]:
        """Derive fine-grained locks for a backfill task based on its target.

        Instead of a global "backfill" lock, we use task-type-specific locks.
        This allows multiple backfill tasks to run concurrently when they
        target different areas (e.g., lint and docs can run together).

        Lock strategy:
        - backfill:type:<task_type> - prevents two lint tasks colliding
        - Different task types can run in parallel
        """
        # Use task type as lock key - allows parallel backfill of different types
        task_type = filler.task_type
        return [f"backfill:type:{task_type}"]

    def convert_filler_to_parallel_task(filler: FillerTask) -> ParallelTask:
        """Convert a FillerTask to a ParallelTask for execution.

        Backfill tasks are scope-constrained to safe directories to prevent
        merge conflicts with primary tasks.

        Uses file-derived locks instead of global "backfill" lock to allow
        concurrent backfill when tasks target different areas.
        """
        # Add scope constraint to description
        scope_note = (
            f"\n\nSCOPE CONSTRAINT: This is a FILLER task. You MUST only modify files in: "
            f"{', '.join(BACKFILL_ALLOWED_DIRS)}. "
            f"Do NOT touch files outside these directories (especially src/, api.py, bridge/loop.py, DESIGN_DOCUMENT.md, etc.)"
        )

        # Derive locks based on task type - allows parallel execution of different types
        task_locks = derive_backfill_lock(filler)

        return ParallelTask(
            id=filler.id,
            title=filler.title,
            description=filler.description + scope_note,
            agent="claude",  # Use Claude for safe filler tasks
            intensity="light",  # Filler tasks are always light
            locks=task_locks,  # Type-based locks instead of global "backfill"
            depends_on=[],  # No dependencies
            touched_paths=[],  # Will be filled during execution
            solo=False,
            max_retries=1,  # One retry max for filler tasks
        )

    def maybe_generate_backfill() -> int:
        """Generate backfill tasks if needed. Returns count of tasks added.

        BACKFILL POLICY (prevents runaway/waste):
        1. No FILLER when root failures exist
        2. No FILLER when total queued tasks >= max_workers
        3. No FILLER when queued FILLER tasks >= max_workers / 2
        4. No FILLER while ANY runnable core (non-FILLER) task exists
        5. Budget cap: max_backfill_tasks total per run
        6. Batch limit: max 2 FILLER tasks per generation cycle
        """
        nonlocal backfill_tasks_generated, tasks, by_id

        if planner_profile == "engineering":
            return 0

        # Don't generate in selftest mode
        if selftest_mode:
            return 0

        # POLICY 1: SUPPRESS FILLER WHEN ROOT FAILURES EXIST
        # Root failures are non-backfill tasks that have failed, need manual work, or were resource-killed.
        root_failure_statuses = ("failed", "manual", "resource_killed")
        has_root_failures = any(t.status in root_failure_statuses and not _is_backfill_task_id(t.id) for t in tasks)
        if has_root_failures:
            # Only log once per suppression cycle to avoid spam
            if backfill_tasks_generated == 0:
                print("[orchestrator] BACKFILL SUPPRESSED: Root failures exist - focusing on critical work")
            return 0

        # POLICY 5: Check if we've hit the backfill budget cap
        if backfill_tasks_generated >= max_backfill_tasks:
            return 0

        # POLICY 2: No FILLER when total queued tasks >= max_workers
        total_queued = sum(1 for t in tasks if t.status == "pending")
        if total_queued >= max_workers:
            return 0

        # POLICY 3: No FILLER when queued FILLER tasks >= max_workers / 2
        queued_filler = sum(1 for t in tasks if t.status == "pending" and _is_backfill_task_id(t.id))
        max_queued_filler = max(1, max_workers // 2)  # At least 1, at most half of workers
        if queued_filler >= max_queued_filler:
            return 0

        # POLICY 4: No FILLER while ANY runnable core (non-FILLER) task exists
        # This ensures backfill is ONLY used when truly idle (no core work available)
        ready_tasks = get_ready_tasks()
        core_ready_tasks = [t for t in ready_tasks if not _is_backfill_task_id(t.id)]
        if core_ready_tasks:
            # There are core tasks ready to run - don't generate filler
            return 0

        # Check if we should generate based on queue depth
        pending_count = sum(1 for t in tasks if t.status == "pending")
        if not backfill_generator.should_generate(pending_count, max_workers):
            return 0

        # Check if there are available workers but no ready tasks
        ready_count = len(ready_tasks)
        available_workers = len(available_worker_ids)
        if ready_count >= available_workers:
            return 0

        # POLICY 6: Generate at most 2 filler tasks at a time (prevents runaway)
        count_needed = min(
            available_workers - ready_count,
            max_backfill_tasks - backfill_tasks_generated,
            max_queued_filler - queued_filler,  # Respect queued FILLER cap
            2,  # HARD LIMIT: Generate at most 2 at a time to prevent runaway
        )

        if count_needed <= 0:
            return 0

        filler_tasks = backfill_generator.generate_filler_tasks(count_needed)
        added = 0
        for filler in filler_tasks:
            parallel_task = convert_filler_to_parallel_task(filler)
            tasks.append(parallel_task)
            by_id[parallel_task.id] = parallel_task
            backfill_tasks_generated += 1
            added += 1
            print(f"[orchestrator] BACKFILL: Generated {parallel_task.id} - {parallel_task.title}")

        # Update scheduler with new tasks
        if added > 0:
            two_lane_scheduler.update_tasks(tasks)

        return added

    def record_backfill_rejection(task_id: str, reason: str, rejected_paths: list[str]) -> None:
        """Record a backfill rejection for cooldown tracking.

        When a backfill patch is scope-rejected, this records the rejection
        to prevent generating similar tasks for a cooldown period.

        Args:
            task_id: The FILLER-* task ID that was rejected
            reason: The rejection reason (e.g., "SCOPE_REJECTED")
            rejected_paths: List of paths that caused the rejection
        """
        nonlocal backfill_rejection_cooldown

        current_cycle = backfill_tasks_generated  # Use as cycle counter
        for path in rejected_paths:
            backfill_rejection_cooldown[path] = current_cycle
            print(
                f"[orchestrator] BACKFILL-COOLDOWN: {path} on cooldown for {BACKFILL_COOLDOWN_CYCLES} cycles (rejected in {task_id})"
            )

    def is_backfill_task_type_on_cooldown(task_type: str) -> bool:
        """Check if a backfill task type is on cooldown due to recent rejections.

        Returns True if this task type should not be generated yet.
        """
        current_cycle = backfill_tasks_generated
        # Check if any rejected paths are still on cooldown for this task type
        for path, rejection_cycle in list(backfill_rejection_cooldown.items()):
            if current_cycle - rejection_cycle < BACKFILL_COOLDOWN_CYCLES:
                # This path is still on cooldown
                # For now, we just check if any rejections are recent
                # More sophisticated: match task_type to path patterns
                if task_type in ("lint", "type_hints") and "bridge/" in path:
                    return True
                if task_type in ("test",) and "tests/" in path:
                    return True
            else:
                # Cooldown expired, remove from tracking
                del backfill_rejection_cooldown[path]
        return False

    def cleanup_expired_cooldowns() -> None:
        """Clean up expired cooldown entries."""
        nonlocal backfill_rejection_cooldown
        current_cycle = backfill_tasks_generated
        expired = [
            path for path, cycle in backfill_rejection_cooldown.items() if current_cycle - cycle >= BACKFILL_COOLDOWN_CYCLES
        ]
        for path in expired:
            del backfill_rejection_cooldown[path]

    # Integration helper: prefers patch-based integration, falls back to git merge
    def merge_task(t: ParallelTask) -> bool:
        if selftest_mode or not t.branch:
            return True

        # Compute task milestone from ID prefix
        task_milestone = _extract_milestone_from_task_id(t.id, fallback=milestone_id)

        # Try patch-based integration first (preferred - no sandbox issues)
        if getattr(t, "has_patch", False) and t.task_dir:
            success, msg, commit_sha = patch_integrator.integrate_task(
                task_id=t.id,
                task_dir=t.task_dir,
                task_branch=t.branch,
                agent_name=t.agent,
                task_context=f"Task: {t.title}\nDescription: {t.description}",
                milestone_id=task_milestone,
            )
            if success:
                if commit_sha:
                    t.commit_sha = commit_sha
                    # Record successful result for backfill cooldown tracking
                    if t.id.startswith("FILLER-"):
                        backfill_generator.record_successful_result(t.id)
                elif "No changes to commit" in msg:
                    # No-op result - track for backfill cooldown
                    if t.id.startswith("FILLER-"):
                        backfill_generator.record_noop_result(t.id)
                        print(f"[orchestrator] FILLER NO-OP: {t.id} produced no changes")
                print(f"[orchestrator] PATCH INTEGRATED: {t.id} - {msg}")
                return True

            # Handle SCOPE_REJECTED - do NOT fall back to git merge
            if "SCOPE_REJECTED" in msg:
                print(f"[orchestrator] SCOPE_REJECTED for {t.id}: {msg}")
                if t.id.startswith("FILLER-"):
                    backfill_generator.record_rejection(t.id)
                t.status = "failed"
                t.error = msg
                return False

            # Only fall back to git merge if:
            # 1. Not needs_manual_resolution
            # 2. Worker actually created a commit (has commit_sha or branch differs from base)
            if "needs_manual_resolution" in msg:
                print(f"[orchestrator] Patch integration needs manual resolution for {t.id}: {msg}")
                # Backfill tasks are optional - don't mark manual, just log and return success
                if _is_backfill_task_id(t.id):
                    t.error = msg  # Record for visibility
                    print(f"[orchestrator] FILLER OPTIONAL SKIP: {t.id} needs manual resolution but is optional")
                    return True
                # Non-backfill tasks: mark manual with proper manual_path
                _mark_task_manual(
                    task=t,
                    reason=msg,
                    manual_dir=manual_dir,
                    schema_path=state.schema_path,
                )
                return False

            # Check if there's actually a commit to merge
            # If worker only produced a patch artifact without committing, git merge would be a no-op
            if not getattr(t, "commit_sha", None):
                # No commit SHA means no actual commit to merge - treat as integration failure
                print(f"[orchestrator] Patch integration failed for {t.id} (no commit to merge): {msg}")
                t.status = "failed"
                t.error = f"Patch integration failed (no commit): {msg}"
                return False

            print(f"[orchestrator] Patch integration issue for {t.id}: {msg}, trying git merge...")

        # Fall back to git merge (for legacy commits or when patch integration fails)
        with git_lock:
            rc, out, err = _run_cmd(
                ["git", "merge", "--no-ff", "--no-edit", t.branch],
                cwd=state.project_root,
                env=os.environ.copy(),
                stream=True,
            )
            if rc == 0:
                return True

            # Merge conflict detected - attempt auto-resolution
            print(f"[orchestrator] MERGE CONFLICT detected for {t.id}, attempting auto-resolution...")

            # Try auto-resolution (handles __init__.py files and uses agent for others)
            auto_ok, auto_msg = _attempt_auto_merge_resolution(
                project_root=state.project_root,
                task_id=t.id,
                runs_dir=state.runs_dir,
                task_context=f"Task: {t.title}\nDescription: {t.description}",
                milestone_id=task_milestone,
            )

            if auto_ok:
                print(f"[orchestrator] AUTO-RESOLVED: {auto_msg}")
                return True

            # Auto-resolution failed - abort and mark for manual
            print(f"[orchestrator] AUTO-RESOLUTION FAILED: {auto_msg}")
            _run_cmd(["git", "merge", "--abort"], cwd=state.project_root, env=os.environ.copy())
            t.status = "manual"
            t.error = f"Merge conflict; auto-resolve failed: {auto_msg}"
            t.manual_path = _write_manual_task_file(
                manual_dir=manual_dir,
                task=t,
                reason=t.error,
                agent_cmd=[],
                schema_path=state.schema_path,
                prompt_path=t.prompt_path or Path(""),
                out_path=t.out_path or Path(""),
                raw_log_path=t.raw_log_path or Path(""),
            )
            return False

    # Task execution wrapper with exception handling
    def execute_task(t: ParallelTask, worker_id: int) -> str:
        """Execute a task and return its ID when done. Handles exceptions."""
        # Compute per-task milestone_id from task ID prefix (e.g., "M2-SIM-SCHEMA" -> "M2")
        # This ensures each task gets the correct milestone context in MULTI mode
        task_milestone_id = _extract_milestone_from_task_id(t.id, fallback=milestone_id)
        try:
            if selftest_mode:
                _run_selftest_task(task=t, worker_id=worker_id, tasks_dir=tasks_dir)
            else:
                _run_parallel_task(
                    task=t,
                    worker_id=worker_id,
                    state=state,
                    config=config,
                    milestone_id=task_milestone_id,
                    design_doc_text=design_doc_text,
                    system_prompt=system_prompt,
                    planner_profile=planner_profile,
                    stats_id_set=stats_id_set,
                    tasks_dir=tasks_dir,
                    worktrees_dir=worktrees_dir,
                    manual_dir=manual_dir,
                    git_lock=git_lock,
                    cpu_threshold_pct_total=cpu_thr,
                    mem_threshold_pct_total=mem_thr,
                    terminal_max_bytes=term_bytes,
                    terminal_max_line_length=term_line,
                    allow_resource_intensive=allow_resource_intensive,
                )
        except Exception as e:
            t.status = "failed"
            t.error = f"Exception: {e}"
            # Write exception.txt
            if t.task_dir:
                _ensure_dir(t.task_dir)
                exc_path = t.task_dir / "exception.txt"
                exc_path.write_text(traceback.format_exc(), encoding="utf-8")
            print(f"[orchestrator] ERROR: task {t.id} raised exception: {e}")
        finally:
            _maybe_write_task_report(
                task=t,
                runs_dir=state.runs_dir,
                planner_profile=planner_profile,
                config=config,
            )
        return t.id

    # Main scheduler loop using concurrent.futures with FIRST_COMPLETED
    print(f"[orchestrator] parallel: {len(tasks)} task(s) queued")
    last_heartbeat = time.monotonic()
    heartbeat_interval = 15.0  # seconds

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        worker_ids = list(range(max_workers))
        available_worker_ids = list(worker_ids)

        while True:
            # Calculate stats
            done_count = sum(1 for t in tasks if t.status == "done")
            failed_count = sum(1 for t in tasks if t.status in ("failed", "manual", "pending_rerun", "resource_killed"))
            blocked_count = sum(1 for t in tasks if t.status == "blocked")
            running_count = len(running)
            pending_count = sum(1 for t in tasks if t.status == "pending")
            ready_tasks = get_ready_tasks()

            # Heartbeat logging with lane stats
            now = time.monotonic()
            if now - last_heartbeat >= heartbeat_interval:
                lane_stats = two_lane_scheduler.get_lane_stats()
                # Sample metrics for utilization tracking
                two_lane_scheduler.sample_metrics(queue_depth=pending_count)
                print(
                    f"[orchestrator] parallel: progress done={done_count} running={running_count} queued={pending_count} ready={len(ready_tasks)} "
                    f"lanes[coding={lane_stats['coding_active']}/{lane_stats['coding_capacity']}, exec={lane_stats['executor_active']}/{lane_stats['executor_capacity']}]"
                )
                last_heartbeat = now

            # Check completion - terminal states end the run
            terminal_states = ("done", "failed", "manual", "blocked", "pending_rerun", "resource_killed")
            all_finished = all(t.status in terminal_states for t in tasks)
            if all_finished:
                break

            # Detect stuck state: no tasks running, no tasks ready, but tasks remain pending
            # SELF-HEALING: Instead of immediately marking as stuck, try recovery actions
            if not running and not ready_tasks and pending_count > 0:
                # Identify root failures (tasks that failed on their own, not due to dependencies)
                root_failure_ids = {t.id for t in tasks if is_root_failure(t)}
                root_failures = [t for t in tasks if t.id in root_failure_ids]

                # Compute transitive closure of all blocked tasks
                blocked_ids = _compute_transitive_blocked(tasks, root_failure_ids)
                pending_but_not_blocked = [t for t in tasks if t.status == "pending" and t.id not in blocked_ids]

                print(f"\n[orchestrator] STUCK DETECTED: {pending_count} task(s) cannot proceed")
                print("[orchestrator] Attempting SELF-HEALING recovery...")

                # Track if we recovered
                recovered = False
                recovery_reason = ""

                # SELF-HEALING STRATEGY 1: Check for lock contention without root failures
                # Tasks may be waiting on locks held by no one - re-check lock availability
                if pending_but_not_blocked and not root_failures:
                    print("[orchestrator] RECOVERY: Checking for stale lock contention...")
                    for t in pending_but_not_blocked:
                        # Re-check if locks are available now
                        locks_free = all(str(lk) not in held_locks for lk in t.locks)
                        paths_free = all(str(p) not in held_locks for p in t.touched_paths)
                        deps_done = all(by_id.get(d) and by_id[d].status == "done" for d in t.depends_on)
                        if locks_free and paths_free and deps_done:
                            print(f"[orchestrator] RECOVERY: Task {t.id} is now ready (stale lock cleared)")
                            recovered = True
                            recovery_reason = "stale_lock_cleared"
                            break  # Break out of for loop, will continue scheduler loop

                # SELF-HEALING STRATEGY 2: If there are root failures, schedule them for rerun
                # This converts permanent failures into retriable tasks
                if not recovered and root_failures:
                    print(f"[orchestrator] RECOVERY: {len(root_failures)} root failure(s) detected")
                    rerun_count = 0
                    rerun_candidates: list[tuple[ParallelTask, str]] = []
                    for t in root_failures:
                        allowed, reason = _should_self_heal_task(t, planner_profile)
                        if not allowed:
                            if planner_profile == "engineering":
                                print(f"[orchestrator] SELF-HEAL SKIP: {t.id} ({reason})")
                            continue
                        rerun_candidates.append((t, reason))

                    if planner_profile == "engineering" and rerun_candidates:
                        # Fail-fast mode: only attempt one targeted rerun
                        rerun_candidates = rerun_candidates[:1]

                    for t, reason in rerun_candidates:
                        # Only retry if the task hasn't been retried too many times
                        current_retries = getattr(t, "_self_heal_retries", 0)
                        max_self_heal_retries = 1 if planner_profile == "engineering" else 2
                        if current_retries < max_self_heal_retries:
                            t._self_heal_retries = current_retries + 1
                            # Reset the task for rerun
                            print(
                                f"[orchestrator] RECOVERY: Scheduling {t.id} for self-heal rerun "
                                f"(attempt {t._self_heal_retries}, reason={reason})"
                            )
                            if reason == "resource_killed":
                                # Reduce parallelism impact for resource-killed tasks
                                t.solo = True
                                t.intensity = "high"
                            t.status = "pending"
                            t.error = None
                            t.retry_count = 0  # Reset retry counter
                            rerun_count += 1
                        else:
                            print(f"[orchestrator] RECOVERY: {t.id} exhausted self-heal retries ({max_self_heal_retries})")

                    if rerun_count > 0:
                        recovered = True
                        recovery_reason = f"scheduled_{rerun_count}_reruns"
                        print(f"[orchestrator] RECOVERY: {rerun_count} task(s) scheduled for self-heal rerun")

                # If still not recovered, print detailed diagnostics and mark as truly stuck
                if not recovered:
                    print("\n[orchestrator] SELF-HEALING FAILED: Marking run as STUCK")

                    if root_failures:
                        print(f"\n  ROOT FAILURES ({len(root_failures)}):")
                        for t in root_failures:
                            reason = t.error or t.status
                            manual_hint = f" -> {t.manual_path}" if t.manual_path else ""
                            retries = getattr(t, "_self_heal_retries", 0)
                            print(f"    - {t.id}: {reason} (retries={retries}){manual_hint}")

                    if blocked_ids:
                        blocked_pending = [t for t in tasks if t.status == "pending" and t.id in blocked_ids]
                        if blocked_pending:
                            print(f"\n  BLOCKED TASKS ({len(blocked_pending)}):")
                            for t in blocked_pending:
                                # Find which root failures this task is transitively blocked by
                                blocking_roots = []
                                visited = set()
                                queue = list(t.depends_on)
                                while queue:
                                    dep_id = queue.pop(0)
                                    if dep_id in visited:
                                        continue
                                    visited.add(dep_id)
                                    if dep_id in root_failure_ids:
                                        blocking_roots.append(dep_id)
                                    elif dep_id in blocked_ids:
                                        # This dep is also blocked, trace further
                                        dep_task = by_id.get(dep_id)
                                        if dep_task:
                                            queue.extend(dep_task.depends_on)
                                if blocking_roots:
                                    print(f"    - {t.id}: blocked by root failures: {blocking_roots}")
                                else:
                                    print(f"    - {t.id}: transitively blocked")

                    # Check for true cycles (tasks pending but not blocked by any root failure)
                    if pending_but_not_blocked:
                        print(f"\n  DEPENDENCY CYCLES ({len(pending_but_not_blocked)}):")
                        for t in pending_but_not_blocked:
                            unmet = [d for d in t.depends_on if by_id.get(d) and by_id[d].status != "done"]
                            print(f"    - {t.id}: cycle/deadlock waiting on: {unmet}")

                    # Mark stuck tasks with proper status
                    for t in tasks:
                        if t.status == "pending":
                            if t.id in blocked_ids:
                                # Transitively blocked by a root failure
                                t.status = "blocked"
                                # Find the immediate failing dependencies
                                blocking_roots = [d for d in t.depends_on if d in root_failure_ids]
                                blocked_deps = [d for d in t.depends_on if d in blocked_ids]
                                if blocking_roots:
                                    t.error = f"Blocked by root failures: {blocking_roots}"
                                elif blocked_deps:
                                    t.error = f"Blocked by transitively blocked tasks: {blocked_deps}"
                                else:
                                    t.error = "Transitively blocked"
                            else:
                                # True dependency cycle
                                unmet = [d for d in t.depends_on if by_id.get(d) and by_id[d].status != "done"]
                                t.status = "failed"
                                t.error = f"Dependency cycle: waiting on {unmet}"
                    break
                else:
                    # Recovery succeeded - continue the scheduler loop
                    print(f"[orchestrator] SELF-HEALING: Recovered via {recovery_reason}, continuing...")
                    # Small delay to avoid tight loop
                    time.sleep(0.5)
                    continue

            # BACKFILL: Generate safe filler tasks to keep workers busy
            # This prevents worker idle time when primary tasks are blocked
            if available_worker_ids and not ready_tasks:
                backfill_added = maybe_generate_backfill()
                if backfill_added > 0:
                    # Re-check ready tasks after backfill
                    ready_tasks = get_ready_tasks()

            # Start ready tasks up to capacity using two-lane scheduler
            started_any = False
            while available_worker_ids and ready_tasks and len(running) < max_workers:
                # Re-check ready tasks since state may have changed
                ready_tasks = get_ready_tasks()
                if not ready_tasks:
                    break

                t = ready_tasks[0]
                worker_id = available_worker_ids.pop(0)

                t.status = "running"
                t.worker_id = worker_id

                # Assign to lane using two-lane scheduler
                lane = two_lane_scheduler.assign_to_lane(t.id)

                # Acquire named locks
                for lk in t.locks:
                    held_locks.add(str(lk))
                # Acquire path-based locks from touched_paths
                for path in t.touched_paths:
                    held_locks.add(str(path))

                # Log agent selection with lane info
                lane_stats = two_lane_scheduler.get_lane_stats()
                print(
                    f"[orchestrator] parallel: starting {t.id} on worker {worker_id} (agent={t.agent}, lane={lane}, coding={lane_stats['coding_active']}/{lane_stats['coding_capacity']}, exec={lane_stats['executor_active']}/{lane_stats['executor_capacity']})"
                )
                future = executor.submit(execute_task, t, worker_id)
                running[t.id] = future
                started_any = True

            # Wait for any task to complete (with timeout for heartbeat)
            if running:
                try:
                    done_futures, _ = concurrent.futures.wait(
                        running.values(), timeout=min(5.0, heartbeat_interval), return_when=concurrent.futures.FIRST_COMPLETED
                    )
                except Exception as e:
                    print(f"[orchestrator] WARNING: wait() raised: {e}")
                    time.sleep(1.0)
                    continue

                # Process completed tasks
                for future in done_futures:
                    # Find the task for this future
                    completed_tid = None
                    for tid, f in list(running.items()):
                        if f is future:
                            completed_tid = tid
                            break

                    if completed_tid:
                        del running[completed_tid]
                        t = by_id.get(completed_tid)
                        if t:
                            # Release from two-lane scheduler
                            two_lane_scheduler.release_from_lane(completed_tid)
                            # Release named locks
                            for lk in t.locks:
                                held_locks.discard(str(lk))
                            # Release path-based locks
                            for path in t.touched_paths:
                                held_locks.discard(str(path))
                            if t.worker_id is not None:
                                available_worker_ids.append(t.worker_id)

                            # Get result (might raise if task raised)
                            try:
                                future.result()
                            except Exception as e:
                                if t.status not in ("failed", "manual"):
                                    t.status = "failed"
                                    t.error = f"Future exception: {e}"
                                print(f"[orchestrator] ERROR: task {t.id} future raised: {e}")

                            # Merge if successful
                            if t.status == "done":
                                print(f"[orchestrator] parallel: {t.id} completed successfully")
                                merge_task(t)
                            else:
                                print(f"[orchestrator] parallel: {t.id} finished with status={t.status}")
            elif not started_any:
                # Nothing running and nothing started, wait a bit before retrying
                time.sleep(0.5)

    # Final verification with auto-repair loop (skip in selftest mode; respect verify_mode)
    # STALL PREVENTION: The repair callback ensures verify failures trigger automatic
    # repair actions instead of waiting for manual intervention.
    if not selftest_mode:
        verify_mode = getattr(args, "verify_mode", "strict")
        verify_json = state.runs_dir / "final_verify.json"
        repair_report_path = state.runs_dir / "verify_repair_report.json"
        max_repair_attempts = getattr(args, "max_repair_attempts", 5)

        if verify_mode == "off":
            print("[orchestrator] parallel: skipping final verification (--verify-mode=off)")
            rc_v = 0
        else:
            strict_git = verify_mode != "skip-git"
            mode_label = "strict" if strict_git else "skip-git"
            print(f"[orchestrator] parallel: running verify auto-repair loop ({mode_label}, max_attempts={max_repair_attempts})")

            # Create repair callback for automatic execution of repair tasks
            # This prevents the orchestrator from stalling on verify failures
            repair_callback = create_repair_callback(
                project_root=state.project_root,
                runs_dir=state.runs_dir,
                verbose=True,
                scheduler_callback=None,  # No external scheduler, use internal executor
            )

            repair_result = run_verify_repair_loop(
                project_root=state.project_root,
                verify_json_path=verify_json,
                max_attempts=max_repair_attempts,
                strict_git=strict_git,
                verbose=True,
                runs_dir=state.runs_dir,
                bootstrap_on_start=True,
                agent_task_callback=repair_callback,  # CRITICAL: enables auto-repair
            )

            rc_v = repair_result.final_exit_code
            write_repair_report(repair_result, repair_report_path)

            if repair_result.success:
                print(f"[orchestrator] parallel: verify PASSED after {repair_result.total_attempts} attempt(s)")
            else:
                print(f"[orchestrator] parallel: verify FAILED after {repair_result.total_attempts} attempt(s)")
                print(f"[orchestrator] parallel: remaining failures: {repair_result.remaining_failures}")
                print(f"[orchestrator] parallel: repair report: {repair_report_path}")
                # Write final verify attempt artifact
                final_verify_artifact = state.runs_dir / f"final_verify_attempt_{repair_result.total_attempts}.json"
                if verify_json.exists():
                    shutil.copy(verify_json, final_verify_artifact)
                    print(f"[orchestrator] parallel: final verify artifact: {final_verify_artifact}")
                # Check for out-of-scope repairs that need manual intervention
                out_of_scope_path = state.runs_dir / "out_of_scope_repairs.json"
                if out_of_scope_path.exists():
                    print(f"[orchestrator] parallel: out-of-scope repairs (need manual): {out_of_scope_path}")
    else:
        rc_v = 0

    # Summary
    print("\n[orchestrator] parallel summary")
    done_count = 0
    failed_count = 0
    pending_rerun_count = 0
    blocked_count = 0
    for t in tasks:
        is_backfill = _is_backfill_task_id(t.id)
        extra = ""
        if t.status in ("failed", "manual", "pending_rerun", "resource_killed") and t.manual_path:
            extra = f" (see {t.manual_path})"
        elif t.status in ("failed", "manual", "pending_rerun", "resource_killed", "blocked") and t.error:
            extra = f" ({t.error})"
        # Mark backfill tasks as optional in printed output
        optional_suffix = " (optional)" if is_backfill else ""
        print(f"- {t.id}: {t.status}{extra}{optional_suffix}")

        # Only count non-backfill tasks for needs_continuation calculation
        if is_backfill:
            continue
        if t.status == "done":
            done_count += 1
        elif t.status in ("failed", "manual", "resource_killed"):
            failed_count += 1
        elif t.status == "pending_rerun":
            pending_rerun_count += 1
        elif t.status in ("blocked", "skipped"):
            blocked_count += 1

    if selftest_mode:
        if done_count == len(tasks):
            print("\n[orchestrator] SELFTEST PASSED: all tasks completed successfully")
            return 0
        else:
            print(f"\n[orchestrator] SELFTEST FAILED: {failed_count} task(s) failed")
            return 1

    # Safety net: ensure all manual tasks have manual_path written
    for t in tasks:
        if t.status == "manual" and t.manual_path is None:
            _mark_task_manual(
                task=t,
                reason=t.error or "Task requires manual intervention",
                manual_dir=manual_dir,
                schema_path=state.schema_path,
            )

    # Generate structured summary.json
    summary = _generate_run_summary(
        tasks=tasks,
        runs_dir=state.runs_dir,
        verify_exit_code=rc_v,
        planner_profile=planner_profile,
    )
    summary_path = state.runs_dir / "summary.json"
    summary_path.write_text(json.dumps(summary, indent=2, default=str), encoding="utf-8")
    print(f"\n[orchestrator] Summary written to: {summary_path}")

    # Generate continuation_prompt.txt if there are failures
    needs_continuation = (failed_count + pending_rerun_count + blocked_count) > 0 or rc_v != 0
    if needs_continuation:
        continuation_prompt = _generate_continuation_prompt(
            summary=summary,
            tasks=tasks,
            design_doc_text=design_doc_text,
            runs_dir=state.runs_dir,
        )
        continuation_path = state.runs_dir / "continuation_prompt.txt"
        continuation_path.write_text(continuation_prompt, encoding="utf-8")
        print(f"[orchestrator] Continuation prompt written to: {continuation_path}")

        print("\n[orchestrator] RUN INCOMPLETE:")
        print(f"  - Completed: {done_count}")
        print(f"  - Failed: {failed_count}")
        print(f"  - Pending Rerun: {pending_rerun_count}")
        print(f"  - Blocked: {blocked_count}")
        print(f"  - Verify: {'PASS' if rc_v == 0 else 'FAIL'}")
        print("\n[orchestrator] To continue, run:")
        print(f"  ./run_parallel.sh --continuation {continuation_path}")
    else:
        print(f"\n[orchestrator] RUN COMPLETE: all {done_count} tasks succeeded, verify passed")

    return 0 if rc_v == 0 and not needs_continuation else 1


# -----------------------------
# Auto-continue loop
# -----------------------------


def _compute_failure_signature(error: str) -> str:
    """Compute a normalized signature for an error message.

    This allows detecting when different task IDs have the same underlying error,
    which indicates no real progress was made.

    The signature is computed by:
    1. Normalizing timestamps, run_ids, task IDs
    2. Normalizing paths
    3. Normalizing whitespace
    4. Hashing the result
    """
    if not error:
        return "empty"

    import hashlib

    # Normalize: strip timestamps like 20260126T011919Z
    normalized = re.sub(r"\d{8}T\d{6}Z", "TIMESTAMP", error)

    # Normalize: strip task branch names like task/TIMESTAMP/TASK-ID
    # This catches "branch 'task/TIMESTAMP/M0-RF-01-LOGS-CLAUDE-LIMIT-CASE' already exists"
    normalized = re.sub(r"task/TIMESTAMP/[A-Za-z0-9_-]+", "task/TIMESTAMP/TASK_ID", normalized)

    # Normalize: strip generic task IDs (M0-*, FILLER-*, etc.)
    normalized = re.sub(r"M\d+-[A-Za-z0-9_-]+", "TASK_ID", normalized)
    normalized = re.sub(r"FILLER-[A-Za-z0-9_-]+", "FILLER_ID", normalized)

    # Normalize: strip absolute paths, keep only the filename
    normalized = re.sub(r"/[^\s]+/([^/\s]+)", r"\1", normalized)

    # Normalize: strip line numbers
    normalized = re.sub(r":\d+:", ":LINE:", normalized)

    # Normalize: strip worker IDs
    normalized = re.sub(r"w\d{2}", "wXX", normalized)

    # Normalize whitespace
    normalized = " ".join(normalized.split())

    # Hash to get a stable signature
    return hashlib.sha256(normalized.encode()).hexdigest()[:16]


def _compute_failure_signatures_from_summary(summary: dict[str, Any]) -> set[str]:
    """Compute failure signatures from a run summary.

    Returns a set of normalized failure signatures that can be compared
    across runs to detect if the underlying errors are the same.
    """
    signatures = set()
    for failure in summary.get("root_failures", []):
        error = failure.get("error", "")
        sig = _compute_failure_signature(error)
        signatures.add(sig)
    return signatures


def _generate_repair_context_for_failures(
    root_failures: list[dict[str, Any]],
    runs_dir: Path,
) -> str:
    """Generate repair context for root failures to feed into the next planning cycle.

    This enables self-healing by giving the planner specific guidance on what failed
    and how to approach fixing it.
    """
    if not root_failures:
        return ""

    lines = [
        "\n\n# REPAIR CONTEXT - CRITICAL",
        "",
        "The previous run had the following root failures that need targeted repair:",
        "",
    ]

    for failure in root_failures:
        task_id = failure.get("id", "unknown")
        title = failure.get("title", "unknown task")
        error = failure.get("error", "unknown error")
        agent = failure.get("agent", "unknown")

        lines.append(f"## REPAIR NEEDED: {task_id}")
        lines.append(f"- Original task: {title}")
        lines.append(f"- Agent: {agent}")
        lines.append(f"- Error: {error}")
        lines.append("")
        lines.append("**REPAIR GUIDANCE:**")

        # Provide specific guidance based on error type
        error_lower = error.lower() if error else ""
        if "tools" in error_lower and ("disabled" in error_lower or "cannot" in error_lower):
            lines.append("- The agent incorrectly believed tools were disabled.")
            lines.append("- Tools ARE ENABLED. Use Read, Edit, Write, Bash to implement changes.")
            lines.append("- Do NOT claim tools are unavailable.")
        elif "merge conflict" in error_lower or "conflict" in error_lower:
            lines.append("- There was a merge conflict during integration.")
            lines.append("- Ensure changes don't conflict with other parallel tasks.")
            lines.append("- Consider more granular changes that are less likely to conflict.")
        elif "work_completed=false" in error_lower or "plan" in error_lower:
            lines.append("- The agent returned work_completed=false without making changes.")
            lines.append("- IMPLEMENT the changes directly. Do NOT just plan.")
            lines.append("- Set work_completed=true after implementing.")
        elif "json" in error_lower or "validation" in error_lower:
            lines.append("- JSON output was invalid.")
            lines.append("- Ensure output is a valid JSON object matching the schema.")
            lines.append("- No markdown fences. No prose before/after JSON.")
        else:
            lines.append("- Analyze the error and implement a fix.")
            lines.append("- If the task is too complex, break it into smaller sub-tasks.")

        lines.append("")

    lines.append("## INSTRUCTIONS FOR THIS RUN")
    lines.append("")
    lines.append("1. Generate a repair-focused task plan that addresses the above failures.")
    lines.append("2. Each repair task should be atomic and independently verifiable.")
    lines.append("3. Include targeted test coverage for repaired functionality.")
    lines.append("4. Prioritize unblocking dependent tasks.")
    lines.append("")

    return "\n".join(lines)


def _run_parallel_with_auto_continue(
    *,
    args: argparse.Namespace,
    config: RunConfig,
    project_root: Path,
    schema_path: Path,
    system_prompt_path: Path,
    stats_ids: list[str],
    stats_id_set: set,
    system_prompt: str,
) -> int:
    """Run the parallel runner with auto-continue until success or max runs.

    This function implements SELF-HEALING behavior:
    - Does NOT immediately give up on planning failures (rc==2)
    - Generates repair context for failed tasks to guide subsequent runs
    - Has a high tolerance for stalled runs (max_stalled=10) before escalation
    - Continues burning credits to maximize chance of success
    """
    planner_profile = _normalize_planner_profile(getattr(args, "planner_profile", DEFAULT_PLANNER_PROFILE))
    if planner_profile == "engineering":
        print("[orchestrator] engineering profile: auto-continue disabled (single run only)")
        run_id = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        runs_dir = project_root / "runs" / run_id
        _git_init_if_needed(project_root)
        if not getattr(args, "no_agent_branch", False):
            _checkout_agent_branch(project_root, run_id)
        state = RunState(
            run_id=run_id,
            project_root=project_root,
            runs_dir=runs_dir,
            schema_path=schema_path,
            system_prompt_path=system_prompt_path,
            design_doc_path=(project_root / args.design_doc).resolve(),
            smoke_route=config.smoke_route,
            readonly=getattr(args, "readonly", False),
        )
        _ensure_dir(state.runs_dir)
        get_agent_policy().runs_dir = state.runs_dir
        return run_parallel(
            args=args,
            config=config,
            state=state,
            stats_ids=stats_ids,
            stats_id_set=stats_id_set,
            system_prompt=system_prompt,
        )
    max_runs = max(1, args.max_continuation_runs)
    run_count = 0
    last_summary: dict[str, Any] | None = None
    prev_failure_signatures: set = set()  # Track failure signatures, not just task IDs
    stalled_count = 0
    # SELF-HEALING: Reduced from 10 to 3 - stop early when signatures are stable
    max_stalled = 3
    planning_failure_count = 0
    max_planning_failures = 3  # Retry planning up to 3 times before escalating

    print(f"[orchestrator] AUTO-CONTINUE MODE (SELF-HEALING): max_runs={max_runs}, max_stalled={max_stalled}")

    while run_count < max_runs:
        run_count += 1
        print(f"\n{'=' * 80}")
        print(f"[orchestrator] AUTO-CONTINUE: Starting run {run_count}/{max_runs}")
        print(f"{'=' * 80}\n")

        # Create new run state for this iteration
        run_id = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        runs_dir = project_root / "runs" / run_id
        _ensure_dir(runs_dir)

        # Update agent policy with runs_dir for violation artifacts
        get_agent_policy().runs_dir = runs_dir

        # Handle continuation from previous run
        design_doc_path = (project_root / args.design_doc).resolve()
        if run_count > 1 and last_summary:
            # Use continuation prompt from previous run
            prev_runs_dir = Path(last_summary.get("run_dir", ""))
            continuation_path = prev_runs_dir / "continuation_prompt.txt"
            if continuation_path.exists():
                # Append continuation context to design doc
                continuation_text = continuation_path.read_text(encoding="utf-8")
                design_doc_text = design_doc_path.read_text(encoding="utf-8") if design_doc_path.exists() else ""
                augmented_doc = design_doc_text + "\n\n---\n\n" + continuation_text
                augmented_path = runs_dir / "augmented_design_doc.md"
                augmented_path.write_text(augmented_doc, encoding="utf-8")
                # Temporarily override design doc path
                original_design_doc = args.design_doc
                args.design_doc = str(augmented_path.relative_to(project_root))

        _git_init_if_needed(project_root)
        if not args.no_agent_branch:
            _checkout_agent_branch(project_root, run_id)

        state = RunState(
            run_id=run_id,
            project_root=project_root,
            runs_dir=runs_dir,
            schema_path=schema_path,
            system_prompt_path=system_prompt_path,
            design_doc_path=(project_root / args.design_doc).resolve(),
            smoke_route=config.smoke_route,
            readonly=getattr(args, "readonly", False),
        )

        # Run parallel
        rc = run_parallel(
            args=args,
            config=config,
            state=state,
            stats_ids=stats_ids,
            stats_id_set=stats_id_set,
            system_prompt=system_prompt,
        )

        # Restore design doc path if we modified it
        if run_count > 1 and "original_design_doc" in dir():
            args.design_doc = original_design_doc

        # Check result
        summary_path = runs_dir / "summary.json"
        if summary_path.exists():
            last_summary = json.loads(summary_path.read_text(encoding="utf-8"))
        else:
            last_summary = None

        if rc == 0:
            print(f"\n[orchestrator] AUTO-CONTINUE: SUCCESS after {run_count} run(s)")
            return 0

        # SELF-HEALING: Planning failures (rc == 2) get retried with repair context
        # We do NOT immediately give up - we burn credits to maximize success chance
        if rc == 2:
            planning_failure_count += 1
            print(
                f"\n[orchestrator] AUTO-CONTINUE: Planning failure (rc=2), attempt {planning_failure_count}/{max_planning_failures}"
            )

            if planning_failure_count >= max_planning_failures:
                print(f"\n[orchestrator] AUTO-CONTINUE: ESCALATING - Planning failed {planning_failure_count} times")
                print("[orchestrator] This may be a structural issue with the design document.")
                # Print debug file locations for the user
                plan_debug = runs_dir / "task_plan.extract_debug.txt"
                raw_stream = runs_dir / "task_plan.json.wrapper_schema_claude_raw_stream.txt"
                if plan_debug.exists():
                    print(f"[orchestrator] Debug file: {plan_debug}")
                if raw_stream.exists():
                    print(f"[orchestrator] Raw stream: {raw_stream}")
                print("[orchestrator] Check the files above to diagnose the planning failure.")
                _write_escalation_file(
                    runs_dir,
                    {
                        "root_failures": [
                            {
                                "id": "PLANNING",
                                "title": "Planning Step",
                                "error": "Planning step failed repeatedly",
                                "agent": "unknown",
                            }
                        ]
                    },
                )
                return 2

            # SELF-HEALING: Generate repair context for planning issues
            print("[orchestrator] AUTO-CONTINUE: Retrying planning with simplified context...")
            # Continue to next iteration - the repair context will help
            continue

        # Check for progress - compare failure SIGNATURES (not just task IDs)
        # This prevents treating "different task IDs with same underlying error" as progress.
        if last_summary:
            current_signatures = _compute_failure_signatures_from_summary(last_summary)
            {t["id"] for t in last_summary.get("root_failures", [])}

            # Compare signatures - same signatures mean same underlying errors
            if current_signatures == prev_failure_signatures:
                stalled_count += 1
                print(
                    f"[orchestrator] AUTO-CONTINUE: No progress (same failure signatures). Stalled count: {stalled_count}/{max_stalled}"
                )
                print(f"[orchestrator] Signatures: {current_signatures}")

                # SELF-HEALING: Generate repair context for stalled failures
                root_failures = last_summary.get("root_failures", [])
                if root_failures:
                    repair_context = _generate_repair_context_for_failures(root_failures, runs_dir)
                    repair_path = runs_dir / "repair_context.md"
                    repair_path.write_text(repair_context, encoding="utf-8")
                    print(f"[orchestrator] AUTO-CONTINUE: Generated repair context at {repair_path}")

                if stalled_count >= max_stalled:
                    print(f"\n[orchestrator] AUTO-CONTINUE: ESCALATING - no progress for {stalled_count} consecutive runs")
                    print("[orchestrator] Root failures that could not be resolved automatically:")
                    for t in last_summary.get("root_failures", []):
                        print(f"  - {t['id']}: {t.get('error', 'unknown')}")
                    _write_escalation_file(runs_dir, last_summary)
                    return 1
            else:
                stalled_count = 0
                planning_failure_count = 0  # Reset planning failures on progress
                print("[orchestrator] AUTO-CONTINUE: Progress detected (different failure signatures)")
                print(f"[orchestrator] Previous: {prev_failure_signatures}")
                print(f"[orchestrator] Current: {current_signatures}")

            prev_failure_signatures = current_signatures
        else:
            stalled_count += 1

        print(f"[orchestrator] AUTO-CONTINUE: Run {run_count} incomplete, continuing with self-healing...")

    print(f"\n[orchestrator] AUTO-CONTINUE: GIVING UP - max runs ({max_runs}) reached")
    if last_summary:
        _write_escalation_file(project_root / "runs" / "escalation", last_summary)
    return 1


def _write_escalation_file(runs_dir: Path, summary: dict[str, Any]) -> None:
    """Write an escalation file explaining what's stuck and how to proceed."""
    _ensure_dir(runs_dir)
    escalation_path = runs_dir / "ESCALATION_REQUIRED.md"

    lines = [
        "# Manual Escalation Required",
        "",
        "The auto-continue loop could not resolve the following failures:",
        "",
    ]

    for t in summary.get("root_failures", []):
        lines.append(f"## {t['id']}: {t.get('title', 'Unknown')}")
        lines.append(f"- Status: {t['status']}")
        lines.append(f"- Agent: {t['agent']}")
        if t.get("error"):
            lines.append(f"- Error: {t['error']}")
        if t.get("manual_path"):
            lines.append(f"- Manual file: {t['manual_path']}")
        lines.append("")
        lines.append("### To run this task manually:")
        lines.append("```bash")
        lines.append(f"./run_parallel.sh --only-task {t['id']} --max-workers 1 --allow-resource-intensive")
        lines.append("```")
        lines.append("")

    lines.extend(
        [
            "## Next Steps",
            "",
            "1. Review the manual files in runs/*/manual/",
            "2. Run failing tasks individually with --only-task",
            "3. Once fixed, re-run the full parallel run",
            "",
        ]
    )

    escalation_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"[orchestrator] Escalation file written to: {escalation_path}")


# -----------------------------
# Main
# -----------------------------


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--project-root", default=".")
    ap.add_argument("--config", default="bridge/config.json")
    ap.add_argument("--schema", default="bridge/turn.schema.json")
    ap.add_argument("--system-prompt", default="bridge/prompts/system.md")
    ap.add_argument("--design-doc", default="DESIGN_DOCUMENT.md")
    ap.add_argument(
        "--design-doc-contract",
        choices=["strict", "loose", "off"],
        default="loose",
        help="Design doc contract validation mode: strict (fail if missing fields), "
        "loose (warn but continue), off (no validation)",
    )
    ap.add_argument(
        "--milestone-id",
        type=str,
        default=None,
        help="Override milestone ID from design doc (required if strict mode and parser can't infer)",
    )
    ap.add_argument("--start-agent", choices=AGENTS, default="codex")
    ap.add_argument(
        "--smoke-route",
        type=_parse_smoke_route_arg,
        default=None,
        help="Comma-separated agent route for smoke tests (e.g. 'codex,claude').",
    )
    ap.add_argument("--mode", choices=["live", "mock"], default="mock")
    ap.add_argument("--runner", choices=["sequential", "parallel"], default="sequential")
    # Parallel runner knobs (safe defaults; can override on CLI)
    ap.add_argument("--task-plan-schema", default="bridge/task_plan.schema.json")
    ap.add_argument("--max-workers", type=int, default=0, help="0=auto")
    ap.add_argument("--only-task", action="append", default=[], help="Run only specific task id(s) from the generated plan")
    ap.add_argument(
        "--allow-resource-intensive", action="store_true", help="Do not auto-kill tasks that exceed resource thresholds"
    )
    ap.add_argument(
        "--planner-profile",
        choices=["balanced", "throughput", "engineering"],
        default="balanced",
        help="Planner profile: 'balanced' (default) for conservative planning, "
        "'throughput' for many small parallel tasks to maximize worker utilization, "
        "'engineering' for fail-fast, no-backfill execution with durable reports",
    )
    ap.add_argument("--cpu-threshold", type=float, default=0.0, help="Override CPU%% threshold for auto-stop (0=use config)")
    ap.add_argument("--mem-threshold", type=float, default=0.0, help="Override RAM%% threshold for auto-stop (0=use config)")
    ap.add_argument("--terminal-max-bytes", type=int, default=0, help="Override per-worker terminal output cap (0=use config)")
    ap.add_argument(
        "--terminal-max-line-len", type=int, default=0, help="Override per-worker terminal line length cap (0=use config)"
    )
    ap.add_argument("--mock-scenario", default="bridge/mock_scenarios/milestone_demo.json")
    ap.add_argument("--verbose", action="store_true")
    ap.add_argument(
        "--stream-agent-output",
        choices=["stdout", "stderr", "both", "none"],
        default="none",
        help="Stream agent output to console (stdout, stderr, both, none)",
    )
    ap.add_argument("--no-agent-branch", action="store_true")
    ap.add_argument(
        "--readonly",
        action="store_true",
        help="Force read-only mode: ignore needs_write_access and keep WRITE_ACCESS=0.",
    )
    ap.add_argument(
        "--selftest-parallel",
        action="store_true",
        help="Run selftest mode: synthetic tasks with trivial commands to verify scheduler",
    )
    # Auto-continue support
    # Default: enabled if ORCH_AUTO_CONTINUE=1 is set, otherwise disabled
    auto_continue_default = os.environ.get("ORCH_AUTO_CONTINUE", "").strip() in ("1", "true", "yes")
    ap.add_argument(
        "--auto-continue",
        action="store_true",
        default=auto_continue_default,
        help="Automatically continue with new runs until all tasks complete or max runs reached. "
        "Also enabled by ORCH_AUTO_CONTINUE=1 environment variable.",
    )
    ap.add_argument("--no-auto-continue", action="store_true", help="Disable auto-continue even if ORCH_AUTO_CONTINUE is set")
    ap.add_argument(
        "--max-continuation-runs", type=int, default=5, help="Maximum number of continuation runs before giving up (default: 5)"
    )
    ap.add_argument(
        "--continuation",
        type=str,
        default="",
        help="Path to continuation_prompt.txt from a previous run (use instead of design doc)",
    )

    # Agent-only flags for hard enforcement
    ap.add_argument(
        "--only-codex", action="store_true", help="ONLY use Codex agent for ALL operations (planner, workers, fallbacks)"
    )
    ap.add_argument(
        "--only-claude", action="store_true", help="ONLY use Claude agent for ALL operations (planner, workers, fallbacks)"
    )

    # Robustness flags for unattended runs
    ap.add_argument(
        "--auto-stash",
        action="store_true",
        help="Auto-stash uncommitted changes before starting (prints stash ref, does NOT auto-pop)",
    )
    ap.add_argument(
        "--verify-mode",
        choices=["strict", "skip-git", "off"],
        default="strict",
        help="Verification mode: strict (fail on dirty repo), skip-git (skip git checks during run), off (no verification)",
    )
    ap.add_argument(
        "--force-dirty",
        action="store_true",
        help="Allow starting with dirty repo (DANGEROUS: may cause verify stalls, use with --verify-mode=skip-git)",
    )
    ap.add_argument(
        "--max-repair-attempts",
        type=int,
        default=3,
        help="Maximum verify auto-repair attempts before giving up (default: 3)",
    )

    args = ap.parse_args()

    # Validate mutually exclusive agent flags
    if args.only_codex and args.only_claude:
        print("[orchestrator] ERROR: --only-codex and --only-claude are mutually exclusive")
        sys.exit(1)

    if os.environ.get("FF_SKIP_VERIFY") == "1":
        args.verify_mode = "off"

    # Planner profile override via environment (unless explicitly set on CLI)
    env_profile = os.environ.get("ORCH_PLANNER_PROFILE", "").strip().lower()
    if env_profile and "--planner-profile" not in sys.argv:
        normalized_env = _normalize_planner_profile(env_profile)
        if normalized_env != env_profile:
            print(f"[orchestrator] WARNING: ORCH_PLANNER_PROFILE '{env_profile}' not recognized; using {normalized_env}")
        args.planner_profile = normalized_env

    # Normalize planner profile for downstream logic
    args.planner_profile = _normalize_planner_profile(args.planner_profile)

    readonly_env = os.environ.get("ORCH_READONLY", "").strip().lower() in ("1", "true", "yes")
    args.readonly = args.readonly or readonly_env
    if args.readonly and not args.no_agent_branch:
        print("[orchestrator] READONLY: enabling --no-agent-branch")
        args.no_agent_branch = True

    # Handle --no-auto-continue override
    if args.no_auto_continue:
        args.auto_continue = False

    if args.planner_profile == "engineering" and args.auto_continue:
        print("[orchestrator] engineering profile: disabling auto-continue (fail-fast mode)")
        args.auto_continue = False

    project_root = Path(args.project_root).resolve()
    config = load_config(project_root / args.config)
    config = dataclasses.replace(config, smoke_route=tuple(args.smoke_route or ()))

    if args.readonly and (args.runner == "parallel" or args.selftest_parallel):
        print("[orchestrator] ERROR: --readonly is only supported for sequential runner; use worktree isolation for parallel.")
        return 2

    # Initialize agent policy based on --only-* flags
    forced_agent: str | None = None
    if args.only_codex:
        forced_agent = "codex"
    elif args.only_claude:
        forced_agent = "claude"

    if forced_agent:
        # Override start-agent to match forced agent
        args.start_agent = forced_agent
        print(f"[orchestrator] AGENT POLICY: --only-{forced_agent} active. ALL operations will use {forced_agent} only.")

    # Note: runs_dir is set later, so we'll update the policy then
    policy = AgentPolicy(
        forced_agent=forced_agent,
        allowed_agents=tuple(config.enable_agents) if config.enable_agents else AGENTS,
    )
    set_agent_policy(policy)

    schema_path = project_root / args.schema
    system_prompt_path = project_root / args.system_prompt
    design_doc_path = (project_root / args.design_doc).resolve()

    run_id = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    runs_dir = project_root / "runs" / run_id
    _ensure_dir(runs_dir)

    # Update agent policy with runs_dir for violation artifacts
    get_agent_policy().runs_dir = runs_dir

    _git_init_if_needed(project_root)
    if not args.no_agent_branch:
        _checkout_agent_branch(project_root, run_id)

    state = RunState(
        run_id=run_id,
        project_root=project_root,
        runs_dir=runs_dir,
        schema_path=schema_path,
        system_prompt_path=system_prompt_path,
        design_doc_path=design_doc_path,
        smoke_route=config.smoke_route,
        readonly=args.readonly,
    )

    stats_md_path = project_root / "STATS.md"
    stats_md = _read_text(stats_md_path) if stats_md_path.exists() else ""
    stats_ids = _extract_stats_ids(stats_md)
    stats_id_set = set(stats_ids)

    system_prompt, effective_prompt_path = _load_system_prompt(
        project_root=project_root,
        system_prompt_path=system_prompt_path,
        planner_profile=args.planner_profile,
    )
    if effective_prompt_path != system_prompt_path:
        system_prompt_path = effective_prompt_path

    if args.runner == "parallel" or args.selftest_parallel:
        if not args.selftest_parallel and args.mode != "live":
            print("[orchestrator] ERROR: parallel runner supports --mode live only.")
            return 2

        # Auto-continue loop
        if args.auto_continue:
            return _run_parallel_with_auto_continue(
                args=args,
                config=config,
                project_root=project_root,
                schema_path=schema_path,
                system_prompt_path=system_prompt_path,
                stats_ids=stats_ids,
                stats_id_set=stats_id_set,
                system_prompt=system_prompt,
            )
        else:
            return run_parallel(
                args=args, config=config, state=state, stats_ids=stats_ids, stats_id_set=stats_id_set, system_prompt=system_prompt
            )

    # Mock scenario support.
    scenario: dict[str, Any] = {}
    mock_indices: dict[str, int] = {a: 0 for a in AGENTS}
    if args.mode == "mock":
        scenario = _load_json(project_root / args.mock_scenario)

    next_prompt = textwrap.dedent(
        """
        You are at the start of the milestone run.

        1) Read STATS.md.
        2) Read DESIGN_DOCUMENT.md and identify the milestone ID + all normative requirements.
        3) If the design document is not in the required format, rewrite it to comply.
        4) Propose the next smallest implementation step that can be proven with pytest.

        IMPORTANT: always keep the Test Matrix up to date and runnable.
        """
    ).strip()

    agent = args.start_agent
    if config.smoke_route:
        if agent != config.smoke_route[0]:
            print(f"[orchestrator] SMOKE ROUTE: overriding --start-agent {agent} -> {config.smoke_route[0]}")
        agent = config.smoke_route[0]

    print(f"[orchestrator] run_id={state.run_id} mode={args.mode} project_root={project_root}")
    print(
        f"[orchestrator] limits: max_calls_per_agent={config.max_calls_per_agent} "
        f"quota_retry_attempts={config.quota_retry_attempts} max_total_calls={config.max_total_calls}"
    )

    call_no = 0

    # Tracks consecutive JSON parse/validation failures for the current agent.
    correction_agent: str | None = None
    correction_attempts = 0

    while state.total_calls < config.max_total_calls:
        if state.call_counts.get(agent, 0) >= config.max_calls_per_agent:
            fb = _pick_fallback(config, state, current_agent=agent)
            if not fb:
                print("[orchestrator] ERROR: all agents exhausted call cap; stopping.")
                return 2
            print(f"[orchestrator] OVERRIDE: agent '{agent}' exceeded call cap; switching to '{fb}'")
            agent = fb

        call_no += 1
        state.total_calls += 1
        state.call_counts[agent] += 1

        call_dir = runs_dir / "calls" / f"call_{call_no:04d}"
        _ensure_dir(call_dir)

        design_doc_text = _read_text(design_doc_path) if design_doc_path.exists() else ""
        milestone_id = _parse_milestone_id(design_doc_text)

        verify_json = call_dir / "verify.json"
        if os.environ.get("FF_SKIP_VERIFY") == "1":
            verify_report_text = "{}"
        else:
            verify_rc, verify_out, verify_err = _run_verify(project_root, verify_json, strict_git=False)
            if verify_rc != 0 and not verify_json.exists():
                # Ensure something is present for embedding.
                _write_text(verify_json, json.dumps({"error": (verify_out + "\n" + verify_err).strip()}))
            verify_report_text = _read_text(verify_json) if verify_json.exists() else "{}"

        repo_info = _git_snapshot(project_root)

        prompt_text = build_prompt(
            agent=agent,
            system_prompt=system_prompt,
            design_doc_text=design_doc_text,
            milestone_id=milestone_id,
            repo_info=repo_info,
            verify_report_text=verify_report_text,
            history=state.history,
            next_prompt=next_prompt,
            call_counts=state.call_counts,
            disabled_by_quota=state.disabled_by_quota,
            stats_ids=stats_ids,
            readonly=state.readonly,
            planner_profile=args.planner_profile,
        )

        prompt_path = call_dir / "prompt.txt"
        raw_path = call_dir / "raw.txt"
        out_path = call_dir / "out.json"
        _write_text(prompt_path, prompt_text)

        print("=" * 88)
        model = config.agent_models.get(agent, "(default)")
        effective_write_access = state.grant_write_access and not state.readonly
        print(
            f"CALL {call_no:04d} | agent={agent} | total_calls={state.total_calls} | "
            f"agent_calls={state.call_counts[agent]}/{config.max_calls_per_agent} | "
            f"write_access={'1' if effective_write_access else '0'} | "
            f"readonly={'1' if state.readonly else '0'}"
        )
        print(f"[orchestrator] TURN (agent={agent} model={model})")

        if args.mode == "mock":
            rc, out, err = _run_agent_mock(agent=agent, scenario=scenario, mock_indices=mock_indices)
        else:
            rc, out, err = _run_agent_live(
                agent=agent,
                prompt_path=prompt_path,
                schema_path=schema_path,
                out_path=out_path,
                config=config,
                state=state,
                stream_agent_output=args.stream_agent_output,
                call_dir=call_dir,
            )

        _write_text(raw_path, (out or "") + ("\n" if out and err else "") + (err or ""))

        if args.verbose and err.strip():
            print("[orchestrator] stderr:\n" + err.strip())

        if rc != 0:
            combined = (out + "\n" + err).strip()
            quota = _is_quota_error(agent, combined, config)
            label = "QUOTA" if quota else "ERROR"
            print(f"[orchestrator] {label}: agent '{agent}' exited rc={rc}")

            if quota:
                state.quota_failures[agent] += 1
                remaining = config.quota_retry_attempts - state.quota_failures[agent]
                if remaining > 0 and state.call_counts[agent] < config.max_calls_per_agent:
                    print(
                        f"[orchestrator] quota retry {state.quota_failures[agent]}/"
                        f"{config.quota_retry_attempts} for agent '{agent}' (remaining={remaining})"
                    )
                    continue

                print(f"[orchestrator] DISABLE: agent '{agent}' marked disabled_by_quota=true")
                state.disabled_by_quota[agent] = True
                fb = _pick_fallback(config, state, current_agent=agent)
                print(f"[orchestrator] FAILOVER: switching to '{fb}' with same work item")
                agent = fb
                continue

            fb = _pick_fallback(config, state, current_agent=agent)
            print(f"[orchestrator] FAILOVER: switching to '{fb}' after error")
            agent = fb
            continue

        # Parse and validate JSON using TurnNormalizer for robustness.
        try:
            out_for_parse = out
            if out_path.exists() and out_path.stat().st_size > 0:
                out_for_parse = _read_text(out_path)

            # Use TurnNormalizer for robust payload extraction
            norm_result = normalize_agent_output(
                out_for_parse,
                expected_agent=agent,
                expected_milestone_id=milestone_id,
                stats_id_set=stats_id_set,
                default_phase="implement",
            )

            if norm_result.success and norm_result.turn:
                turn_obj = norm_result.turn
                # Log normalization warnings (auto-corrections)
                for warning in norm_result.warnings:
                    print(f"[orchestrator] NORMALIZED: {warning}")

                # Use lenient validation (auto-corrects mismatches with warnings)
                ok, msg, val_warnings = _validate_turn_lenient(
                    turn_obj,
                    expected_agent=agent,
                    expected_milestone_id=milestone_id,
                    stats_id_set=stats_id_set,
                )
                for warning in val_warnings:
                    print(f"[orchestrator] VALIDATION: {warning}")
                if not ok:
                    raise ValueError(msg)
            else:
                # Normalization failed - try legacy direct parsing
                turn_obj = _try_parse_json(out_for_parse)
                ok, msg, val_warnings = _validate_turn_lenient(
                    turn_obj,
                    expected_agent=agent,
                    expected_milestone_id=milestone_id,
                    stats_id_set=stats_id_set,
                )
                for warning in val_warnings:
                    print(f"[orchestrator] VALIDATION: {warning}")
                if not ok:
                    raise ValueError(msg)
        except Exception as e:
            print(f"[orchestrator] JSON INVALID: {e}")

            if correction_agent != agent:
                correction_agent = agent
                correction_attempts = 0
            correction_attempts += 1

            if correction_attempts <= config.max_json_correction_attempts:
                print(
                    f"[orchestrator] correction attempt {correction_attempts}/{config.max_json_correction_attempts} (same agent)"
                )
                prev_prompt = next_prompt
                next_prompt = textwrap.dedent(
                    f"""
                    Your previous response could not be parsed/validated as a single JSON object matching bridge/turn.schema.json.

                    Error: {e}

                    Return ONLY one JSON object (no prose, no markdown, no code fences) that satisfies the schema.
                    Re-answer the ORIGINAL PROMPT below:

                    --- ORIGINAL PROMPT START ---
                    {prev_prompt}
                    --- ORIGINAL PROMPT END ---
                    """
                ).strip()
                continue

            correction_agent = None
            correction_attempts = 0

            fb = _pick_fallback(config, state, current_agent=agent)
            print(f"[orchestrator] FAILOVER: switching to '{fb}'")
            agent = fb
            continue

        # Successful parse/validate.
        correction_agent = None
        correction_attempts = 0

        turn_path = call_dir / "turn.json"
        _write_text(turn_path, json.dumps(turn_obj, indent=2, sort_keys=True))
        state.history.append(turn_obj)

        print(f"[orchestrator] summary: {turn_obj['summary']}")

        # Update write-access grant for next call (readonly mode blocks escalation).
        requested_write_access = bool(turn_obj.get("needs_write_access", False))
        if state.readonly and requested_write_access:
            print("[orchestrator] READONLY: ignoring needs_write_access request")
        state.grant_write_access = requested_write_access and not state.readonly

        # Completion gates: only stop if they actually pass.
        if bool(turn_obj["project_complete"]):
            ok, msg = _completion_gates_ok(project_root)
            if ok:
                print("[orchestrator] PROJECT COMPLETE (gates passed)")
                state_path = runs_dir / "state.json"
                _write_text(state_path, json.dumps(dataclasses.asdict(state), indent=2, sort_keys=True, default=str))
                return 0

            print("[orchestrator] PROJECT_COMPLETE rejected: completion gates failed")
            print(_truncate(msg, 2000))
            next_prompt = textwrap.dedent(
                f"""
                You attempted to mark the project complete, but completion gates failed.

                Gate failure details:
                {msg}

                Fix the repo so that:
                - python -m tools.verify --strict-git passes
                - git status --porcelain is empty
                - changes are committed

                Then try completion again.
                """
            ).strip()

            # Heuristic: if it looks like only commit/cleanup, ask Claude; otherwise Codex.
            heuristic_agent = "claude" if "git status not clean" in msg or "committed" in msg else "codex"
            agent = get_agent_policy().enforce(heuristic_agent, "project completion failure heuristic")
            continue

        # Decide next agent.
        requested = str(turn_obj["next_agent"])
        effective, override_reason = _override_next_agent(requested, config, state)
        if override_reason:
            print(f"[orchestrator] OVERRIDE next_agent: {requested} -> {effective} ({override_reason})")
        else:
            print(f"[orchestrator] next_agent={effective} (as requested)")

        next_prompt = str(turn_obj["next_prompt"]).strip()
        agent = effective

    print("[orchestrator] ERROR: max_total_calls exceeded")
    return 6


if __name__ == "__main__":
    raise SystemExit(main())

================================================================================
 FILE: merge_resolver.py
================================================================================

#!/usr/bin/env python3
"""Agent-driven merge conflict resolver.

This module provides intelligent merge conflict resolution using Claude
to understand and resolve conflicts while preserving intent from both sides.

Key features:
- Detects conflict files via git status (UU paths)
- Creates a merge-resolution context for Claude
- Runs a Claude repair pass that resolves conflict markers
- Retries after resolution with bounded attempts
- Falls back to manual only after N attempts fail
"""

from __future__ import annotations

import json
import os
import re
import subprocess
import textwrap
import threading
from collections.abc import Callable
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any


@dataclass
class ConflictFile:
    """Represents a file with merge conflicts."""

    path: str
    content: str
    conflict_count: int
    ours_content: str  # Content from our branch
    theirs_content: str  # Content from their branch


@dataclass
class MergeResolutionResult:
    """Result of a merge resolution attempt."""

    success: bool
    resolved_files: list[str] = field(default_factory=list)
    unresolved_files: list[str] = field(default_factory=list)
    error: str | None = None
    attempt: int = 0
    agent_output: str = ""


def _run_cmd(
    cmd: list[str],
    cwd: Path | str,
    env: dict[str, str] | None = None,
    timeout: int = 60,
) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    try:
        result = subprocess.run(
            cmd,
            cwd=str(cwd),
            env=env or os.environ.copy(),
            capture_output=True,
            text=True,
            timeout=timeout,
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", "Command timed out"
    except Exception as e:
        return -1, "", str(e)


def detect_conflict_files(project_root: Path) -> list[str]:
    """Detect files with merge conflicts via git status.

    Returns list of file paths with UU (unmerged) status.
    """
    rc, stdout, _ = _run_cmd(["git", "status", "--porcelain=v1"], cwd=project_root)
    if rc != 0:
        return []

    conflict_files = []
    for line in stdout.strip().split("\n"):
        if not line or len(line) < 3:
            continue
        status = line[:2]
        file_path = line[3:].strip()
        # UU = unmerged, both modified (conflict)
        # AA = unmerged, both added
        # DD = unmerged, both deleted
        if status in ("UU", "AA", "UD", "DU"):
            conflict_files.append(file_path)

    return conflict_files


def parse_conflict_file(project_root: Path, file_path: str) -> ConflictFile | None:
    """Parse a file with merge conflicts and extract conflict regions.

    Returns ConflictFile with extracted ours/theirs content.
    """
    full_path = project_root / file_path
    if not full_path.exists():
        return None

    try:
        content = full_path.read_text(encoding="utf-8", errors="replace")
    except Exception:
        return None

    # Count conflict markers
    conflict_count = content.count("<<<<<<< ")
    if conflict_count == 0:
        return None

    # Extract ours and theirs sections (simplified extraction)
    ours_parts = []
    theirs_parts = []

    conflict_pattern = re.compile(r"<<<<<<< ([^\n]+)\n(.*?)=======\n(.*?)>>>>>>> ([^\n]+)", re.DOTALL)

    for match in conflict_pattern.finditer(content):
        ours_parts.append(match.group(2))
        theirs_parts.append(match.group(3))

    return ConflictFile(
        path=file_path,
        content=content,
        conflict_count=conflict_count,
        ours_content="\n".join(ours_parts),
        theirs_content="\n".join(theirs_parts),
    )


def build_merge_resolution_prompt(
    conflicts: list[ConflictFile],
    task_context: str = "",
    milestone_id: str = "M0",
) -> str:
    """Build a prompt for Claude to resolve merge conflicts."""

    conflict_sections = []
    for cf in conflicts:
        conflict_sections.append(f"""
### File: {cf.path} ({cf.conflict_count} conflict(s))

```
{cf.content}
```

The conflict markers are:
- `<<<<<<< HEAD` or `<<<<<<< ours`: Start of our version
- `=======`: Separator
- `>>>>>>> theirs` or similar: End of their version

Your task: Resolve this conflict by combining BOTH versions intelligently:
- Preserve functionality from BOTH sides
- Don't lose any code changes from either side
- Make the result syntactically valid and functional
""")

    return textwrap.dedent(f"""
        # Merge Conflict Resolution Task

        You are resolving merge conflicts in a codebase. Your goal is to produce
        a clean, working version of each conflicted file that preserves the intent
        and changes from BOTH sides of the merge.

        ## Context
        {task_context}

        ## Conflicts to Resolve
        {"".join(conflict_sections)}

        ## Instructions

        1. For each conflicted file, produce the RESOLVED content.
        2. Remove all conflict markers (<<<<<<, =======, >>>>>>>).
        3. Combine changes intelligently - don't just pick one side.
        4. Ensure the result is syntactically valid.
        5. If you cannot resolve a conflict safely, explain why.

        ## Output Format

        Output a JSON object with this structure:
        {{
            "resolutions": [
                {{
                    "path": "path/to/file.py",
                    "resolved_content": "full resolved file content here",
                    "notes": "brief explanation of how you resolved conflicts"
                }}
            ],
            "unresolvable": [
                {{
                    "path": "path/to/other.py",
                    "reason": "why this conflict cannot be safely auto-resolved"
                }}
            ]
        }}

        Output ONLY the JSON object, no markdown fences or extra text.
    """).strip()


AgentRunnerCallback = Callable[
    [list[ConflictFile], str, str, int],  # conflicts, task_context, milestone_id, attempt
    dict[str, Any],  # resolution JSON output
]


class MergeResolver:
    """Agent-driven merge conflict resolver.

    This resolver uses Claude to intelligently resolve merge conflicts,
    preserving intent from both sides of the merge.

    For testing, an agent_runner callback can be injected to avoid calling real agents.
    """

    def __init__(
        self,
        project_root: Path,
        runs_dir: Path,
        max_attempts: int = 3,
        agent_script: str = "bridge/agents/claude.sh",
        agent_runner: AgentRunnerCallback | None = None,
    ):
        """Initialize merge resolver.

        Args:
            project_root: Path to the git repository
            runs_dir: Directory for resolution artifacts
            max_attempts: Maximum resolution attempts
            agent_script: Path to the agent script (relative to project_root)
            agent_runner: Optional callback for testing. If provided, this is called
                         instead of invoking the real agent script. The callback
                         receives (conflicts, task_context, milestone_id, attempt)
                         and should return a resolution JSON dict.
        """
        self.project_root = project_root
        self.runs_dir = runs_dir
        self.max_attempts = max_attempts
        self.agent_script = agent_script
        self.agent_runner = agent_runner  # For dependency injection in tests
        self._lock = threading.Lock()

    def resolve_conflicts(
        self,
        task_id: str = "",
        task_context: str = "",
        milestone_id: str = "M0",
    ) -> MergeResolutionResult:
        """Attempt to resolve all merge conflicts in the repository.

        Args:
            task_id: ID of the task that caused the conflicts
            task_context: Additional context about what the merge was trying to do
            milestone_id: Current milestone ID

        Returns:
            MergeResolutionResult with resolution status
        """
        with self._lock:
            return self._resolve_conflicts_impl(task_id, task_context, milestone_id)

    def _resolve_conflicts_impl(
        self,
        task_id: str,
        task_context: str,
        milestone_id: str,
    ) -> MergeResolutionResult:
        """Internal implementation of conflict resolution."""

        # Detect conflicted files
        conflict_paths = detect_conflict_files(self.project_root)
        if not conflict_paths:
            return MergeResolutionResult(
                success=True,
                resolved_files=[],
                unresolved_files=[],
                error=None,
                attempt=0,
            )

        print(f"[merge_resolver] Detected {len(conflict_paths)} conflicted file(s): {conflict_paths}")

        # Parse conflict files
        conflicts = []
        for path in conflict_paths:
            cf = parse_conflict_file(self.project_root, path)
            if cf:
                conflicts.append(cf)

        if not conflicts:
            # Files have UU status but no conflict markers - try simple git add
            for path in conflict_paths:
                _run_cmd(["git", "add", path], cwd=self.project_root)
            return MergeResolutionResult(
                success=True,
                resolved_files=conflict_paths,
                unresolved_files=[],
                attempt=0,
            )

        # Try agent-based resolution with bounded attempts
        for attempt in range(1, self.max_attempts + 1):
            print(f"[merge_resolver] Attempt {attempt}/{self.max_attempts}")

            result = self._run_agent_resolution(conflicts, task_context, milestone_id, attempt)

            if result.success:
                return result

            # Check if there are still conflicts after this attempt
            remaining = detect_conflict_files(self.project_root)
            if not remaining:
                result.success = True
                return result

            print(f"[merge_resolver] {len(remaining)} conflict(s) remain after attempt {attempt}")

        # All attempts failed
        return MergeResolutionResult(
            success=False,
            resolved_files=[],
            unresolved_files=conflict_paths,
            error=f"Failed to resolve conflicts after {self.max_attempts} attempts",
            attempt=self.max_attempts,
        )

    def _run_agent_resolution(
        self,
        conflicts: list[ConflictFile],
        task_context: str,
        milestone_id: str,
        attempt: int,
    ) -> MergeResolutionResult:
        """Run a single agent resolution attempt."""

        # Create resolution workspace
        resolution_dir = self.runs_dir / f"merge_resolution_{attempt}"
        resolution_dir.mkdir(parents=True, exist_ok=True)

        # Build prompt
        prompt = build_merge_resolution_prompt(
            conflicts=conflicts,
            task_context=task_context,
            milestone_id=milestone_id,
        )

        prompt_path = resolution_dir / "prompt.txt"
        prompt_path.write_text(prompt, encoding="utf-8")

        # Prepare schema (simple JSON schema for resolution output)
        schema = {
            "type": "object",
            "properties": {
                "resolutions": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "path": {"type": "string"},
                            "resolved_content": {"type": "string"},
                            "notes": {"type": "string"},
                        },
                        "required": ["path", "resolved_content"],
                    },
                },
                "unresolvable": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "path": {"type": "string"},
                            "reason": {"type": "string"},
                        },
                        "required": ["path", "reason"],
                    },
                },
            },
            "required": ["resolutions"],
        }

        schema_path = resolution_dir / "resolution.schema.json"
        schema_path.write_text(json.dumps(schema, indent=2), encoding="utf-8")

        out_path = resolution_dir / "resolution.json"

        # Get agent output - either from callback (for testing) or real agent
        output: dict[str, Any] | None = None

        if self.agent_runner is not None:
            # Use injected callback (for testing)
            print("[merge_resolver] Using injected agent_runner callback...")
            try:
                output = self.agent_runner(conflicts, task_context, milestone_id, attempt)
                # Save output for debugging
                out_path.write_text(json.dumps(output, indent=2), encoding="utf-8")
            except Exception as e:
                return MergeResolutionResult(
                    success=False,
                    unresolved_files=[cf.path for cf in conflicts],
                    error=f"Agent callback failed: {e}",
                    attempt=attempt,
                )
        else:
            # Run real agent
            agent_script_path = self.project_root / self.agent_script
            if not agent_script_path.exists():
                return MergeResolutionResult(
                    success=False,
                    unresolved_files=[cf.path for cf in conflicts],
                    error=f"Agent script not found: {agent_script_path}",
                    attempt=attempt,
                )

            env = os.environ.copy()
            # Use generic JSON mode (NOT task_plan which expects milestone_id/tasks/etc.)
            # The "json" mode outputs pure JSON matching arbitrary schemas without task_plan keys
            env["ORCH_SCHEMA_KIND"] = "json"

            cmd = [str(agent_script_path), str(prompt_path), str(schema_path), str(out_path)]

            print("[merge_resolver] Running agent for conflict resolution...")
            rc, stdout, stderr = _run_cmd(cmd, cwd=self.project_root, env=env, timeout=300)

            if rc != 0:
                return MergeResolutionResult(
                    success=False,
                    unresolved_files=[cf.path for cf in conflicts],
                    error=f"Agent failed with exit code {rc}: {stderr[:500]}",
                    attempt=attempt,
                    agent_output=stdout[:2000],
                )

            # Parse agent output
            if not out_path.exists():
                return MergeResolutionResult(
                    success=False,
                    unresolved_files=[cf.path for cf in conflicts],
                    error="Agent did not produce output file",
                    attempt=attempt,
                )

            try:
                output = json.loads(out_path.read_text(encoding="utf-8"))
            except json.JSONDecodeError as e:
                return MergeResolutionResult(
                    success=False,
                    unresolved_files=[cf.path for cf in conflicts],
                    error=f"Failed to parse agent output: {e}",
                    attempt=attempt,
                )

        if output is None:
            return MergeResolutionResult(
                success=False,
                unresolved_files=[cf.path for cf in conflicts],
                error="No output from agent",
                attempt=attempt,
            )

        # Apply resolutions
        resolved = []
        unresolved = [cf.path for cf in conflicts]

        resolutions = output.get("resolutions", [])
        for resolution in resolutions:
            path = resolution.get("path", "")
            content = resolution.get("resolved_content", "")

            if not path or not content:
                continue

            full_path = self.project_root / path
            if not full_path.exists():
                continue

            try:
                # Write resolved content
                full_path.write_text(content, encoding="utf-8")

                # Stage the file
                rc, _, _ = _run_cmd(["git", "add", path], cwd=self.project_root)
                if rc == 0:
                    resolved.append(path)
                    if path in unresolved:
                        unresolved.remove(path)
                    print(f"[merge_resolver] Resolved: {path}")
            except Exception as e:
                print(f"[merge_resolver] Failed to apply resolution for {path}: {e}")

        # Handle explicitly unresolvable files
        for unresolv in output.get("unresolvable", []):
            path = unresolv.get("path", "")
            reason = unresolv.get("reason", "unknown")
            print(f"[merge_resolver] Agent marked as unresolvable: {path} - {reason}")

        # Check if all conflicts are resolved
        remaining = detect_conflict_files(self.project_root)

        return MergeResolutionResult(
            success=len(remaining) == 0,
            resolved_files=resolved,
            unresolved_files=remaining,
            error=None if len(remaining) == 0 else f"{len(remaining)} conflict(s) remain",
            attempt=attempt,
        )


def attempt_agent_merge_resolution(
    project_root: Path,
    runs_dir: Path,
    task_id: str = "",
    task_context: str = "",
    milestone_id: str = "M0",
    max_attempts: int = 3,
) -> MergeResolutionResult:
    """Convenience function to attempt merge conflict resolution.

    This is the main entry point for the merge resolver.

    Args:
        project_root: Path to the git repository
        runs_dir: Directory for resolution artifacts
        task_id: ID of the task that caused the conflicts
        task_context: Additional context about the merge
        milestone_id: Current milestone ID
        max_attempts: Maximum resolution attempts before falling back to manual

    Returns:
        MergeResolutionResult with resolution status
    """
    resolver = MergeResolver(
        project_root=project_root,
        runs_dir=runs_dir,
        max_attempts=max_attempts,
    )

    return resolver.resolve_conflicts(
        task_id=task_id,
        task_context=task_context,
        milestone_id=milestone_id,
    )

================================================================================
 FILE: patch_integration.py
================================================================================

#!/usr/bin/env python3
"""Patch-based integration for orchestrator workers.

This module implements patch-artifact integration where workers never need to
commit changes directly. Instead, the orchestrator collects all changes as
patches and applies them centrally.

Key benefits:
- Workers never write to .git/worktrees/*/index.lock
- No sandbox permission issues
- Centralized conflict resolution
- Atomic integration with deterministic commits

Additionally, this module provides ScopeGuard for anti-drift enforcement:
- Validates patches against allowlist/denylist patterns
- Rejects patches that touch out-of-scope files
- Writes rejected_patch artifacts for audit
"""

from __future__ import annotations

import fnmatch
import hashlib
import json
import os
import subprocess
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

# =============================================================================
# Default scope configuration for orchestrator repair tasks
# =============================================================================

DEFAULT_DENYLIST: tuple[str, ...] = (
    "src/**",
    "tools/**",
    "docs/**",
    "DESIGN_DOCUMENT.md",
    "*.md",  # Most markdown files
    "pyproject.toml",
    "setup.py",
    "setup.cfg",
    ".github/**",
    ".gitignore",
)

DEFAULT_ALLOWLIST: tuple[str, ...] = (
    "bridge/**",
    "tests/test_orchestrator*.py",
    "tests/test_verify_repair*.py",
)

# Files that are always allowed regardless of denylist
ALWAYS_ALLOWED: tuple[str, ...] = ("bridge/**",)

# =============================================================================
# User-owned files - NEVER modify these
# These files are manually controlled by humans and must not be touched by agents
# =============================================================================

USER_OWNED_FILES: tuple[str, ...] = ("DESIGN_DOCUMENT.md",)

# =============================================================================
# Backfill task scope configuration
# Backfill tasks (FILLER-*) are limited to safe directories to prevent conflicts
# =============================================================================

BACKFILL_ALLOWLIST: tuple[str, ...] = (
    "tests/**",
    "docs/**",
    # NOTE: bridge/** removed from default backfill allowlist for safety
    # Use --backfill-allow-bridge to opt-in
    ".github/**",
)

BACKFILL_DENYLIST: tuple[str, ...] = (
    "src/**",
    "coupongen/**",
    "tools/**",
    "*.toml",
    "*.cfg",
    # Hot integration files that shouldn't be touched by backfill
    "**/api.py",
    "**/board_writer.py",
    "**/cli_main.py",
    "**/pipeline.py",
)

# =============================================================================
# Orchestrator core files - excluded from backfill unless explicitly allowed
# These files are critical for orchestrator stability
# =============================================================================

ORCHESTRATOR_CORE_FILES: tuple[str, ...] = (
    "bridge/loop.py",
    "bridge/design_doc.py",
    "bridge/patch_integration.py",
    "bridge/merge_resolver.py",
    "bridge/scheduler.py",
    "bridge/loop_pkg/**",
    "DESIGN_DOCUMENT.md",
)

# Extended backfill allowlist (when --backfill-allow-bridge is used)
BACKFILL_ALLOWLIST_WITH_BRIDGE: tuple[str, ...] = (
    "tests/**",
    "docs/**",
    "bridge/**",
    ".github/**",
)

# Extended backfill denylist (blocks orchestrator core even when bridge is allowed)
BACKFILL_DENYLIST_CORE: tuple[str, ...] = BACKFILL_DENYLIST + ORCHESTRATOR_CORE_FILES


def is_backfill_task(task_id: str) -> bool:
    """Check if a task ID indicates a backfill (filler) task."""
    return task_id.startswith("FILLER-")


def normalize_diff_path(path: str) -> str:
    """Normalize a path from git diff output to a clean relative path.

    Git diff paths often include "a/" or "b/" prefixes (e.g., "a/bridge/loop.py").
    This function safely removes those prefixes without accidentally stripping
    characters from the actual path.

    IMPORTANT: Uses removeprefix(), NOT lstrip(), to avoid bugs like:
    - lstrip("a/") on "a/bridge/foo.py" would produce "ridge/foo.py" (WRONG!)
    - removeprefix("a/") on "a/bridge/foo.py" produces "bridge/foo.py" (CORRECT!)

    Args:
        path: A file path, potentially with git diff prefixes

    Returns:
        Clean relative path without leading prefixes
    """
    # Normalize path separators first
    path = path.replace("\\", "/")

    # Remove git diff prefixes using safe removeprefix (Python 3.9+)
    # Order matters: check "a/" and "b/" prefixes that git adds to diff output
    if path.startswith("a/"):
        path = path[2:]  # Equivalent to removeprefix("a/")
    elif path.startswith("b/"):
        path = path[2:]  # Equivalent to removeprefix("b/")

    # Also handle leading "./" (current directory prefix)
    if path.startswith("./"):
        path = path[2:]

    # Handle edge cases with multiple leading slashes
    path = path.lstrip("/")

    return path


def is_user_owned_file(path: str) -> bool:
    """Check if a path is a user-owned file that agents must never modify.

    Args:
        path: File path to check

    Returns:
        True if this is a user-owned file
    """
    normalized = normalize_diff_path(path)
    return normalized in USER_OWNED_FILES or any(fnmatch.fnmatch(normalized, pattern) for pattern in USER_OWNED_FILES)


def create_backfill_scope_guard(
    runs_dir: Path | None = None,
    allow_bridge: bool = False,
) -> ScopeGuard:
    """Create a scope guard for backfill tasks with restricted allowlist.

    Args:
        runs_dir: Directory for writing rejected patch artifacts
        allow_bridge: If True, allows bridge/** files (excluding core files).
                     Default is False for maximum safety.

    Returns:
        ScopeGuard configured for backfill task constraints
    """
    if allow_bridge:
        # Allow bridge/** but still block orchestrator core files
        # Use ignore_always_allowed to enforce denylist over ALWAYS_ALLOWED
        return ScopeGuard(
            allowlist=BACKFILL_ALLOWLIST_WITH_BRIDGE,
            denylist=BACKFILL_DENYLIST_CORE,
            runs_dir=runs_dir,
            ignore_always_allowed=True,  # Enforce denylist even for bridge/**
        )
    else:
        # Default: most restrictive - only tests/docs
        return ScopeGuard(
            allowlist=BACKFILL_ALLOWLIST,
            denylist=BACKFILL_DENYLIST,
            runs_dir=runs_dir,
            ignore_always_allowed=True,  # Backfill guards need strict constraints
        )


@dataclass
class ScopeViolation:
    """Represents a single scope violation in a patch."""

    path: str
    reason: str  # "denylist_match" or "not_in_allowlist"
    matched_pattern: str


@dataclass
class ScopeCheckResult:
    """Result of scope validation for a patch."""

    allowed: bool
    violations: list[ScopeViolation] = field(default_factory=list)
    checked_paths: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "allowed": self.allowed,
            "violations": [
                {
                    "path": v.path,
                    "reason": v.reason,
                    "matched_pattern": v.matched_pattern,
                }
                for v in self.violations
            ],
            "checked_paths": self.checked_paths,
        }


class ScopeGuard:
    """Validates patches against allowlist/denylist for anti-drift enforcement.

    The guard uses a two-phase check:
    1. If path matches any denylist pattern AND is not in ALWAYS_ALLOWED: rejected
    2. If allowlist is provided: path must match at least one allowlist pattern

    Usage:
        guard = ScopeGuard(
            allowlist=["bridge/**", "tests/test_orchestrator*.py"],
            denylist=["src/**", "tools/**"],
        )
        result = guard.check_paths(["bridge/loop.py", "src/foo.py"])
        if not result.allowed:
            # Reject the patch
    """

    def __init__(
        self,
        allowlist: tuple[str, ...] | list[str] | None = None,
        denylist: tuple[str, ...] | list[str] | None = None,
        runs_dir: Path | None = None,
        ignore_always_allowed: bool = False,
    ) -> None:
        """Initialize scope guard.

        Args:
            allowlist: Glob patterns for allowed paths. If None, uses DEFAULT_ALLOWLIST.
            denylist: Glob patterns for denied paths. If None, uses DEFAULT_DENYLIST.
            runs_dir: Directory for writing rejected patch artifacts.
            ignore_always_allowed: If True, don't bypass checks for ALWAYS_ALLOWED paths.
                                   Used for backfill guards that need stricter constraints.
        """
        self.allowlist: tuple[str, ...] = tuple(allowlist) if allowlist else DEFAULT_ALLOWLIST
        self.denylist: tuple[str, ...] = tuple(denylist) if denylist else DEFAULT_DENYLIST
        self.runs_dir = runs_dir
        self.ignore_always_allowed = ignore_always_allowed

    def _matches_any(self, path: str, patterns: tuple[str, ...]) -> str | None:
        """Check if path matches any of the patterns.

        Returns the matched pattern or None.
        """
        # Normalize path separators
        path = path.replace("\\", "/")

        for pattern in patterns:
            # Check direct match
            if fnmatch.fnmatch(path, pattern):
                return pattern
            # Check if path starts with pattern prefix (for ** patterns)
            if pattern.endswith("/**"):
                prefix = pattern[:-3]
                if path.startswith(prefix + "/") or path == prefix:
                    return pattern
            # Check basename match (for *.ext patterns)
            if pattern.startswith("*."):
                if fnmatch.fnmatch(os.path.basename(path), pattern):
                    return pattern

        return None

    def check_paths(self, paths: list[str]) -> ScopeCheckResult:
        """Check if all paths are within allowed scope.

        Args:
            paths: List of file paths to check (relative to project root)

        Returns:
            ScopeCheckResult with allowed=True if all paths are valid
        """
        violations: list[ScopeViolation] = []

        for path in paths:
            # Normalize path using safe prefix removal (NOT lstrip!)
            # This prevents bugs like "bridge/foo.py" becoming "ridge/foo.py"
            path = normalize_diff_path(path)

            # CRITICAL: Check if this is a user-owned file FIRST
            # User-owned files (like DESIGN_DOCUMENT.md) must NEVER be modified
            if is_user_owned_file(path):
                violations.append(
                    ScopeViolation(
                        path=path,
                        reason="user_owned_file",
                        matched_pattern="USER_OWNED_FILES",
                    )
                )
                continue

            # Check denylist BEFORE ALWAYS_ALLOWED when ignore_always_allowed is set
            # This is used for backfill guards that need stricter constraints
            denied_pattern = self._matches_any(path, self.denylist)
            if denied_pattern:
                violations.append(
                    ScopeViolation(
                        path=path,
                        reason="denylist_match",
                        matched_pattern=denied_pattern,
                    )
                )
                continue

            # Check if in always-allowed list (unless ignore_always_allowed is set)
            if not self.ignore_always_allowed and self._matches_any(path, ALWAYS_ALLOWED):
                continue

            # Check allowlist (if provided)
            if self.allowlist:
                allowed_pattern = self._matches_any(path, self.allowlist)
                if not allowed_pattern:
                    violations.append(
                        ScopeViolation(
                            path=path,
                            reason="not_in_allowlist",
                            matched_pattern="",
                        )
                    )

        return ScopeCheckResult(
            allowed=len(violations) == 0,
            violations=violations,
            checked_paths=paths,
        )

    def check_patch_artifact(self, artifact: PatchArtifact) -> ScopeCheckResult:
        """Check if a patch artifact is within allowed scope.

        Args:
            artifact: The patch artifact to validate

        Returns:
            ScopeCheckResult with validation details
        """
        paths = [change.path for change in artifact.changes]
        paths.extend(artifact.untracked_patches.keys())
        return self.check_paths(paths)

    def write_rejected_artifact(
        self,
        task_id: str,
        agent_name: str,
        result: ScopeCheckResult,
        runs_dir: Path | None = None,
    ) -> Path | None:
        """Write a rejected patch artifact for audit.

        Args:
            task_id: The task ID that generated the patch
            agent_name: Name of the agent that generated the patch
            result: The scope check result with violations
            runs_dir: Directory to write to (uses self.runs_dir if not provided)

        Returns:
            Path to the written artifact, or None if no runs_dir
        """
        output_dir = runs_dir or self.runs_dir
        if not output_dir:
            return None

        output_dir.mkdir(parents=True, exist_ok=True)
        artifact_path = output_dir / f"rejected_patch_{task_id}.json"

        artifact_data = {
            "task_id": task_id,
            "agent_name": agent_name,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "scope_check": result.to_dict(),
            "rejection_reason": "Patch touches files outside allowed scope",
            "remediation_prompt": (
                "You attempted to modify out-of-scope files. "
                f"Only files matching these patterns are allowed: {list(self.allowlist)}. "
                f"These patterns are denied: {list(self.denylist)}. "
                "Please revise your changes to only modify allowed files."
            ),
        }

        artifact_path.write_text(json.dumps(artifact_data, indent=2), encoding="utf-8")
        return artifact_path

    def get_remediation_prompt(self, result: ScopeCheckResult) -> str:
        """Generate a prompt to send back to the agent for re-attempt.

        Args:
            result: The scope check result with violations

        Returns:
            A remediation prompt string
        """
        violation_paths = [v.path for v in result.violations]
        return (
            "ERROR: Your changes were rejected because they modify out-of-scope files.\n"
            f"Rejected paths: {violation_paths}\n\n"
            f"You may ONLY modify files matching: {list(self.allowlist)}\n"
            f"You may NOT modify files matching: {list(self.denylist)}\n\n"
            "Please revise your changes to stay within the allowed scope. "
            "Focus only on bridge/** files and orchestrator tests."
        )


@dataclass
class FileChange:
    """Represents a single file change."""

    path: str
    operation: str  # "modify", "add", "delete", "binary"
    sha256: str | None = None
    old_mode: str | None = None
    new_mode: str | None = None


@dataclass
class PatchArtifact:
    """Complete patch artifact from a worker."""

    task_id: str
    base_sha: str
    changes: list[FileChange] = field(default_factory=list)
    patch_content: str = ""
    untracked_patches: dict[str, str] = field(default_factory=dict)  # path -> patch content
    success: bool = True
    error: str | None = None


def _run_cmd(
    cmd: list[str],
    cwd: Path | str,
    env: dict[str, str] | None = None,
    check: bool = False,
) -> tuple[int, str, str]:
    """Run a command and return (returncode, stdout, stderr)."""
    try:
        result = subprocess.run(
            cmd,
            cwd=str(cwd),
            env=env or os.environ.copy(),
            capture_output=True,
            text=True,
            timeout=60,
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", "Command timed out"
    except Exception as e:
        return -1, "", str(e)


def _compute_sha256(file_path: Path) -> str | None:
    """Compute SHA256 hash of a file."""
    try:
        if not file_path.exists() or file_path.is_dir():
            return None
        with open(file_path, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    except Exception:
        return None


def collect_patch_artifact(
    worktree_path: Path,
    task_id: str,
    base_sha: str,
) -> PatchArtifact:
    """Collect all changes from a worker's worktree as a patch artifact.

    This function:
    1. Gets tracked modifications/deletions via `git diff --binary`
    2. Detects untracked new files and generates patches for them
    3. Creates a manifest of all changed files with their SHA256 hashes

    Args:
        worktree_path: Path to the worker's worktree
        task_id: The task ID for this work
        base_sha: The base commit SHA the worktree was created from

    Returns:
        PatchArtifact containing all changes
    """
    artifact = PatchArtifact(task_id=task_id, base_sha=base_sha)

    # Step 1: Get tracked changes (modified, deleted files)
    rc, diff_out, diff_err = _run_cmd(
        ["git", "diff", "--binary", "HEAD"],
        cwd=worktree_path,
    )
    if rc != 0:
        # Try diff against base_sha if HEAD doesn't work
        rc, diff_out, diff_err = _run_cmd(
            ["git", "diff", "--binary", base_sha],
            cwd=worktree_path,
        )

    if rc == 0 and diff_out.strip():
        artifact.patch_content = diff_out

    # Step 2: Get list of modified/deleted files for manifest
    rc, status_out, _ = _run_cmd(
        ["git", "status", "--porcelain=v1"],
        cwd=worktree_path,
    )

    if rc != 0:
        artifact.success = False
        artifact.error = f"Failed to get git status: {status_out}"
        return artifact

    # Parse status output
    # Git porcelain v1 format: "XY PATH" or "XY\tPATH" (tab for rename detection)
    # X = index status, Y = worktree status, then space/tab, then path
    for line in status_out.strip().split("\n"):
        if not line or len(line) < 3:
            continue

        status_code = line[:2]

        # Handle both space and tab delimiters (git uses tab for rename detection)
        # The delimiter is at index 2; path starts at index 3
        rest = line[2:]
        if rest.startswith(" ") or rest.startswith("\t"):
            file_path = rest[1:].strip()
        else:
            # Fallback: try to find first space/tab after status code
            for i, c in enumerate(rest):
                if c in " \t":
                    file_path = rest[i + 1 :].strip()
                    break
            else:
                # No delimiter found - use entire rest as path (shouldn't happen)
                file_path = rest.strip()

        # Handle renamed files (R status shows "old -> new" with space or tab separator)
        if " -> " in file_path:
            _, file_path = file_path.split(" -> ", 1)
        elif "\t" in file_path and status_code.startswith("R"):
            # Git can use tab to separate old and new names in rename detection
            parts = file_path.split("\t")
            if len(parts) == 2:
                file_path = parts[1]

        # VALIDATION: Detect path truncation - a valid path should not start with
        # common directory suffixes that indicate truncation
        truncation_indicators = ["ests/", "ocs/", "rc/", "idge/", "ools/"]
        for indicator in truncation_indicators:
            if file_path.startswith(indicator):
                # Log warning but don't fail - attempt to recover
                expected_prefix = {"ests/": "t", "ocs/": "d", "rc/": "s", "idge/": "br", "ools/": "t"}
                prefix = expected_prefix.get(indicator, "")
                print(f"[patch] WARNING: Possible path truncation detected: '{file_path}' - attempting recovery")
                file_path = prefix + file_path
                break

        # Strip any quotes from filenames
        if file_path.startswith('"') and file_path.endswith('"'):
            file_path = file_path[1:-1]

        full_path = worktree_path / file_path

        # Determine operation type
        if status_code.startswith("D") or status_code.endswith("D"):
            operation = "delete"
            sha256 = None
        elif status_code == "??" or status_code.startswith("A"):
            operation = "add"
            sha256 = _compute_sha256(full_path)

            # For untracked/new files, generate a diff patch
            if status_code == "??":
                # Use git diff --no-index to create a patch for untracked files
                rc2, patch_out, _ = _run_cmd(
                    ["git", "diff", "--binary", "--no-index", "/dev/null", file_path],
                    cwd=worktree_path,
                )
                # git diff --no-index returns 1 when files differ (which is expected)
                if patch_out.strip():
                    artifact.untracked_patches[file_path] = patch_out
        else:
            operation = "modify"
            sha256 = _compute_sha256(full_path)

        # Detect binary files
        if full_path.exists() and not full_path.is_dir():
            try:
                with open(full_path, "rb") as f:
                    chunk = f.read(8192)
                    if b"\x00" in chunk:
                        operation = "binary"
            except Exception:
                pass

        artifact.changes.append(
            FileChange(
                path=file_path,
                operation=operation,
                sha256=sha256,
            )
        )

    return artifact


def save_patch_artifact(
    artifact: PatchArtifact,
    task_dir: Path,
) -> tuple[Path, Path]:
    """Save patch artifact to disk atomically.

    Args:
        artifact: The patch artifact to save
        task_dir: Directory to save artifacts in

    Returns:
        Tuple of (patch_file_path, manifest_file_path)
    """
    task_dir.mkdir(parents=True, exist_ok=True)

    # Save main patch file atomically
    patch_path = task_dir / "changes.patch"
    _atomic_write(patch_path, artifact.patch_content)

    # Save untracked file patches if any
    if artifact.untracked_patches:
        untracked_dir = task_dir / "untracked_patches"
        untracked_dir.mkdir(exist_ok=True)
        for file_path, patch_content in artifact.untracked_patches.items():
            safe_name = file_path.replace("/", "__").replace("\\", "__")
            patch_file = untracked_dir / f"{safe_name}.patch"
            _atomic_write(patch_file, patch_content)

    # Save manifest atomically
    manifest_path = task_dir / "changed_files.json"
    manifest = {
        "task_id": artifact.task_id,
        "base_sha": artifact.base_sha,
        "success": artifact.success,
        "error": artifact.error,
        "files": [
            {
                "path": c.path,
                "operation": c.operation,
                "sha256": c.sha256,
            }
            for c in artifact.changes
        ],
        "untracked_files": list(artifact.untracked_patches.keys()),
    }
    _atomic_write_json(manifest_path, manifest)

    return patch_path, manifest_path


def _atomic_write(path: Path, content: str) -> None:
    """Write content to a file atomically."""
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    try:
        with open(tmp_path, "w", encoding="utf-8") as f:
            f.write(content)
            f.flush()
            os.fsync(f.fileno())
        tmp_path.rename(path)
    except Exception:
        if tmp_path.exists():
            tmp_path.unlink()
        raise


def _atomic_write_json(path: Path, data: dict[str, Any]) -> None:
    """Write JSON data to a file atomically."""
    content = json.dumps(data, indent=2)
    _atomic_write(path, content)


def _run_auto_ruff(
    project_root: Path,
    python_files: list[str],
) -> tuple[bool, str]:
    """Run ruff format and ruff check --fix on staged Python files.

    This automatically fixes lint/format issues during patch integration,
    reducing the need for separate lint agent runs.

    Args:
        project_root: Root of the git repository
        python_files: List of Python file paths (relative to project_root)

    Returns:
        Tuple of (success, message)
    """
    if not python_files:
        return True, "No Python files to format"

    # Check if auto-ruff is enabled (default: ON)
    auto_ruff_enabled = os.environ.get("ORCH_AUTO_RUFF", "1").lower() not in ("0", "false", "no", "off")
    if not auto_ruff_enabled:
        return True, "Auto-ruff disabled via ORCH_AUTO_RUFF=0"

    # Filter to only existing files
    existing_files = [f for f in python_files if (project_root / f).exists()]
    if not existing_files:
        return True, "No existing Python files to format"

    # Try running ruff format
    try:
        rc, out, err = _run_cmd(
            ["ruff", "format"] + existing_files,
            cwd=project_root,
        )
        if rc != 0:
            # Non-fatal: log warning and continue
            print(f"[patch] Warning: ruff format returned {rc}: {err}")
    except Exception as e:
        print(f"[patch] Warning: ruff format failed ({e}), continuing without format")

    # Try running ruff check --fix
    try:
        rc, out, err = _run_cmd(
            ["ruff", "check", "--fix"] + existing_files,
            cwd=project_root,
        )
        if rc != 0 and "error" in err.lower():
            # Non-fatal: log warning and continue
            print(f"[patch] Warning: ruff check --fix returned {rc}: {err}")
    except Exception as e:
        print(f"[patch] Warning: ruff check --fix failed ({e}), continuing without lint fixes")

    # Re-stage the files that were potentially modified
    for file_path in existing_files:
        _run_cmd(["git", "add", file_path], cwd=project_root)

    return True, f"Auto-ruff ran on {len(existing_files)} Python files"


def apply_patch_artifact(
    project_root: Path,
    task_dir: Path,
    task_id: str,
    task_branch: str,
    scope_guard: ScopeGuard | None = None,
    runs_dir: Path | None = None,
    agent_name: str = "unknown",
) -> tuple[bool, str, str | None]:
    """Apply a patch artifact to the project and create a commit.

    This function:
    1. Validates patch against scope guard (if provided)
    2. Applies the main patch using `git apply --binary`
    3. Applies untracked file patches
    4. Stages all changes
    5. Creates a deterministic commit

    Args:
        project_root: Root of the git repository
        task_dir: Directory containing the patch artifacts
        task_id: The task ID for commit message
        task_branch: The branch name for this task
        scope_guard: Optional ScopeGuard for drift prevention
        runs_dir: Directory for writing rejected patch artifacts
        agent_name: Name of the agent that created the patch

    Returns:
        Tuple of (success, message, commit_sha)
    """
    patch_path = task_dir / "changes.patch"
    manifest_path = task_dir / "changed_files.json"

    # Read manifest
    if not manifest_path.exists():
        return False, "Manifest file not found", None

    try:
        with open(manifest_path) as f:
            manifest = json.load(f)
    except Exception as e:
        return False, f"Failed to read manifest: {e}", None

    # Scope guard validation - CRITICAL for drift prevention
    if scope_guard is not None:
        files_to_check = [f["path"] for f in manifest.get("files", [])]
        files_to_check.extend(manifest.get("untracked_files", []))

        scope_result = scope_guard.check_paths(files_to_check)
        if not scope_result.allowed:
            # Write rejected patch artifact
            artifact_dir = runs_dir or task_dir
            scope_guard.write_rejected_artifact(
                task_id=task_id,
                agent_name=agent_name,
                result=scope_result,
                runs_dir=artifact_dir,
            )
            # Return rejection with remediation info
            violation_paths = [v.path for v in scope_result.violations]
            return (
                False,
                f"SCOPE_REJECTED: Patch touches out-of-scope files: {violation_paths}",
                None,
            )

    # Apply main patch if it exists and has content
    if patch_path.exists():
        patch_content = patch_path.read_text(encoding="utf-8")
        if patch_content.strip():
            rc, out, err = _run_cmd(
                ["git", "apply", "--binary", "--3way", str(patch_path)],
                cwd=project_root,
            )
            if rc != 0:
                # Try without 3way
                rc, out, err = _run_cmd(
                    ["git", "apply", "--binary", str(patch_path)],
                    cwd=project_root,
                )
                if rc != 0:
                    return False, f"Failed to apply patch: {err}", None

    # Apply untracked file patches
    untracked_dir = task_dir / "untracked_patches"
    if untracked_dir.exists():
        for patch_file in untracked_dir.glob("*.patch"):
            rc, out, err = _run_cmd(
                ["git", "apply", "--binary", str(patch_file)],
                cwd=project_root,
            )
            if rc != 0:
                # Non-fatal: log and continue
                print(f"[patch] Warning: Failed to apply untracked patch {patch_file.name}: {err}")

    # Stage all changes
    files_to_stage = [f["path"] for f in manifest.get("files", [])]
    files_to_stage.extend(manifest.get("untracked_files", []))

    if files_to_stage:
        # Stage specific files to avoid including unrelated changes
        for file_path in files_to_stage:
            full_path = project_root / file_path
            if full_path.exists():
                _run_cmd(["git", "add", file_path], cwd=project_root)
            elif any(f["path"] == file_path and f["operation"] == "delete" for f in manifest.get("files", [])):
                # Handle deleted files
                _run_cmd(["git", "add", file_path], cwd=project_root)

    # Auto-run ruff format and lint fix on staged Python files
    # This reduces the need for separate lint agent runs
    python_files = [f for f in files_to_stage if f.endswith(".py")]
    if python_files:
        ruff_ok, ruff_msg = _run_auto_ruff(project_root, python_files)
        if ruff_ok:
            print(f"[patch] {ruff_msg}")

    # Check if there are staged changes
    rc, status, _ = _run_cmd(
        ["git", "diff", "--cached", "--quiet"],
        cwd=project_root,
    )

    if rc == 0:
        # No staged changes
        return True, "No changes to commit", None

    # Create commit with deterministic message
    commit_msg = f"task({task_id}): integrate worker patch"
    rc, out, err = _run_cmd(
        ["git", "commit", "-m", commit_msg],
        cwd=project_root,
    )

    if rc != 0:
        return False, f"Failed to commit: {err}", None

    # Get commit SHA
    rc, sha_out, _ = _run_cmd(
        ["git", "rev-parse", "HEAD"],
        cwd=project_root,
    )

    commit_sha = sha_out.strip() if rc == 0 else None

    return True, "Patch applied and committed successfully", commit_sha


def attempt_conflict_resolution(
    project_root: Path,
    task_id: str,
    patch_path: Path,
) -> tuple[bool, str]:
    """Attempt to resolve conflicts when patch application fails.

    This function:
    1. Rebases the current branch
    2. Retries patch application with 3-way merge
    3. If still failing, marks as needing manual resolution

    Args:
        project_root: Root of the git repository
        task_id: The task ID for logging
        patch_path: Path to the patch file

    Returns:
        Tuple of (success, message)
    """
    # First, try to update to latest main
    rc, out, err = _run_cmd(
        ["git", "fetch", "origin"],
        cwd=project_root,
    )

    # Try rebasing
    rc, out, err = _run_cmd(
        ["git", "rebase", "origin/main"],
        cwd=project_root,
    )

    if rc != 0:
        # Abort rebase and return
        _run_cmd(["git", "rebase", "--abort"], cwd=project_root)
        return False, f"Rebase failed: {err}"

    # Retry patch application with 3-way merge
    rc, out, err = _run_cmd(
        ["git", "apply", "--binary", "--3way", str(patch_path)],
        cwd=project_root,
    )

    if rc == 0:
        return True, "Conflict resolved after rebase"

    return False, f"Conflict resolution failed: {err}"


class PatchIntegrator:
    """High-level patch integration manager.

    This class manages the integration of patches from multiple workers,
    handling conflicts and maintaining integration order.

    With scope guard enabled, patches that touch out-of-scope files are
    rejected with a SCOPE_REJECTED status, and artifacts are written for audit.

    When merge conflicts occur, the integrator will:
    1. Try basic rebase-based conflict resolution
    2. Try agent-based intelligent conflict resolution (up to N attempts)
    3. Only fall back to manual resolution if all automated approaches fail
    """

    def __init__(
        self,
        project_root: Path,
        runs_dir: Path,
        scope_guard: ScopeGuard | None = None,
        max_merge_resolution_attempts: int = 3,
    ):
        self.project_root = project_root
        self.runs_dir = runs_dir
        self.scope_guard = scope_guard
        self.max_merge_resolution_attempts = max_merge_resolution_attempts
        self._integration_lock = None  # Set externally if threading is used

    def set_lock(self, lock) -> None:
        """Set the threading lock for integration operations."""
        self._integration_lock = lock

    def set_scope_guard(self, scope_guard: ScopeGuard) -> None:
        """Set or update the scope guard."""
        self.scope_guard = scope_guard

    def integrate_task(
        self,
        task_id: str,
        task_dir: Path,
        task_branch: str,
        agent_name: str = "unknown",
        task_context: str = "",
        milestone_id: str = "M0",
    ) -> tuple[bool, str, str | None]:
        """Integrate a task's changes into the main branch.

        Args:
            task_id: The task ID
            task_dir: Directory containing the patch artifacts
            task_branch: The branch name (for fallback if needed)
            agent_name: Name of the agent that created the patch
            task_context: Context about the task (for merge resolver)
            milestone_id: Current milestone ID

        Returns:
            Tuple of (success, message, commit_sha)
            If scope_guard rejects: (False, "SCOPE_REJECTED: ...", None)
        """
        if self._integration_lock:
            with self._integration_lock:
                return self._do_integrate(task_id, task_dir, task_branch, agent_name, task_context, milestone_id)
        else:
            return self._do_integrate(task_id, task_dir, task_branch, agent_name, task_context, milestone_id)

    def _do_integrate(
        self,
        task_id: str,
        task_dir: Path,
        task_branch: str,
        agent_name: str = "unknown",
        task_context: str = "",
        milestone_id: str = "M0",
    ) -> tuple[bool, str, str | None]:
        """Internal integration implementation."""
        # Determine which scope guard to use
        # Backfill tasks (FILLER-*) get a restricted scope to prevent conflicts
        effective_scope_guard = self.scope_guard
        if is_backfill_task(task_id):
            backfill_guard = create_backfill_scope_guard(runs_dir=self.runs_dir)
            # Use backfill guard if no custom scope guard is set, or combine with it
            if effective_scope_guard is None:
                effective_scope_guard = backfill_guard
            else:
                # For backfill tasks, the backfill restrictions take precedence
                effective_scope_guard = backfill_guard

        # First, try patch-based integration (with scope guard if enabled)
        success, message, commit_sha = apply_patch_artifact(
            self.project_root,
            task_dir,
            task_id,
            task_branch,
            scope_guard=effective_scope_guard,
            runs_dir=self.runs_dir,
            agent_name=agent_name,
        )

        if success:
            return True, message, commit_sha

        # If scope rejected, do not attempt conflict resolution - it's a policy failure
        if "SCOPE_REJECTED" in message:
            return False, message, None

        # If patch failed, try conflict resolution
        patch_path = task_dir / "changes.patch"
        if patch_path.exists():
            # Step 1: Try basic rebase-based conflict resolution
            resolved, resolve_msg = attempt_conflict_resolution(
                self.project_root,
                task_id,
                patch_path,
            )
            if resolved:
                # Retry integration (scope guard already passed on first attempt)
                return apply_patch_artifact(
                    self.project_root,
                    task_dir,
                    task_id,
                    task_branch,
                    scope_guard=self.scope_guard,
                    runs_dir=self.runs_dir,
                    agent_name=agent_name,
                )

            # Step 2: Try agent-based intelligent conflict resolution
            print(f"[patch_integration] {task_id}: Basic resolution failed, trying agent-based merge resolver")
            try:
                from bridge.merge_resolver import attempt_agent_merge_resolution

                merge_result = attempt_agent_merge_resolution(
                    project_root=self.project_root,
                    runs_dir=self.runs_dir,
                    task_id=task_id,
                    task_context=task_context,
                    milestone_id=milestone_id,
                    max_attempts=self.max_merge_resolution_attempts,
                )

                if merge_result.success:
                    print(
                        f"[patch_integration] {task_id}: Agent merge resolver succeeded after {merge_result.attempt} attempt(s)"
                    )
                    # Retry integration after agent resolution
                    return apply_patch_artifact(
                        self.project_root,
                        task_dir,
                        task_id,
                        task_branch,
                        scope_guard=self.scope_guard,
                        runs_dir=self.runs_dir,
                        agent_name=agent_name,
                    )
                else:
                    print(f"[patch_integration] {task_id}: Agent merge resolver failed: {merge_result.error}")
                    print(f"[patch_integration] {task_id}: Unresolved files: {merge_result.unresolved_files}")

            except Exception as e:
                print(f"[patch_integration] {task_id}: Agent merge resolver raised exception: {e}")

        # All automated resolution attempts failed - mark as needing manual resolution
        return False, f"needs_manual_resolution: {message}", None

================================================================================
 FILE: provider_adapters.py
================================================================================

#!/usr/bin/env python3
"""Provider Output Adapters for unified structured output handling.

This module provides adapters for different AI providers (OpenAI, Claude) that
ensure consistent Turn objects are produced regardless of provider-specific
output formats and quirks.

Key features:
- OpenAI: Uses strict JSON schema outputs when supported
- Claude: Uses tool/function calling for structured output
- Unified internal Turn object via ProviderOutputAdapter + TurnNormalizer
- Bounded repair fallback for occasional malformed outputs
"""

from __future__ import annotations

import json
import re
from abc import ABC, abstractmethod
from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any

# Import TurnNormalizer for consistent normalization
# This will be imported from loop.py at runtime to avoid circular imports


@dataclass
class AdapterResult:
    """Result from provider output adapter."""

    success: bool
    turn: dict[str, Any] | None
    raw_output: str
    warnings: list[str] = field(default_factory=list)
    error: str | None = None
    needs_retry: bool = False
    needs_repair: bool = False


@dataclass
class RepairResult:
    """Result from repair attempt."""

    success: bool
    repaired_output: str
    error: str | None = None


class ProviderOutputAdapter(ABC):
    """Base class for provider-specific output adapters."""

    @abstractmethod
    def extract_turn(self, raw_output: str, expected_agent: str, expected_milestone_id: str) -> AdapterResult:
        """Extract Turn object from provider-specific output format.

        Args:
            raw_output: Raw output from the provider
            expected_agent: Expected agent name for normalization
            expected_milestone_id: Expected milestone ID for normalization

        Returns:
            AdapterResult with extracted turn or error information
        """
        pass

    @abstractmethod
    def get_schema_config(self, schema: dict) -> dict:
        """Get provider-specific schema configuration.

        Args:
            schema: JSON schema for the Turn object

        Returns:
            Provider-specific configuration for structured output
        """
        pass


class OpenAIOutputAdapter(ProviderOutputAdapter):
    """Adapter for OpenAI (Codex) structured output.

    OpenAI supports strict JSON schema outputs via response_format.
    This adapter handles:
    - Strict JSON schema validation
    - Extra prose before/after JSON extraction
    - Missing key recovery
    - Type coercion for common mismatches
    """

    def extract_turn(self, raw_output: str, expected_agent: str, expected_milestone_id: str) -> AdapterResult:
        warnings = []
        raw_output = raw_output.strip()

        if not raw_output:
            return AdapterResult(
                success=False,
                turn=None,
                raw_output=raw_output,
                error="Empty output from OpenAI",
                needs_retry=True,
            )

        # OpenAI with strict schema should produce clean JSON
        # But sometimes there's extra prose - try to extract JSON
        turn_obj = self._try_parse_json(raw_output)

        if turn_obj is None:
            # Try extracting from code fences
            turn_obj = self._extract_from_fences(raw_output)
            if turn_obj is not None:
                warnings.append("Extracted JSON from code fences")

        if turn_obj is None:
            # Try extracting first balanced JSON object
            turn_obj = self._extract_balanced_json(raw_output)
            if turn_obj is not None:
                warnings.append("Extracted balanced JSON object from prose")

        if turn_obj is None:
            return AdapterResult(
                success=False,
                turn=None,
                raw_output=raw_output,
                error="Cannot extract JSON from OpenAI output",
                needs_repair=True,
            )

        # Apply invariant overrides
        turn_obj = self._apply_invariants(turn_obj, expected_agent, expected_milestone_id, warnings)

        # Validate required fields
        missing = self._check_required_fields(turn_obj)
        if missing:
            return AdapterResult(
                success=False,
                turn=turn_obj,
                raw_output=raw_output,
                warnings=warnings,
                error=f"Missing required fields: {missing}",
                needs_repair=True,
            )

        return AdapterResult(
            success=True,
            turn=turn_obj,
            raw_output=raw_output,
            warnings=warnings,
        )

    def get_schema_config(self, schema: dict) -> dict:
        """Get OpenAI-specific schema configuration.

        Returns configuration for response_format json_schema strict mode.
        """
        return {
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": "turn",
                    "strict": True,
                    "schema": schema,
                },
            }
        }

    def _try_parse_json(self, text: str) -> dict | None:
        try:
            obj = json.loads(text)
            return obj if isinstance(obj, dict) else None
        except json.JSONDecodeError:
            return None

    def _extract_from_fences(self, text: str) -> dict | None:
        if not text.startswith("```"):
            return None
        lines = text.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        return self._try_parse_json("\n".join(lines))

    def _extract_balanced_json(self, text: str) -> dict | None:
        start = text.find("{")
        if start == -1:
            return None

        depth = 0
        in_str = False
        esc = False

        for i in range(start, len(text)):
            ch = text[i]
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
            else:
                if ch == '"':
                    in_str = True
                elif ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        candidate = text[start : i + 1]
                        return self._try_parse_json(candidate)
        return None

    def _apply_invariants(
        self,
        turn: dict,
        expected_agent: str,
        expected_milestone_id: str,
        warnings: list[str],
    ) -> dict:
        """Apply invariant field overrides."""
        # Agent must match expected
        if turn.get("agent") != expected_agent:
            warnings.append(f"agent corrected: {turn.get('agent')} -> {expected_agent}")
            turn["agent"] = expected_agent

        # Milestone must match expected
        if turn.get("milestone_id") != expected_milestone_id:
            warnings.append(f"milestone_id corrected: {turn.get('milestone_id')} -> {expected_milestone_id}")
            turn["milestone_id"] = expected_milestone_id

        return turn

    def _check_required_fields(self, turn: dict) -> list[str]:
        """Check for missing required fields."""
        required = [
            "summary",
            "work_completed",
            "project_complete",
            "phase",
            "next_agent",
            "next_prompt",
            "delegate_rationale",
            "stats_refs",
            "needs_write_access",
            "artifacts",
            "gates_passed",
            "requirement_progress",
        ]
        return [f for f in required if f not in turn]


class ClaudeOutputAdapter(ProviderOutputAdapter):
    """Adapter for Claude structured output.

    Claude uses tool/function calling for structured output.
    This adapter handles:
    - Tool result extraction from Claude CLI JSON stream
    - Assistant message content parsing
    - Wrapper metadata stripping
    - Extra text around JSON
    """

    def extract_turn(self, raw_output: str, expected_agent: str, expected_milestone_id: str) -> AdapterResult:
        warnings = []
        raw_output = raw_output.strip()

        if not raw_output:
            return AdapterResult(
                success=False,
                turn=None,
                raw_output=raw_output,
                error="Empty output from Claude",
                needs_retry=True,
            )

        # Claude CLI outputs JSON stream with multiple events
        turn_obj = self._extract_from_json_stream(raw_output)

        if turn_obj is not None:
            warnings.append("Extracted from Claude JSON stream")
        else:
            # Try direct JSON parse
            turn_obj = self._try_parse_json(raw_output)

        if turn_obj is None:
            # Try extracting from code fences
            turn_obj = self._extract_from_fences(raw_output)
            if turn_obj is not None:
                warnings.append("Extracted JSON from code fences")

        if turn_obj is None:
            # Try balanced JSON extraction
            turn_obj = self._extract_balanced_json(raw_output)
            if turn_obj is not None:
                warnings.append("Extracted balanced JSON object")

        if turn_obj is None:
            return AdapterResult(
                success=False,
                turn=None,
                raw_output=raw_output,
                error="Cannot extract JSON from Claude output",
                needs_repair=True,
            )

        # Apply invariant overrides
        turn_obj = self._apply_invariants(turn_obj, expected_agent, expected_milestone_id, warnings)

        # Validate required fields
        missing = self._check_required_fields(turn_obj)
        if missing:
            # Try to fill defaults for some fields
            turn_obj = self._fill_defaults(turn_obj, warnings)
            missing = self._check_required_fields(turn_obj)
            if missing:
                return AdapterResult(
                    success=False,
                    turn=turn_obj,
                    raw_output=raw_output,
                    warnings=warnings,
                    error=f"Missing required fields: {missing}",
                    needs_repair=True,
                )

        return AdapterResult(
            success=True,
            turn=turn_obj,
            raw_output=raw_output,
            warnings=warnings,
        )

    def get_schema_config(self, schema: dict) -> dict:
        """Get Claude-specific schema configuration.

        Returns configuration for tool/function calling.
        """
        return {
            "tools": [
                {
                    "name": "submit_turn",
                    "description": "Submit the turn result with structured output",
                    "input_schema": schema,
                }
            ],
            "tool_choice": {"type": "tool", "name": "submit_turn"},
        }

    def _extract_from_json_stream(self, text: str) -> dict | None:
        """Extract turn from Claude CLI JSON stream output."""
        objects = self._parse_json_sequence(text)
        if not objects:
            return None

        # Look for type=="result" events
        for obj in reversed(objects):
            if isinstance(obj, dict) and obj.get("type") == "result":
                result_str = obj.get("result")
                if isinstance(result_str, str):
                    turn = self._try_parse_json(self._strip_fences(result_str))
                    if turn is not None:
                        return turn

        # Look for assistant messages
        for obj in reversed(objects):
            if isinstance(obj, dict) and obj.get("type") == "assistant":
                message = obj.get("message", {})
                content = message.get("content", [])
                for block in content:
                    if isinstance(block, dict) and block.get("type") == "text":
                        text_content = block.get("text", "")
                        turn = self._try_parse_json(self._strip_fences(text_content))
                        if turn is not None:
                            return turn

        return None

    def _parse_json_sequence(self, text: str) -> list:
        """Parse multiple JSON objects from a stream."""
        objects = []
        decoder = json.JSONDecoder()
        idx = 0
        n = len(text)

        while idx < n:
            while idx < n and text[idx] in " \t\n\r":
                idx += 1
            if idx >= n:
                break
            try:
                obj, end = decoder.raw_decode(text, idx)
                if isinstance(obj, list):
                    objects.extend(obj)
                else:
                    objects.append(obj)
                idx = end
            except json.JSONDecodeError:
                idx += 1

        return objects

    def _try_parse_json(self, text: str) -> dict | None:
        try:
            obj = json.loads(text)
            return obj if isinstance(obj, dict) else None
        except json.JSONDecodeError:
            return None

    def _strip_fences(self, text: str) -> str:
        text = text.strip()
        if text.startswith("```"):
            lines = text.split("\n")
            if lines[0].startswith("```"):
                lines = lines[1:]
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            return "\n".join(lines).strip()
        return text

    def _extract_from_fences(self, text: str) -> dict | None:
        stripped = self._strip_fences(text)
        if stripped != text:
            return self._try_parse_json(stripped)
        return None

    def _extract_balanced_json(self, text: str) -> dict | None:
        start = text.find("{")
        if start == -1:
            return None

        depth = 0
        in_str = False
        esc = False

        for i in range(start, len(text)):
            ch = text[i]
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
            else:
                if ch == '"':
                    in_str = True
                elif ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        candidate = text[start : i + 1]
                        return self._try_parse_json(candidate)
        return None

    def _apply_invariants(
        self,
        turn: dict,
        expected_agent: str,
        expected_milestone_id: str,
        warnings: list[str],
    ) -> dict:
        """Apply invariant field overrides."""
        if turn.get("agent") != expected_agent:
            warnings.append(f"agent corrected: {turn.get('agent')} -> {expected_agent}")
            turn["agent"] = expected_agent

        if turn.get("milestone_id") != expected_milestone_id:
            warnings.append(f"milestone_id corrected: {turn.get('milestone_id')} -> {expected_milestone_id}")
            turn["milestone_id"] = expected_milestone_id

        return turn

    def _fill_defaults(self, turn: dict, warnings: list[str]) -> dict:
        """Fill default values for missing optional-ish fields."""
        defaults = {
            "gates_passed": [],
            "requirement_progress": {
                "covered_req_ids": [],
                "tests_added_or_modified": [],
                "commands_run": [],
            },
            "artifacts": [],
            "phase": "implement",
            "needs_write_access": True,
            "delegate_rationale": "",
            "next_prompt": "",
        }

        for key, value in defaults.items():
            if key not in turn:
                turn[key] = value
                warnings.append(f"Filled default for {key}")

        return turn

    def _check_required_fields(self, turn: dict) -> list[str]:
        """Check for missing required fields."""
        required = [
            "summary",
            "work_completed",
            "project_complete",
        ]
        return [f for f in required if f not in turn]


class OutputRepairService:
    """Service for repairing malformed outputs.

    Implements bounded repair: at most 1 repair attempt per output,
    then triggers task retry.
    """

    MAX_REPAIR_ATTEMPTS = 1

    def __init__(self, repair_prompt_builder: Callable[[str, str], str] | None = None):
        """Initialize repair service.

        Args:
            repair_prompt_builder: Optional function to build repair prompts
        """
        self.repair_prompt_builder = repair_prompt_builder or self._default_repair_prompt

    def attempt_repair(
        self,
        raw_output: str,
        error_message: str,
        repair_count: int,
    ) -> RepairResult:
        """Attempt to repair malformed output.

        Args:
            raw_output: The malformed output
            error_message: Error message describing the issue
            repair_count: Number of previous repair attempts

        Returns:
            RepairResult indicating success/failure
        """
        if repair_count >= self.MAX_REPAIR_ATTEMPTS:
            return RepairResult(
                success=False,
                repaired_output="",
                error="Max repair attempts exceeded",
            )

        # Try basic repairs
        repaired = self._try_basic_repairs(raw_output)
        if repaired:
            return RepairResult(success=True, repaired_output=repaired)

        # Can't repair without calling model
        return RepairResult(
            success=False,
            repaired_output="",
            error="Cannot repair without model call - trigger retry",
        )

    def _try_basic_repairs(self, raw_output: str) -> str | None:
        """Try basic text-based repairs without model calls."""
        # Strip common wrapper text
        patterns_to_strip = [
            (r"^Here(?:'s| is) (?:the|my|your) (?:JSON|response|output)[:\s]*", ""),
            (r"```json\s*", ""),
            (r"```\s*$", ""),
            (r"^\s*```\s*", ""),
        ]

        text = raw_output.strip()
        for pattern, replacement in patterns_to_strip:
            text = re.sub(pattern, replacement, text, flags=re.MULTILINE | re.IGNORECASE)
        text = text.strip()

        # Try to parse after stripping
        try:
            obj = json.loads(text)
            if isinstance(obj, dict):
                return json.dumps(obj)
        except json.JSONDecodeError:
            pass

        # Try extracting balanced JSON
        start = text.find("{")
        if start != -1:
            depth = 0
            in_str = False
            esc = False
            for i in range(start, len(text)):
                ch = text[i]
                if in_str:
                    if esc:
                        esc = False
                    elif ch == "\\":
                        esc = True
                    elif ch == '"':
                        in_str = False
                else:
                    if ch == '"':
                        in_str = True
                    elif ch == "{":
                        depth += 1
                    elif ch == "}":
                        depth -= 1
                        if depth == 0:
                            candidate = text[start : i + 1]
                            try:
                                obj = json.loads(candidate)
                                if isinstance(obj, dict):
                                    return json.dumps(obj)
                            except json.JSONDecodeError:
                                pass
                            break

        return None

    def _default_repair_prompt(self, raw_output: str, error_message: str) -> str:
        """Build default repair prompt."""
        return f"""The previous output was invalid JSON. Error: {error_message}

Original output (first 2000 chars):
{raw_output[:2000]}

Please output ONLY valid JSON matching the schema. No markdown, no code fences, no extra text."""


def get_adapter_for_agent(agent: str) -> ProviderOutputAdapter:
    """Get the appropriate adapter for an agent type.

    Args:
        agent: Agent name ("codex" or "claude")

    Returns:
        Appropriate ProviderOutputAdapter instance
    """
    if agent == "codex":
        return OpenAIOutputAdapter()
    elif agent == "claude":
        return ClaudeOutputAdapter()
    else:
        # Default to Claude adapter for unknown agents
        return ClaudeOutputAdapter()


def normalize_provider_output(
    raw_output: str,
    agent: str,
    expected_agent: str,
    expected_milestone_id: str,
    stats_id_set: set[str] | None = None,
) -> AdapterResult:
    """Normalize provider output using the appropriate adapter.

    This is the main entry point for provider-agnostic output normalization.

    Args:
        raw_output: Raw output from the provider
        agent: Provider/agent name ("codex" or "claude")
        expected_agent: Expected agent name for invariant override
        expected_milestone_id: Expected milestone ID for invariant override
        stats_id_set: Optional set of valid stats IDs

    Returns:
        AdapterResult with normalized turn or error information
    """
    adapter = get_adapter_for_agent(agent)
    result = adapter.extract_turn(raw_output, expected_agent, expected_milestone_id)

    # If successful and stats_id_set provided, validate/fix stats_refs
    if result.success and result.turn and stats_id_set:
        turn = result.turn
        stats_refs = turn.get("stats_refs", [])
        valid_refs = [s for s in stats_refs if s in stats_id_set]
        if not valid_refs:
            # Default stats ref
            default = "CL-1" if expected_agent == "claude" else "CX-1"
            if default in stats_id_set:
                valid_refs = [default]
            elif stats_id_set:
                valid_refs = [sorted(stats_id_set)[0]]
            else:
                valid_refs = ["CL-1"]  # Ultimate fallback
            result.warnings.append(f"stats_refs defaulted to {valid_refs}")
        turn["stats_refs"] = valid_refs

    return result

================================================================================
 FILE: reports.py
================================================================================

"""Reporting utilities for orchestrator task runs."""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Protocol


class TaskLike(Protocol):
    id: str
    title: str
    status: str
    agent: str
    locks: list[str]
    touched_paths: list[str]
    depends_on: list[str]
    work_completed: bool | None
    turn_summary: str | None
    error: str | None
    manual_path: Path | None
    prompt_path: Path | None
    out_path: Path | None
    raw_log_path: Path | None
    patch_path: Path | None
    patch_manifest_path: Path | None
    task_dir: Path | None
    turn_obj: dict | None
    commit_sha: str | None
    commands_run: list[str] | None
    tests_added_or_modified: list[str] | None
    covered_req_ids: list[str] | None


@dataclass
class FailureClassification:
    category: str
    next_action: str


def classify_task_failure(status: str, error: str | None) -> FailureClassification:
    if status == "done":
        return FailureClassification(category="success", next_action="")

    error_text = (error or "").lower()

    if "non_progress" in error_text or "work_completed=false" in error_text:
        return FailureClassification(
            category="non_progress",
            next_action="Re-run the task manually with full implementation; ensure commands run and changes are produced.",
        )
    if status == "resource_killed" or "stopped for resources" in error_text:
        return FailureClassification(
            category="resource_killed",
            next_action="Re-run with reduced parallelism or --allow-resource-intensive; consider setting task solo.",
        )
    if "json validation" in error_text or "invalid json" in error_text:
        return FailureClassification(
            category="invalid_json",
            next_action="Fix the agent output to match the schema and re-run the task.",
        )
    if "agent exit code" in error_text:
        return FailureClassification(
            category="agent_error",
            next_action="Inspect raw logs for invocation/environment issues, then re-run the task.",
        )
    if "scope_rejected" in error_text:
        return FailureClassification(
            category="scope_rejected",
            next_action="Restrict changes to the allowed scope or adjust the task scope before re-running.",
        )
    if "merge conflict" in error_text:
        return FailureClassification(
            category="merge_conflict",
            next_action="Resolve conflicts manually or adjust locks to avoid overlap, then re-run.",
        )
    if status == "manual":
        return FailureClassification(
            category="manual_intervention",
            next_action="Follow the manual task file instructions and re-run the task.",
        )

    return FailureClassification(
        category="unknown_failure",
        next_action="Inspect logs, determine root cause, and re-run with a targeted fix.",
    )


def _load_patch_manifest(task: TaskLike) -> dict[str, Any] | None:
    manifest_path = task.patch_manifest_path
    if manifest_path is None and task.task_dir:
        candidate = task.task_dir / "changed_files.json"
        if candidate.exists():
            manifest_path = candidate
    if manifest_path and manifest_path.exists():
        try:
            return json.loads(manifest_path.read_text(encoding="utf-8"))
        except Exception:
            return None
    return None


def _diffstat_from_manifest(manifest: dict[str, Any] | None) -> dict[str, int]:
    if not manifest:
        return {"total": 0, "add": 0, "modify": 0, "delete": 0, "binary": 0}
    counts = {"total": 0, "add": 0, "modify": 0, "delete": 0, "binary": 0}
    for f in manifest.get("files", []) or []:
        op = str(f.get("operation", ""))
        counts["total"] += 1
        if op in counts:
            counts[op] += 1
    return counts


def _fmt_list(items: list[str]) -> str:
    return ", ".join(items) if items else "(none)"


def write_task_report(
    *,
    runs_dir: Path,
    task: TaskLike,
    agent_model: str | None,
    planner_profile: str,
) -> Path:
    report_dir = runs_dir / "reports"
    report_dir.mkdir(parents=True, exist_ok=True)

    safe_id = "".join(c if c.isalnum() or c in "-_" else "-" for c in task.id)
    report_path = report_dir / f"{safe_id}.md"

    classification = classify_task_failure(task.status, task.error)
    manifest = _load_patch_manifest(task)
    diffstat = _diffstat_from_manifest(manifest)

    files_changed = []
    if manifest:
        for f in manifest.get("files", []) or []:
            path = str(f.get("path", ""))
            op = str(f.get("operation", ""))
            if path:
                files_changed.append(f"{path} ({op})")

    commands_run = task.commands_run or []
    tests_added = task.tests_added_or_modified or []
    covered = task.covered_req_ids or []

    if not commands_run and task.turn_obj:
        req = task.turn_obj.get("requirement_progress", {}) or {}
        commands_run = list(req.get("commands_run", []) or [])
        tests_added = list(req.get("tests_added_or_modified", []) or [])
        covered = list(req.get("covered_req_ids", []) or [])

    turn_json = ""
    if task.turn_obj:
        try:
            turn_json = json.dumps(task.turn_obj, indent=2, sort_keys=True)
        except Exception:
            turn_json = ""

    lines = [
        f"# Task Report: {task.id} - {task.title}",
        "",
        "## Task Metadata",
        f"- Status: {task.status}",
        f"- Planner profile: {planner_profile}",
        f"- Agent: {task.agent} (model: {agent_model or 'unknown'})",
        f"- Dependencies: {_fmt_list([str(d) for d in task.depends_on])}",
        f"- Locks: {_fmt_list([str(l) for l in task.locks])}",
        f"- Touched paths: {_fmt_list([str(p) for p in task.touched_paths])}",
        f"- Work completed: {task.work_completed}",
        f"- Commit SHA: {task.commit_sha or 'none'}",
        "",
        "## Agent Output",
        f"- Summary: {task.turn_summary or '(none)'}",
        f"- Covered requirements: {_fmt_list([str(c) for c in covered])}",
        f"- Tests added/modified: {_fmt_list([str(t) for t in tests_added])}",
        f"- Commands run: {_fmt_list([str(c) for c in commands_run])}",
    ]

    if turn_json:
        lines.extend(["", "### Full Turn JSON", "```json", turn_json, "```"])
    else:
        lines.extend(["", "### Full Turn JSON", "(no valid JSON payload)"])

    lines.extend(
        [
            "",
            "## Changes",
            f"- Files changed: {_fmt_list(files_changed) if files_changed else 'no changes'}",
            f"- Diffstat: total={diffstat['total']} add={diffstat['add']} modify={diffstat['modify']} delete={diffstat['delete']} binary={diffstat['binary']}",
            f"- Patch artifact: {task.patch_path or 'none'}",
        ]
    )

    lines.extend(
        [
            "",
            "## Execution Artifacts",
            f"- Prompt: {task.prompt_path or 'none'}",
            f"- Output JSON: {task.out_path or 'none'}",
            f"- Raw log: {task.raw_log_path or 'none'}",
        ]
    )

    if task.status != "done":
        lines.extend(
            [
                "",
                "## Failure Analysis",
                f"- Root cause: {classification.category}",
                f"- Next action: {classification.next_action}",
                f"- Error: {task.error or 'none'}",
                f"- Manual file: {task.manual_path or 'none'}",
            ]
        )

    report_path.write_text("\n".join(lines), encoding="utf-8")
    return report_path

================================================================================
 FILE: scheduler.py
================================================================================

#!/usr/bin/env python3
"""Two-lane scheduler for parallel task execution.

This module implements an event-driven scheduler that maintains high worker
utilization (target: 10 concurrent workers) while respecting constraints.

Key features:
- Two-lane execution: coding lane (n-1 workers) + executor lane (1 worker)
- Priority scoring based on dependency graph analysis
- Backfilling with safe tasks when queue depth is low
- Comprehensive instrumentation for debugging utilization issues
"""

from __future__ import annotations

import time
from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any, Protocol


class SchedulableTask(Protocol):
    """Protocol for tasks that can be scheduled."""

    id: str
    status: str
    solo: bool
    intensity: str
    locks: list[str]
    touched_paths: list[str]
    depends_on: list[str]


@dataclass
class SchedulerMetrics:
    """Metrics tracked by the scheduler for instrumentation."""

    start_time: float = field(default_factory=time.monotonic)
    total_tasks: int = 0
    completed_tasks: int = 0
    failed_tasks: int = 0
    blocked_tasks: int = 0

    # Utilization tracking
    active_workers_samples: list[tuple[float, int]] = field(default_factory=list)
    queue_depth_samples: list[tuple[float, int]] = field(default_factory=list)

    # Stall tracking
    stall_events: list[dict[str, Any]] = field(default_factory=list)
    retry_counts: dict[str, int] = field(default_factory=dict)
    json_repair_counts: dict[str, int] = field(default_factory=dict)

    # Lane utilization
    coding_lane_samples: list[tuple[float, int]] = field(default_factory=list)
    executor_lane_samples: list[tuple[float, int]] = field(default_factory=list)

    def sample(
        self,
        active_workers: int,
        queue_depth: int,
        coding_lane_active: int,
        executor_lane_active: int,
    ) -> None:
        """Record a utilization sample."""
        now = time.monotonic()
        self.active_workers_samples.append((now, active_workers))
        self.queue_depth_samples.append((now, queue_depth))
        self.coding_lane_samples.append((now, coding_lane_active))
        self.executor_lane_samples.append((now, executor_lane_active))

    def record_stall(self, reason: str, task_ids: list[str]) -> None:
        """Record a stall event."""
        self.stall_events.append(
            {
                "time": time.monotonic(),
                "reason": reason,
                "task_ids": task_ids,
            }
        )

    def record_retry(self, task_id: str) -> None:
        """Record a task retry."""
        self.retry_counts[task_id] = self.retry_counts.get(task_id, 0) + 1

    def record_json_repair(self, task_id: str) -> None:
        """Record a JSON repair attempt."""
        self.json_repair_counts[task_id] = self.json_repair_counts.get(task_id, 0) + 1

    def get_average_utilization(self) -> float:
        """Get average worker utilization over the run."""
        if not self.active_workers_samples:
            return 0.0
        total = sum(count for _, count in self.active_workers_samples)
        return total / len(self.active_workers_samples)

    def to_dict(self) -> dict[str, Any]:
        """Export metrics to dictionary."""
        elapsed = time.monotonic() - self.start_time
        return {
            "elapsed_seconds": elapsed,
            "total_tasks": self.total_tasks,
            "completed_tasks": self.completed_tasks,
            "failed_tasks": self.failed_tasks,
            "blocked_tasks": self.blocked_tasks,
            "average_utilization": self.get_average_utilization(),
            "stall_count": len(self.stall_events),
            "total_retries": sum(self.retry_counts.values()),
            "total_json_repairs": sum(self.json_repair_counts.values()),
        }


@dataclass
class LaneConfig:
    """Configuration for two-lane execution."""

    coding_lane_size: int  # Number of workers for coding tasks
    executor_lane_size: int = 1  # Number of workers for heavy/executor tasks

    @classmethod
    def from_max_workers(cls, max_workers: int) -> LaneConfig:
        """Create lane config from total max workers."""
        # Reserve 1 worker for executor lane, rest for coding
        executor_size = 1
        coding_size = max(1, max_workers - executor_size)
        return cls(coding_lane_size=coding_size, executor_lane_size=executor_size)

    @property
    def total_workers(self) -> int:
        return self.coding_lane_size + self.executor_lane_size


class TwoLaneScheduler:
    """Two-lane scheduler for parallel task execution.

    Implements a two-lane model:
    - Coding lane: Most workers, handles regular implementation tasks
    - Executor lane: 1 worker, handles heavy/solo/benchmark tasks

    This ensures heavy tasks don't block coding progress.
    """

    def __init__(
        self,
        lane_config: LaneConfig,
        tasks: list[SchedulableTask],
        deps_satisfied_fn: Callable[[SchedulableTask], bool],
        locks_available_fn: Callable[[SchedulableTask], bool],
    ):
        self.lane_config = lane_config
        self.tasks = tasks
        self.by_id = {t.id: t for t in tasks}
        self.deps_satisfied = deps_satisfied_fn
        self.locks_available = locks_available_fn
        self.metrics = SchedulerMetrics(total_tasks=len(tasks))

        # Lane state
        self.coding_lane_running: set[str] = set()
        self.executor_lane_running: set[str] = set()

        # Priority cache
        self._priority_cache: dict[str, float] = {}
        self._dependents_cache: dict[str, set[str]] = {}
        self._build_dependency_graph()

    def _build_dependency_graph(self) -> None:
        """Build reverse dependency graph for priority scoring."""
        self._dependents_cache = {t.id: set() for t in self.tasks}
        for t in self.tasks:
            for dep_id in t.depends_on:
                if dep_id in self._dependents_cache:
                    self._dependents_cache[dep_id].add(t.id)

    def update_tasks(self, tasks: list[SchedulableTask]) -> None:
        """Update the task list with new tasks (e.g., from backfill generator).

        This is used to add new tasks dynamically during execution.
        """
        self.tasks = tasks
        self.by_id = {t.id: t for t in tasks}
        self.metrics.total_tasks = len(tasks)
        # Rebuild dependency graph and clear priority cache
        self._priority_cache.clear()
        self._build_dependency_graph()

    def _compute_priority(self, task: SchedulableTask) -> float:
        """Compute priority score for a task.

        Higher score = higher priority. Factors:
        1. Number of dependents (unblocks more work)
        2. On critical path (chain length to root)
        3. Intensity (defer heavy tasks unless necessary)
        """
        if task.id in self._priority_cache:
            return self._priority_cache[task.id]

        # Base score: number of tasks this unblocks
        direct_dependents = len(self._dependents_cache.get(task.id, set()))

        # Transitive dependents (weighted less)
        transitive = self._count_transitive_dependents(task.id)
        dependent_score = direct_dependents * 10 + transitive

        # Intensity penalty for executor lane tasks
        intensity_penalty = 0
        if self._is_executor_lane_task(task):
            intensity_penalty = 5  # Slight preference for coding lane tasks

        # Calculate final priority
        priority = dependent_score - intensity_penalty
        self._priority_cache[task.id] = priority
        return priority

    def _count_transitive_dependents(self, task_id: str, visited: set | None = None) -> int:
        """Count transitive dependents for priority scoring."""
        if visited is None:
            visited = set()
        if task_id in visited:
            return 0
        visited.add(task_id)

        dependents = self._dependents_cache.get(task_id, set())
        count = len(dependents)
        for dep in dependents:
            count += self._count_transitive_dependents(dep, visited)
        return count

    def _is_executor_lane_task(self, task: SchedulableTask) -> bool:
        """Check if task should run in executor lane."""
        # Solo tasks always go to executor lane
        return task.solo or task.intensity == "high"

    def _can_start_in_coding_lane(self, task: SchedulableTask) -> bool:
        """Check if task can start in coding lane."""
        return not self._is_executor_lane_task(task) and len(self.coding_lane_running) < self.lane_config.coding_lane_size

    def _can_start_in_executor_lane(self, task: SchedulableTask) -> bool:
        """Check if task can start in executor lane."""
        # Solo tasks require empty executor lane AND empty coding lane
        if task.solo and (self.coding_lane_running or self.executor_lane_running):
            return False
        return len(self.executor_lane_running) < self.lane_config.executor_lane_size

    def can_start(self, task: SchedulableTask) -> bool:
        """Check if a task can start now."""
        if task.status != "pending":
            return False
        if not self.deps_satisfied(task):
            return False
        if not self.locks_available(task):
            return False

        # Check lane availability
        if self._is_executor_lane_task(task):
            return self._can_start_in_executor_lane(task)
        else:
            # Coding tasks can also overflow to executor lane if coding lane is full
            if self._can_start_in_coding_lane(task):
                return True
            # Allow coding tasks to use executor lane when coding lane is full
            # but only if it's not a solo situation
            return not task.solo and self._can_start_in_executor_lane(task)

    def get_ready_tasks(self) -> list[SchedulableTask]:
        """Get all tasks that can start now, sorted by priority."""
        ready = [t for t in self.tasks if self.can_start(t)]
        # Sort by priority (highest first)
        ready.sort(key=lambda t: self._compute_priority(t), reverse=True)
        return ready

    def get_ready_tasks_by_lane(self) -> tuple[list[SchedulableTask], list[SchedulableTask]]:
        """Get ready tasks separated by lane.

        Returns:
            Tuple of (coding_lane_tasks, executor_lane_tasks)
        """
        coding = []
        executor = []

        for task in self.get_ready_tasks():
            if self._is_executor_lane_task(task):
                executor.append(task)
            else:
                coding.append(task)

        return coding, executor

    def assign_to_lane(self, task_id: str) -> str:
        """Assign a task to a lane when it starts.

        Returns:
            Lane name ("coding" or "executor")
        """
        task = self.by_id.get(task_id)
        if not task:
            return "coding"

        if self._is_executor_lane_task(task):
            self.executor_lane_running.add(task_id)
            return "executor"
        elif len(self.coding_lane_running) < self.lane_config.coding_lane_size:
            self.coding_lane_running.add(task_id)
            return "coding"
        else:
            # Overflow to executor lane
            self.executor_lane_running.add(task_id)
            return "executor"

    def release_from_lane(self, task_id: str) -> None:
        """Release a task from its lane when it completes."""
        self.coding_lane_running.discard(task_id)
        self.executor_lane_running.discard(task_id)

    def get_lane_stats(self) -> dict[str, int]:
        """Get current lane utilization stats."""
        return {
            "coding_active": len(self.coding_lane_running),
            "coding_capacity": self.lane_config.coding_lane_size,
            "executor_active": len(self.executor_lane_running),
            "executor_capacity": self.lane_config.executor_lane_size,
        }

    def sample_metrics(self, queue_depth: int) -> None:
        """Record current utilization metrics."""
        self.metrics.sample(
            active_workers=len(self.coding_lane_running) + len(self.executor_lane_running),
            queue_depth=queue_depth,
            coding_lane_active=len(self.coding_lane_running),
            executor_lane_active=len(self.executor_lane_running),
        )


@dataclass
class FillerTask:
    """A filler task generated for backfilling."""

    id: str
    title: str
    description: str
    task_type: str  # "lint", "docs", "test", "type_hints", "schema_lint"
    priority: int = 0


class BackfillGenerator:
    """Generates filler tasks when queue depth is low.

    Filler tasks are safe, always-beneficial work that keeps workers busy:
    - Lint fixes
    - Documentation improvements
    - Unit test additions
    - Type hint additions
    - Schema validation fixes

    IMPORTANT: Backfill tasks are scope-constrained to prevent merge conflicts.
    By default, they may only modify files in: tests/, docs/, .github/

    CRITICAL FILES EXCLUDED (even with allow_bridge=True):
    - bridge/loop.py
    - bridge/design_doc.py
    - bridge/patch_integration.py
    - bridge/merge_resolver.py
    - bridge/scheduler.py
    - bridge/loop_pkg/**
    - DESIGN_DOCUMENT.md

    NO-OP STREAK COOLDOWN:
    If a backfill task type repeatedly produces empty patches ("No changes to commit"),
    that type is put on cooldown and skipped for N generation cycles. This prevents
    wasting agent credits on no-op work.
    """

    # Allowed directories for backfill tasks (bridge/ excluded by default for safety)
    ALLOWED_DIRS = ["tests/", "docs/", ".github/"]

    # Extended allowed dirs when bridge is explicitly enabled
    ALLOWED_DIRS_WITH_BRIDGE = ["tests/", "docs/", "bridge/", ".github/"]

    # Files that should NEVER be touched by backfill, even with allow_bridge
    EXCLUDED_CORE_FILES = [
        "bridge/loop.py",
        "bridge/design_doc.py",
        "bridge/patch_integration.py",
        "bridge/merge_resolver.py",
        "bridge/scheduler.py",
        "bridge/loop_pkg/",
        "DESIGN_DOCUMENT.md",
    ]

    # Cooldown configuration for no-op streak suppression
    NOOP_STREAK_THRESHOLD = 2  # Put type on cooldown after this many consecutive no-ops
    COOLDOWN_CYCLES = 5  # Skip this many generation cycles when on cooldown

    # Task types with scope-aware descriptions (bridge removed from defaults)
    TASK_TYPES = [
        (
            "lint",
            "Fix linting issues in tests/",
            "Run ruff check on tests/ directory and fix any issues. "
            "MAX 3 files changed. Do NOT touch src/, bridge/, or any files outside tests/, docs/.",
        ),
        (
            "test",
            "Add unit tests",
            "Add missing unit tests for uncovered code in tests/ directory. "
            "MAX 2 new test files. Do NOT touch src/ or any non-test files.",
        ),
        (
            "type_hints",
            "Add type hints to tests/",
            "Add type annotations to untyped functions in tests/ directory only. "
            "MAX 3 files changed. Do NOT touch src/, bridge/, or anything outside tests/.",
        ),
        (
            "docs",
            "Improve documentation",
            "Add or improve docstrings in docs/ directory. MAX 3 files changed. Do NOT touch src/ or bridge/.",
        ),
        (
            "schema_lint",
            "Fix test schema issues",
            "Validate and fix JSON schema issues in tests/ directory. MAX 2 files changed. Do NOT touch src/ or bridge/.",
        ),
    ]

    # Alternative task types when bridge is allowed
    TASK_TYPES_WITH_BRIDGE = [
        (
            "lint",
            "Fix linting issues in tests/",
            "Run ruff check on tests/ directory and fix any issues. "
            "MAX 3 files changed. Do NOT touch src/, bridge/loop.py, bridge/patch_integration.py, DESIGN_DOCUMENT.md.",
        ),
        (
            "test",
            "Add unit tests",
            "Add missing unit tests for uncovered code in tests/ directory. "
            "MAX 2 new test files. Do NOT touch src/ or any non-test files.",
        ),
        (
            "type_hints",
            "Add type hints",
            "Add type annotations to untyped functions in tests/ or safe bridge/ modules. "
            "MAX 3 files. NEVER touch: bridge/loop.py, bridge/patch_integration.py, bridge/scheduler.py.",
        ),
        (
            "docs",
            "Improve documentation",
            "Add or improve docstrings in docs/ or safe bridge/ modules. "
            "MAX 3 files. NEVER touch: bridge/loop.py, bridge/design_doc.py.",
        ),
        (
            "schema_lint",
            "Fix schema issues",
            "Validate and fix JSON schema issues in tests/ directory. "
            "MAX 2 files changed. Do NOT touch src/ or orchestrator core files.",
        ),
    ]

    # Profile-based task type restrictions
    # In "throughput" profile, disable low-ROI task types by default
    THROUGHPUT_PROFILE_DISABLED_TYPES = {"docs", "type_hints"}  # Low ROI, often scope-rejected

    # Rejection threshold for permanent disable (per-run)
    REJECTION_DISABLE_THRESHOLD = 2  # After 2 SCOPE_REJECTED, disable type for entire run

    def __init__(
        self,
        project_root: str,
        min_queue_depth: int = 10,
        allow_bridge: bool = False,
        planner_profile: str = "default",
    ):
        self.project_root = project_root
        self.min_queue_depth = min_queue_depth
        self.generated_count = 0
        self.allow_bridge = allow_bridge
        self.planner_profile = planner_profile
        self.disabled = planner_profile == "engineering"

        # No-op streak tracking for cooldown suppression
        # Maps task_type -> consecutive no-op count
        self._noop_streaks: dict[str, int] = {}
        # Maps task_type -> cycle number when cooldown started
        self._cooldown_start: dict[str, int] = {}
        # Total cycles (increments each call to generate_filler_tasks)
        self._generation_cycle = 0

        # Rejection tracking for permanent disable
        # Maps task_type -> total rejection count
        self._rejection_counts: dict[str, int] = {}
        # Types permanently disabled for this run
        self._permanently_disabled: set[str] = set()

        # Log profile-based policy
        if planner_profile == "throughput":
            print(f"[backfill] Profile '{planner_profile}': Disabled low-ROI types: {self.THROUGHPUT_PROFILE_DISABLED_TYPES}")
        if self.disabled:
            print(f"[backfill] Profile '{planner_profile}': Backfill disabled")

    def should_generate(self, current_queue_depth: int, worker_count: int) -> bool:
        """Check if backfill tasks should be generated."""
        if self.disabled:
            return False
        target_depth = worker_count * 2
        return current_queue_depth < min(target_depth, self.min_queue_depth)

    def record_noop_result(self, task_id: str) -> None:
        """Record that a filler task produced no changes (no-op).

        Call this when a FILLER-* task completes with "No changes to commit".
        After NOOP_STREAK_THRESHOLD consecutive no-ops for a type, that type
        goes on cooldown.

        Args:
            task_id: The task ID (e.g., "FILLER-LINT-001")
        """
        # Extract task type from ID: "FILLER-LINT-001" -> "lint"
        if not task_id.startswith("FILLER-"):
            return
        parts = task_id.split("-")
        if len(parts) < 3:
            return
        task_type = parts[1].lower()

        # Increment no-op streak
        streak = self._noop_streaks.get(task_type, 0) + 1
        self._noop_streaks[task_type] = streak

        if streak >= self.NOOP_STREAK_THRESHOLD:
            # Put on cooldown
            self._cooldown_start[task_type] = self._generation_cycle
            print(f"[backfill] Task type '{task_type}' on cooldown after {streak} consecutive no-ops")

    def record_successful_result(self, task_id: str) -> None:
        """Record that a filler task produced actual changes.

        Call this when a FILLER-* task completes with a commit.
        Resets the no-op streak and removes cooldown.

        Args:
            task_id: The task ID (e.g., "FILLER-LINT-001")
        """
        if not task_id.startswith("FILLER-"):
            return
        parts = task_id.split("-")
        if len(parts) < 3:
            return
        task_type = parts[1].lower()

        # Reset streak and remove from cooldown
        self._noop_streaks[task_type] = 0
        self._cooldown_start.pop(task_type, None)

    def record_rejection(self, task_id: str) -> None:
        """Record that a filler task was rejected (SCOPE_REJECTED).

        Rejections are tracked more strictly than no-ops:
        - Counts toward cooldown (like no-ops)
        - After REJECTION_DISABLE_THRESHOLD rejections, type is PERMANENTLY disabled for the run

        Args:
            task_id: The task ID (e.g., "FILLER-LINT-001")
        """
        if not task_id.startswith("FILLER-"):
            return
        parts = task_id.split("-")
        if len(parts) < 3:
            return
        task_type = parts[1].lower()

        # Track rejection count
        self._rejection_counts[task_type] = self._rejection_counts.get(task_type, 0) + 1
        rejection_count = self._rejection_counts[task_type]

        # Check if type should be permanently disabled
        if rejection_count >= self.REJECTION_DISABLE_THRESHOLD:
            if task_type not in self._permanently_disabled:
                self._permanently_disabled.add(task_type)
                print(
                    f"[backfill] Task type '{task_type}' PERMANENTLY DISABLED for this run "
                    f"after {rejection_count} SCOPE_REJECTED events"
                )
        else:
            print(
                f"[backfill] Task type '{task_type}' rejection {rejection_count}/{self.REJECTION_DISABLE_THRESHOLD} "
                "(will disable at threshold)"
            )

        # Also treat as no-op for cooldown purposes
        self.record_noop_result(task_id)

    def is_type_on_cooldown(self, task_type: str) -> bool:
        """Check if a task type is currently on cooldown or disabled.

        Args:
            task_type: The task type (e.g., "lint", "test")

        Returns:
            True if the type is on cooldown/disabled and should be skipped
        """
        if self.disabled:
            return True
        task_type = task_type.lower()

        # Check if permanently disabled for this run (due to repeated rejections)
        if task_type in self._permanently_disabled:
            return True

        # Check if disabled by profile policy (e.g., "throughput" disables docs/type_hints)
        if self.planner_profile == "throughput" and task_type in self.THROUGHPUT_PROFILE_DISABLED_TYPES:
            return True

        if task_type not in self._cooldown_start:
            return False

        # Check if cooldown has expired
        cycles_since = self._generation_cycle - self._cooldown_start[task_type]
        if cycles_since >= self.COOLDOWN_CYCLES:
            # Cooldown expired - remove and reset streak
            del self._cooldown_start[task_type]
            self._noop_streaks[task_type] = 0
            print(f"[backfill] Task type '{task_type}' cooldown expired, re-enabling")
            return False

        return True

    def get_cooldown_status(self) -> dict[str, int]:
        """Get the current cooldown status for all types.

        Returns:
            Dict mapping task_type to remaining cooldown cycles
        """
        status = {}
        for task_type, start_cycle in self._cooldown_start.items():
            remaining = self.COOLDOWN_CYCLES - (self._generation_cycle - start_cycle)
            if remaining > 0:
                status[task_type] = remaining
        return status

    def generate_filler_tasks(self, count: int) -> list[FillerTask]:
        """Generate filler tasks.

        Skips task types that are on cooldown due to repeated no-op results.
        If all types are on cooldown, returns fewer tasks than requested.

        Args:
            count: Number of filler tasks to generate

        Returns:
            List of FillerTask objects (may be fewer than count if types are on cooldown)
        """
        if self.disabled:
            return []
        # Increment generation cycle for cooldown tracking
        self._generation_cycle += 1

        # Use appropriate task types based on configuration
        task_types = self.TASK_TYPES_WITH_BRIDGE if self.allow_bridge else self.TASK_TYPES

        tasks = []
        # Track attempts to avoid infinite loop if all types are on cooldown
        max_attempts = count + len(task_types)
        attempts = 0

        while len(tasks) < count and attempts < max_attempts:
            attempts += 1

            # Use generated_count for task type selection BEFORE incrementing
            # This ensures task types rotate across calls even when count=1
            type_index = self.generated_count % len(task_types)
            task_type, title, description = task_types[type_index]

            # Check if this type is on cooldown
            if self.is_type_on_cooldown(task_type):
                # Skip this type and try the next one
                self.generated_count += 1
                continue

            self.generated_count += 1
            task_id = f"FILLER-{task_type.upper()}-{self.generated_count:03d}"

            # Add explicit exclusion reminder for core files
            exclusion_note = (
                "\n\nCRITICAL EXCLUSIONS - NEVER MODIFY THESE FILES:\n"
                "- DESIGN_DOCUMENT.md (user-owned, read-only)\n"
                "- bridge/loop.py (orchestrator core)\n"
                "- bridge/patch_integration.py (orchestrator core)\n"
                "- bridge/scheduler.py (orchestrator core)\n"
                "- bridge/design_doc.py (orchestrator core)\n"
            )

            tasks.append(
                FillerTask(
                    id=task_id,
                    title=title,
                    description=description + exclusion_note,
                    task_type=task_type,
                    priority=-10,  # Low priority so real work takes precedence
                )
            )

        if len(tasks) < count:
            cooldown_status = self.get_cooldown_status()
            print(f"[backfill] Generated {len(tasks)}/{count} tasks (types on cooldown: {cooldown_status})")

        return tasks


def create_scheduler(
    max_workers: int,
    tasks: list,
    deps_satisfied_fn: Callable,
    locks_available_fn: Callable,
) -> TwoLaneScheduler:
    """Create a two-lane scheduler with default configuration.

    Args:
        max_workers: Maximum number of workers
        tasks: List of tasks to schedule
        deps_satisfied_fn: Function to check if dependencies are satisfied
        locks_available_fn: Function to check if locks are available

    Returns:
        Configured TwoLaneScheduler instance
    """
    lane_config = LaneConfig.from_max_workers(max_workers)
    return TwoLaneScheduler(
        lane_config=lane_config,
        tasks=tasks,
        deps_satisfied_fn=deps_satisfied_fn,
        locks_available_fn=locks_available_fn,
    )

================================================================================
 FILE: smoke_route.py
================================================================================

from __future__ import annotations

from collections.abc import Sequence


def _normalize_route(route: Sequence[str]) -> tuple[str, ...]:
    normalized = tuple(route)
    if not normalized:
        raise ValueError("smoke route must include at least one agent")
    return normalized


def next_agent_for_route(route: Sequence[str], index: int) -> str:
    """Return the agent at the given route index.

    Raises:
        ValueError: when the route is empty.
        IndexError: when index is outside the route bounds.
    """
    normalized = _normalize_route(route)
    if index < 0 or index >= len(normalized):
        raise IndexError(f"smoke route index {index} out of range for length {len(normalized)}")
    return normalized[index]


def smoke_route_override_reason(
    *,
    requested: str,
    routed: str,
    index: int,
    route_len: int,
) -> str | None:
    """Return a clear reason when the smoke route overrides the requested agent."""
    if requested == routed:
        return None
    return f"smoke route override (index {index + 1}/{route_len}): {requested} -> {routed}"


def resolve_smoke_route(
    *,
    requested: str,
    route: Sequence[str],
    index: int,
) -> tuple[str, str | None]:
    """Resolve the next agent from the smoke route with an override reason."""
    normalized = _normalize_route(route)
    routed = next_agent_for_route(normalized, index)
    reason = smoke_route_override_reason(
        requested=requested,
        routed=routed,
        index=index,
        route_len=len(normalized),
    )
    return routed, reason

================================================================================
 FILE: streaming.py
================================================================================

from __future__ import annotations

import contextlib
import subprocess
import sys
import threading
from pathlib import Path
from typing import Any


def run_cmd_with_streaming(
    *,
    cmd: list[str],
    cwd: Path,
    env: dict[str, str],
    agent: str,
    stream_mode: str,
    call_dir: Path,
) -> tuple[int, str, str]:
    """Run subprocess with prefixed streaming and log file output."""
    proc = subprocess.Popen(
        cmd,
        cwd=str(cwd),
        env=env,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        bufsize=1,
    )

    out_chunks: list[str] = []
    err_chunks: list[str] = []

    call_dir.mkdir(parents=True, exist_ok=True)
    stdout_log = call_dir / "agent_stdout.log"
    stderr_log = call_dir / "agent_stderr.log"

    stream_mode = stream_mode.lower()
    stream_stdout = stream_mode in ("stdout", "both")
    stream_stderr = stream_mode in ("stderr", "both")

    def _pump(src: Any, chunks: list[str], log_path: Path, *, is_err: bool) -> None:
        try:
            assert src is not None
            with log_path.open("w", encoding="utf-8") as log_file:
                for line in iter(src.readline, ""):
                    chunks.append(line)
                    log_file.write(line)
                    log_file.flush()
                    if is_err:
                        if stream_stderr:
                            sys.stderr.write(f"[{agent}][stderr] {line}")
                            sys.stderr.flush()
                    else:
                        if stream_stdout:
                            sys.stdout.write(f"[{agent}][stdout] {line}")
                            sys.stdout.flush()
        finally:
            with contextlib.suppress(Exception):
                src.close()

    t_out = threading.Thread(
        target=_pump,
        args=(proc.stdout, out_chunks, stdout_log),
        kwargs={"is_err": False},
        daemon=True,
    )
    t_err = threading.Thread(
        target=_pump,
        args=(proc.stderr, err_chunks, stderr_log),
        kwargs={"is_err": True},
        daemon=True,
    )
    t_out.start()
    t_err.start()

    rc = proc.wait()
    t_out.join(timeout=2)
    t_err.join(timeout=2)
    return rc, "".join(out_chunks), "".join(err_chunks)

================================================================================
 FILE: task_plan.schema.json
================================================================================

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "ParallelTaskPlan",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "milestone_id",
    "max_parallel_tasks",
    "tasks",
    "rationale"
  ],
  "properties": {
    "milestone_id": {"type": "string"},
    "max_parallel_tasks": {"type": "integer", "minimum": 1, "maximum": 32},
    "rationale": {"type": "string"},
    "tasks": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "additionalProperties": false,
        "required": [
          "id",
          "title",
          "description",
          "preferred_agent",
          "estimated_intensity",
          "locks",
          "touched_paths",
          "depends_on",
          "solo"
        ],
        "properties": {
          "id": {"type": "string"},
          "title": {"type": "string"},
          "description": {"type": "string"},
          "preferred_agent": {"type": "string", "enum": ["codex", "claude", "either"]},
          "estimated_intensity": {"type": "string", "enum": ["low", "medium", "high"]},
          "locks": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Named locks for exclusive access (e.g., 'config', 'database')"
          },
          "touched_paths": {
            "type": "array",
            "items": {"type": "string"},
            "description": "File paths this task will modify. Tasks with overlapping paths cannot run concurrently."
          },
          "depends_on": {"type": "array", "items": {"type": "string"}},
          "solo": {"type": "boolean"}
        }
      }
    }
  }
}

================================================================================
 FILE: turn.schema.json
================================================================================

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "AgentTurn",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "agent",
    "milestone_id",
    "phase",
    "work_completed",
    "project_complete",
    "summary",
    "gates_passed",
    "requirement_progress",
    "next_agent",
    "next_prompt",
    "delegate_rationale",
    "stats_refs",
    "needs_write_access",
    "artifacts"
  ],
  "properties": {
    "agent": {
      "type": "string",
      "enum": ["codex", "claude"]
    },
    "milestone_id": {
      "type": "string",
      "minLength": 1
    },
    "phase": {
      "type": "string",
      "enum": ["plan", "implement", "verify", "finalize"]
    },
    "work_completed": {
      "type": "boolean"
    },
    "project_complete": {
      "type": "boolean"
    },
    "summary": {
      "type": "string"
    },
    "gates_passed": {
      "type": "array",
      "items": {"type": "string"}
    },
    "requirement_progress": {
      "type": "object",
      "additionalProperties": false,
      "required": ["covered_req_ids", "tests_added_or_modified", "commands_run"],
      "properties": {
        "covered_req_ids": {
          "type": "array",
          "items": {"type": "string"}
        },
        "tests_added_or_modified": {
          "type": "array",
          "items": {"type": "string"}
        },
        "commands_run": {
          "type": "array",
          "items": {"type": "string"}
        }
      }
    },
    "next_agent": {
      "type": "string",
      "enum": ["codex", "claude"]
    },
    "next_prompt": {
      "type": "string"
    },
    "delegate_rationale": {
      "type": "string"
    },
    "stats_refs": {
      "type": "array",
      "minItems": 1,
      "items": {"type": "string"}
    },
    "needs_write_access": {
      "type": "boolean"
    },
    "artifacts": {
      "type": "array",
      "items": {
        "type": "object",
        "additionalProperties": false,
        "required": ["path", "description"],
        "properties": {
          "path": {"type": "string"},
          "description": {"type": "string"}
        }
      }
    }
  }
}

================================================================================
 FILE: turn_payload.schema.json
================================================================================

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "AgentTurnPayload",
  "description": "Minimal payload schema for agent output. Orchestrator fills invariants.",
  "type": "object",
  "required": [
    "summary",
    "work_completed",
    "project_complete"
  ],
  "properties": {
    "summary": {
      "type": "string",
      "description": "Brief description of work done"
    },
    "work_completed": {
      "type": "boolean",
      "description": "Whether the assigned work was completed"
    },
    "project_complete": {
      "type": "boolean",
      "description": "Whether the entire project is complete",
      "default": false
    },
    "phase": {
      "type": "string",
      "enum": ["plan", "implement", "verify", "finalize"],
      "description": "Current work phase",
      "default": "implement"
    },
    "next_prompt": {
      "type": "string",
      "description": "Prompt for next agent (if delegating)",
      "default": ""
    },
    "delegate_rationale": {
      "type": "string",
      "description": "Reason for delegating or completing",
      "default": ""
    },
    "gates_passed": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Verification gates passed",
      "default": []
    },
    "requirement_progress": {
      "type": "object",
      "properties": {
        "covered_req_ids": {"type": "array", "items": {"type": "string"}, "default": []},
        "tests_added_or_modified": {"type": "array", "items": {"type": "string"}, "default": []},
        "commands_run": {"type": "array", "items": {"type": "string"}, "default": []}
      },
      "default": {}
    },
    "artifacts": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["path", "description"],
        "properties": {
          "path": {"type": "string"},
          "description": {"type": "string"}
        }
      },
      "description": "Files created or modified",
      "default": []
    },
    "next_agent": {
      "type": "string",
      "enum": ["codex", "claude"],
      "description": "Next agent to run (orchestrator may override)"
    },
    "needs_write_access": {
      "type": "boolean",
      "description": "Whether next phase needs write access",
      "default": true
    },
    "agent": {
      "type": "string",
      "description": "Agent identity (orchestrator will override)"
    },
    "milestone_id": {
      "type": "string",
      "description": "Milestone ID (orchestrator will override)"
    },
    "stats_refs": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Stats references (orchestrator will fill if missing)"
    }
  },
  "additionalProperties": true
}

================================================================================
 FILE: turns.py
================================================================================

"""Helpers for building schema-valid agent turns."""

from __future__ import annotations

from collections.abc import Mapping, Sequence
from typing import Any

AGENTS: tuple[str, ...] = ("codex", "claude")
VALID_PHASES: tuple[str, ...] = ("plan", "implement", "verify", "finalize")
DEFAULT_STATS_REF = {"codex": "CX-1", "claude": "CL-1"}
DEFAULT_ERROR_PHASE = "plan"


def build_error_turn(
    *,
    agent: str,
    milestone_id: str,
    summary: str,
    error_detail: str | None = None,
    next_agent: str | None = None,
    next_prompt: str = "",
    delegate_rationale: str = "",
    stats_refs: Sequence[str] | None = None,
    stats_id_set: set[str] | None = None,
    phase: str | None = None,
    needs_write_access: bool = True,
    work_completed: bool = False,
    project_complete: bool = False,
    gates_passed: Sequence[str] | None = None,
    requirement_progress: Mapping[str, Sequence[str]] | None = None,
    artifacts: Sequence[Mapping[str, str]] | None = None,
) -> dict[str, Any]:
    """Build a schema-valid error turn with safe defaults."""
    agent_val = str(agent).strip().lower()
    agent_id = agent_val if agent_val in AGENTS else "codex"
    phase_val = phase if phase in VALID_PHASES else DEFAULT_ERROR_PHASE
    next_agent_val = next_agent if next_agent in AGENTS else ("claude" if agent_id == "codex" else "codex")
    stats = normalize_stats_refs(agent_id, stats_refs, stats_id_set)
    return {
        "agent": agent_id,
        "milestone_id": str(milestone_id),
        "phase": phase_val,
        "work_completed": bool(work_completed),
        "project_complete": bool(project_complete),
        "summary": _join_summary(summary, error_detail),
        "gates_passed": _to_str_list(gates_passed),
        "requirement_progress": normalize_requirement_progress(requirement_progress),
        "next_agent": next_agent_val,
        "next_prompt": str(next_prompt),
        "delegate_rationale": str(delegate_rationale),
        "stats_refs": stats,
        "needs_write_access": bool(needs_write_access),
        "artifacts": normalize_artifacts(artifacts),
    }


def error_turn(**kwargs: Any) -> dict[str, Any]:
    """Alias for build_error_turn for convenience."""
    return build_error_turn(**kwargs)


def normalize_stats_refs(
    agent: str,
    stats_refs: Sequence[str] | None,
    stats_id_set: set[str] | None = None,
) -> list[str]:
    """Normalize stats refs, ensuring at least one valid entry."""
    refs = _to_str_list(stats_refs)
    if stats_id_set:
        refs = [r for r in refs if r in stats_id_set]
    if refs:
        return refs
    fallback = DEFAULT_STATS_REF.get(agent, "CX-1")
    if stats_id_set:
        if fallback in stats_id_set:
            return [fallback]
        if stats_id_set:
            return [sorted(stats_id_set)[0]]
    return [fallback]


def normalize_requirement_progress(
    val: Mapping[str, Sequence[str]] | None,
) -> dict[str, list[str]]:
    """Normalize requirement progress entries to string lists."""
    rp = val if isinstance(val, Mapping) else {}
    return {
        "covered_req_ids": _to_str_list(rp.get("covered_req_ids")),
        "tests_added_or_modified": _to_str_list(rp.get("tests_added_or_modified")),
        "commands_run": _to_str_list(rp.get("commands_run")),
    }


def normalize_artifacts(val: Sequence[Mapping[str, str]] | None) -> list[dict[str, str]]:
    """Normalize artifact list to required schema shape."""
    if not isinstance(val, Sequence):
        return []
    out: list[dict[str, str]] = []
    for item in val:
        if not isinstance(item, Mapping):
            continue
        path = item.get("path")
        desc = item.get("description")
        if isinstance(path, str) and path.strip() and isinstance(desc, str):
            out.append({"path": path.strip(), "description": desc.strip()})
    return out


def _to_str_list(val: Any) -> list[str]:
    if not isinstance(val, Sequence) or isinstance(val, (str, bytes)):
        return []
    return [str(x).strip() for x in val if isinstance(x, str) and x.strip()]


def _join_summary(summary: str, error_detail: str | None) -> str:
    base = str(summary or "").strip()
    detail = str(error_detail or "").strip()
    if detail:
        return f"{base}\n{detail}" if base else detail
    return base


__all__ = [
    "AGENTS",
    "VALID_PHASES",
    "build_error_turn",
    "error_turn",
    "normalize_artifacts",
    "normalize_requirement_progress",
    "normalize_stats_refs",
]

================================================================================
 FILE: verify_repair.py
================================================================================

"""Verify auto-repair: Automatic repair loop for verify failures.

This module is a backwards-compatible shim. The implementation has been
moved to bridge/verify_repair/ package for better organization.

See bridge/verify_repair/__init__.py for the full API.
"""

from __future__ import annotations

# Re-export all public API for backwards compatibility
from bridge.verify_repair import (
    FailureCategory,
    RepairAttemptRecord,
    RepairLoopReport,
    RepairLoopResult,
    VerifyGateResult,
    VerifySummary,
    classify_failures,
    compute_failure_signature,
    run_verify_repair_loop,
    write_repair_report,
)

# Legacy aliases
RepairReport = RepairAttemptRecord

__all__ = [
    "RepairReport",
    "RepairLoopResult",
    "run_verify_repair_loop",
    "write_repair_report",
    # New exports
    "FailureCategory",
    "RepairAttemptRecord",
    "RepairLoopReport",
    "VerifyGateResult",
    "VerifySummary",
    "classify_failures",
    "compute_failure_signature",
]


# CLI entrypoint for testing
def main() -> int:
    """CLI entrypoint for testing the repair loop."""
    import argparse
    from pathlib import Path

    parser = argparse.ArgumentParser(description="Run verify auto-repair loop")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    parser.add_argument("--max-attempts", type=int, default=5, help="Maximum repair attempts")
    parser.add_argument("--strict-git", action="store_true", help="Use strict git mode")
    parser.add_argument("--output", default="verify_repair_report.json", help="Output report path")
    parser.add_argument("--no-bootstrap", action="store_true", help="Skip initial bootstrap")
    args = parser.parse_args()

    project_root = Path(args.project_root).resolve()
    verify_json = project_root / "verify_repair_temp.json"

    result = run_verify_repair_loop(
        project_root=project_root,
        verify_json_path=verify_json,
        max_attempts=args.max_attempts,
        strict_git=args.strict_git,
        verbose=True,
        bootstrap_on_start=not args.no_bootstrap,
    )

    output_path = project_root / args.output
    write_repair_report(result, output_path)
    print(f"\n[verify_repair] Report written to: {output_path}")

    return 0 if result.success else 1


if __name__ == "__main__":
    import sys

    sys.exit(main())

================================================================================
 FILE: agents\claude.sh
================================================================================

#!/usr/bin/env bash
set -euo pipefail

PROMPT_FILE="${1:?prompt_file}"
SCHEMA_FILE="${2:?schema_file}"
OUT_FILE="${3:?out_file}"

MODEL="${CLAUDE_MODEL:-claude-opus-4-5}"
FF_SMOKE="${FF_SMOKE:-0}"
DEFAULT_TIMEOUT_S=86400
SMOKE_TIMEOUT_S=180
if [[ "$FF_SMOKE" == "1" ]]; then
  TIMEOUT_S="${CLAUDE_TIMEOUT_S:-$SMOKE_TIMEOUT_S}"
else
  TIMEOUT_S="${CLAUDE_TIMEOUT_S:-$DEFAULT_TIMEOUT_S}"
fi
CLAUDE_BIN="${CLAUDE_BIN:-claude}"
CLAUDE_ARGS_JSON_MODE="${CLAUDE_ARGS_JSON_MODE:-}"
CLAUDE_TOOLS="${CLAUDE_TOOLS:-}"
CLAUDE_HELP_TIMEOUT_S="${CLAUDE_HELP_TIMEOUT_S:-5}"
SMOKE_DIR="${FF_AGENT_SMOKE_DIR:-}"
WRITE_ACCESS="${WRITE_ACCESS:-0}"

# Schema kind signal from orchestrator: "task_plan", "turn", or "json" (default: "turn")
# - task_plan: Skip turn normalization, extract JSON matching task_plan schema
# - turn: Full turn normalization with tools enabled
# - json: Generic JSON mode for arbitrary schemas (no task_plan keys, no tool encouragement)
ORCH_SCHEMA_KIND="${ORCH_SCHEMA_KIND:-turn}"

prompt="$(cat "$PROMPT_FILE")"

if [[ -n "$SMOKE_DIR" && "$WRITE_ACCESS" == "1" ]]; then
  mkdir -p "$SMOKE_DIR"
  printf '%s %s\n' "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" "claude" > "$SMOKE_DIR/claude.txt"
fi

# Build schema-aware prompt reminder
# The reminder tells the model which schema to use based on what was requested
SCHEMA_BASENAME="$(basename "$SCHEMA_FILE")"

# CRITICAL: Distinguish between planning mode (pure JSON, no tools) vs execution mode (tools enabled)
# - task_plan mode: No tools, output pure JSON task plan
# - turn mode: Tools ARE enabled (Read, Edit, Write, Bash), but final output must be JSON
#
# The contradiction "tools are disabled" for turn mode caused workers to bail out.
# Workers in turn mode have full tool access via Claude Code CLI.

if [[ "$ORCH_SCHEMA_KIND" == "task_plan" ]]; then
  # Planning mode: Tools truly are disabled, we want pure JSON output
  prompt="${prompt}

REMINDER (NON-NEGOTIABLE) - PLANNING MODE:
- You are in PLANNING MODE. Focus on generating a task plan, not implementation.
- Output EXACTLY ONE JSON object matching ${SCHEMA_BASENAME}.
- Top-level required keys: milestone_id, max_parallel_tasks, rationale, tasks (array).
- Each task in tasks array MUST have these exact keys:
  id, title, description, preferred_agent, estimated_intensity, locks, depends_on, solo
- IMPORTANT KEY NAMES:
  * Use \"estimated_intensity\" (NOT \"intensity\")
  * Use \"depends_on\" (NOT \"dependencies\")
  * Use \"preferred_agent\" (NOT \"agent\")
  * Include \"depends_on\" even if empty (use [])
  * Include \"locks\" even if empty (use [])
- No extra keys allowed on task objects.
- No markdown. No code fences. No extra text before/after the JSON."
elif [[ "$ORCH_SCHEMA_KIND" == "json" ]]; then
  # Generic JSON mode: Pure JSON output for arbitrary schemas (NOT task_plan)
  # Used by merge resolver, generic JSON tools, etc.
  prompt="${prompt}

REMINDER (NON-NEGOTIABLE) - JSON OUTPUT MODE:
- Output EXACTLY ONE JSON object matching the schema in ${SCHEMA_BASENAME}.
- The schema defines the required structure - read it carefully.
- Output ONLY the JSON object. No markdown fences. No code blocks. No extra text.
- Do NOT include task_plan keys (milestone_id, max_parallel_tasks, rationale, tasks) unless the schema requires them.
- Ensure your JSON is valid and matches the schema exactly."
else
  # Execution mode: Tools ARE enabled - worker can and SHOULD use Read/Edit/Write/Bash
  prompt="${prompt}

REMINDER (NON-NEGOTIABLE) - EXECUTION MODE:
- Tools ARE ENABLED. You CAN and SHOULD use Read, Edit, Write, Bash tools to complete your task.
- Do NOT claim tools are disabled. They are fully available to you.
- Use tools to read files, edit code, run commands, and verify your changes.
- AFTER completing your work, output EXACTLY ONE JSON object matching ${SCHEMA_BASENAME}.
- The JSON must be your FINAL output after all tool usage is complete.
- No markdown fences around the JSON. No extra text before/after the JSON.
- Set work_completed=true if you successfully implemented the task.
- Set work_completed=false ONLY if you genuinely could not complete the task."
fi

# Claude Code expects --json-schema as an *inline JSON string* (not a file path).
SCHEMA_JSON="$(
  python3 -c 'import json,sys; print(json.dumps(json.load(open(sys.argv[1])), separators=(",",":")))' \
    "$SCHEMA_FILE"
)"

ERR_SCHEMA="${OUT_FILE}.stderr.schema"
ERR_PLAIN="${OUT_FILE}.stderr.plain"
WRAP_SCHEMA="${OUT_FILE}.wrapper_schema.json"
WRAP_PLAIN="${OUT_FILE}.wrapper_plain.json"
RAW_STREAM="${OUT_FILE}.wrapper_schema_claude_raw_stream.txt"

HELP_TEXT="$(python3 - <<'PY' "$CLAUDE_BIN" "$CLAUDE_HELP_TIMEOUT_S"
import os
import signal
import subprocess
import sys

bin_path = sys.argv[1]
timeout_s = int(sys.argv[2])

def run(cmd):
    out = ""
    proc = None
    try:
        proc = subprocess.Popen(
            cmd,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            start_new_session=True,
        )
        stdout, stderr = proc.communicate(timeout=timeout_s)
        out = stdout or stderr or ""
    except subprocess.TimeoutExpired:
        if proc is not None:
            try:
                os.killpg(proc.pid, signal.SIGKILL)
            except Exception:
                pass
        out = ""
    except Exception:
        out = ""
    return out

outputs = []
for cmd in (
    [bin_path, "--help"],
    [bin_path, "-h"],
    [bin_path, "--version"],
):
    out = run(cmd)
    if out:
        outputs.append(out)

print("\n".join(outputs))
PY
)"

supports_flag() {
  local flag="$1"
  [[ "$HELP_TEXT" == *"$flag"* ]]
}

PROMPT_FLAG="-p"
if supports_flag "--prompt"; then
  PROMPT_FLAG="--prompt"
fi

COMMON_ARGS=("$PROMPT_FLAG" "$prompt")
if [[ -n "$MODEL" ]] && supports_flag "--model"; then
  COMMON_ARGS+=(--model "$MODEL")
fi
if supports_flag "--no-session-persistence"; then
  COMMON_ARGS+=(--no-session-persistence)
fi
if supports_flag "--permission-mode"; then
  COMMON_ARGS+=(--permission-mode dontAsk)
fi
# Prefer newer/stronger flags that skip permission prompts entirely (when supported).
if supports_flag "--dangerously-skip-permissions"; then
  COMMON_ARGS+=(--dangerously-skip-permissions)
elif supports_flag "--skip-permissions"; then
  COMMON_ARGS+=(--skip-permissions)
elif supports_flag "--skip-permission-prompts"; then
  COMMON_ARGS+=(--skip-permission-prompts)
fi
if [[ -n "$CLAUDE_TOOLS" ]] && supports_flag "--tools"; then
  COMMON_ARGS+=(--tools "$CLAUDE_TOOLS")
fi

JSON_ARGS=()
if [[ -n "$CLAUDE_ARGS_JSON_MODE" ]]; then
  read -r -a JSON_ARGS <<< "$CLAUDE_ARGS_JSON_MODE"
else
  if supports_flag "--output-format"; then
    JSON_ARGS+=(--output-format json)
  fi
  if supports_flag "--json-schema"; then
    JSON_ARGS+=(--json-schema "$SCHEMA_JSON")
  elif supports_flag "--json"; then
    JSON_ARGS+=(--json)
  fi
fi

PLAIN_ARGS=()
if supports_flag "--output-format"; then
  PLAIN_ARGS+=(--output-format json)
elif supports_flag "--json"; then
  PLAIN_ARGS+=(--json)
fi

run_claude() {
  # args: <mode:json|plain> <wrap_path> <err_path>
  local mode="$1"
  local wrap_path="$2"
  local err_path="$3"
  local -a mode_args=()

  if [[ "$mode" == "json" ]]; then
    mode_args=("${JSON_ARGS[@]}")
  else
    mode_args=("${PLAIN_ARGS[@]}")
  fi

  local -a cmd=("$CLAUDE_BIN" "${COMMON_ARGS[@]}" "${mode_args[@]}")
  local rc=0

  python3 - <<'PY' "$TIMEOUT_S" "$wrap_path" "$err_path" "${wrap_path}.meta.json" "${cmd[@]}"
import json
import os
import signal
import subprocess
import sys
import threading

timeout_s = int(sys.argv[1])
wrap_path = sys.argv[2]
err_path = sys.argv[3]
meta_path = sys.argv[4]
cmd = sys.argv[5:]

out = ""
err = ""
rc = 0

stdout_lines = []
stderr_lines = []

try:
    proc = subprocess.Popen(
        cmd,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        start_new_session=True,
        bufsize=1,
    )
except Exception as exc:
    rc = 127
    err = f"Failed to start Claude CLI: {exc}\n"
    with open(wrap_path, "w", encoding="utf-8") as handle:
        handle.write("")
    with open(err_path, "a", encoding="utf-8") as handle:
        handle.write(err)
    with open(meta_path, "w", encoding="utf-8") as handle:
        json.dump({"cmd": cmd, "rc": rc, "error": str(exc)}, handle)
    sys.exit(0)

def reader(stream, sink, dest):
    for line in iter(stream.readline, ""):
        sink.append(line)
        dest.write(line)
        dest.flush()
    stream.close()

t_out = threading.Thread(target=reader, args=(proc.stdout, stdout_lines, sys.stdout), daemon=True)
t_err = threading.Thread(target=reader, args=(proc.stderr, stderr_lines, sys.stderr), daemon=True)
t_out.start()
t_err.start()

try:
    proc.wait(timeout=timeout_s)
    rc = proc.returncode
except subprocess.TimeoutExpired:
    try:
        os.killpg(proc.pid, signal.SIGKILL)
    except Exception:
        pass
    rc = 124
    stderr_lines.append(f"\\nTIMEOUT after {timeout_s}s\\n")

t_out.join()
t_err.join()
out = "".join(stdout_lines)
err = "".join(stderr_lines)

with open(wrap_path, "w", encoding="utf-8") as handle:
    handle.write(out)

with open(err_path, "a", encoding="utf-8") as handle:
    handle.write(err)

with open(meta_path, "w", encoding="utf-8") as handle:
    json.dump({"cmd": cmd, "rc": rc}, handle)
PY
}

: > "$ERR_SCHEMA"
: > "$ERR_PLAIN"
: > "$WRAP_SCHEMA"
: > "$WRAP_PLAIN"

# Try schema-enforced first (best chance of correct structured output).
run_claude "json" "$WRAP_SCHEMA" "$ERR_SCHEMA"

# If that produced an error wrapper or no usable result, we'll fall back to plain.
# Normalization logic below will choose the better wrapper automatically.
run_claude "plain" "$WRAP_PLAIN" "$ERR_PLAIN"

# Save combined raw stream for debugging
cat "$WRAP_SCHEMA" "$WRAP_PLAIN" > "$RAW_STREAM" 2>/dev/null || true

# ============================================================================
# Schema-kind dispatch: task_plan vs json vs turn
# ============================================================================

if [[ "$ORCH_SCHEMA_KIND" == "task_plan" || "$ORCH_SCHEMA_KIND" == "json" ]]; then
  # TASK_PLAN / JSON MODE: No turn normalization, just extract valid JSON matching schema
  # Both modes want pure JSON output; "json" is for arbitrary schemas (not task_plan specific)
  # Use the extract_json_by_schema.py helper for robust extraction

  SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
  EXTRACT_SCRIPT="$SCRIPT_DIR/../../scripts/extract_json_by_schema.py"

  if [[ -f "$EXTRACT_SCRIPT" ]]; then
    if python3 "$EXTRACT_SCRIPT" "$RAW_STREAM" "$SCHEMA_FILE" "$OUT_FILE"; then
      cat "$OUT_FILE"
      exit 0
    else
      echo "ERROR: extract_json_by_schema.py failed to find valid $ORCH_SCHEMA_KIND JSON" >&2
      echo "  raw_stream: $RAW_STREAM" >&2
      echo "  schema: $SCHEMA_FILE" >&2
      exit 1
    fi
  else
    # Fallback: inline extraction for task_plan schema
    python3 - <<'PY' "$WRAP_SCHEMA" "$WRAP_PLAIN" "$SCHEMA_FILE" "$OUT_FILE"
import json
import sys
from pathlib import Path

wrap_schema = sys.argv[1]
wrap_plain = sys.argv[2]
schema_path = sys.argv[3]
out_path = sys.argv[4]

try:
    import jsonschema
    HAS_JSONSCHEMA = True
except ImportError:
    HAS_JSONSCHEMA = False

def validate(obj, schema):
    if not HAS_JSONSCHEMA:
        # Basic check: must have required keys
        required = schema.get("required", [])
        return isinstance(obj, dict) and all(k in obj for k in required)
    try:
        jsonschema.validate(instance=obj, schema=schema)
        return True
    except jsonschema.ValidationError:
        return False

def strip_fences(s):
    s = s.strip()
    if s.startswith("```"):
        lines = s.splitlines()
        if lines and lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        return "\n".join(lines).strip()
    return s

def extract_from_stream(text, schema):
    candidates = []
    decoder = json.JSONDecoder()
    idx = 0
    n = len(text)

    parsed = []
    while idx < n:
        while idx < n and text[idx] in " \t\n\r":
            idx += 1
        if idx >= n:
            break
        try:
            obj, end = decoder.raw_decode(text, idx)
            parsed.append(obj)
            idx = end
        except json.JSONDecodeError:
            idx += 1

    # Flatten arrays
    flattened = []
    for obj in parsed:
        if isinstance(obj, list):
            flattened.extend(obj)
        else:
            flattened.append(obj)

    for obj in flattened:
        if not isinstance(obj, dict):
            continue

        # Direct match
        if validate(obj, schema):
            candidates.append(obj)
            continue

        # Check for embedded result
        if obj.get("type") == "result":
            result_str = obj.get("result")
            if isinstance(result_str, str):
                try:
                    inner = json.loads(strip_fences(result_str))
                    if isinstance(inner, dict) and validate(inner, schema):
                        candidates.append(inner)
                except:
                    pass

        # Check assistant message content
        if obj.get("type") == "assistant":
            message = obj.get("message", {})
            content = message.get("content", [])
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    text_content = block.get("text", "")
                    try:
                        inner = json.loads(strip_fences(text_content))
                        if isinstance(inner, dict) and validate(inner, schema):
                            candidates.append(inner)
                    except:
                        pass

    return candidates[-1] if candidates else None

# Load schema
schema = json.loads(Path(schema_path).read_text(encoding="utf-8"))

# Try both wrappers
result = None
for wrapper_path in [wrap_schema, wrap_plain]:
    try:
        text = Path(wrapper_path).read_text(encoding="utf-8")
        result = extract_from_stream(text, schema)
        if result:
            break
    except Exception:
        pass

if result is None:
    sys.stderr.write("ERROR: Could not extract valid task_plan JSON from Claude output\n")
    sys.exit(1)

Path(out_path).write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
print(json.dumps(result, ensure_ascii=False, separators=(",", ":")))
PY

    rc=$?
    if [[ $rc -ne 0 ]]; then
      echo "ERROR: Task plan extraction failed" >&2
      exit 1
    fi
    exit 0
  fi
fi

# ============================================================================
# TURN MODE: Full turn normalization (original behavior)
# ============================================================================

python3 - <<'PY' "$WRAP_SCHEMA" "$WRAP_PLAIN" "$SCHEMA_FILE" "$PROMPT_FILE" "$ERR_SCHEMA" "$ERR_PLAIN" > "$OUT_FILE" || true
import json, os, re, sys

wrap_schema, wrap_plain, schema_path, prompt_path, err_schema, err_plain = (
    sys.argv[1],
    sys.argv[2],
    sys.argv[3],
    sys.argv[4],
    sys.argv[5],
    sys.argv[6],
)
schema = json.loads(open(schema_path, "r", encoding="utf-8").read())
prompt_text = open(prompt_path, "r", encoding="utf-8").read()
api_vars = [name for name in ("ANTHROPIC_API_KEY", "CLAUDE_API_KEY") if os.environ.get(name)]
auth_mode = "api_key" if api_vars else "subscription"
auth_warning = ""
if api_vars:
    auth_warning = (
        "WARNING: "
        + ", ".join(api_vars)
        + " set; Claude Code will use API billing instead of Pro/Max subscription."
    )

REQUIRED_KEYS = [
    "agent",
    "milestone_id",
    "phase",
    "work_completed",
    "project_complete",
    "summary",
    "gates_passed",
    "requirement_progress",
    "next_agent",
    "next_prompt",
    "delegate_rationale",
    "stats_refs",
    "needs_write_access",
    "artifacts",
]

def jload(s: str):
    try:
        return json.loads(s)
    except Exception:
        return None

def read_json(path: str):
    try:
        raw = open(path, "r", encoding="utf-8").read().strip()
    except Exception:
        return None
    return jload(raw) if raw else None


def parse_json_sequence(path: str) -> list:
    """Parse a file containing one or more JSON values (sequence/stream).

    Handles:
    - Multiple concatenated JSON values (with or without whitespace)
    - Pretty-printed multi-line JSON arrays/objects
    - JSON Lines (one per line)
    - Single minified JSON array

    Uses json.JSONDecoder().raw_decode() to robustly parse any valid JSON sequence.
    """
    try:
        raw = open(path, "r", encoding="utf-8").read()
    except Exception:
        return []

    objects = []
    decoder = json.JSONDecoder()
    idx = 0
    n = len(raw)

    while idx < n:
        # Skip whitespace
        while idx < n and raw[idx] in " \t\n\r":
            idx += 1
        if idx >= n:
            break

        try:
            obj, end = decoder.raw_decode(raw, idx)
            # Flatten top-level arrays (Claude outputs arrays of events)
            if isinstance(obj, list):
                objects.extend(obj)
            else:
                objects.append(obj)
            idx = end
        except json.JSONDecodeError:
            # Skip one character and try again (handles trailing garbage)
            idx += 1

    return objects


def extract_turn_from_claude_jsonlines(path: str):
    """
    Extract the turn JSON from Claude CLI JSON stream output.

    Claude CLI outputs JSON events that can be:
    - Multiple concatenated JSON values
    - Pretty-printed multi-line JSON arrays/objects
    - JSON Lines (one per line)

    Priority order for extraction:
    1. Last event with type=="result" AND has a string field "result" → parse that string
    2. Else last event with type=="assistant" and message.content[*].type=="text" → concatenate text
    3. Else fail
    """
    objects = parse_json_sequence(path)

    if not objects:
        return None

    # Priority 1: Look for type=="result" events with a "result" string field
    result_events = [
        obj for obj in objects
        if isinstance(obj, dict) and obj.get("type") == "result"
    ]

    for evt in reversed(result_events):
        result_str = evt.get("result")
        if isinstance(result_str, str) and result_str.strip():
            turn = parse_turn_from_text(result_str)
            if turn is not None:
                return turn

    # Priority 2: Look for type=="assistant" events with message.content[*].text
    assistant_messages = [
        obj for obj in objects
        if isinstance(obj, dict) and obj.get("type") == "assistant" and "message" in obj
    ]

    if not assistant_messages:
        return None

    last_msg = assistant_messages[-1]
    message = last_msg.get("message", {})
    content = message.get("content", [])

    # Extract text from content blocks
    text_content = ""
    for block in content:
        if isinstance(block, dict) and block.get("type") == "text":
            text_content += block.get("text", "")

    if not text_content:
        return None

    # Try to parse the text as a turn JSON
    return parse_turn_from_text(text_content)

def read_text(path: str) -> str:
    try:
        return open(path, "r", encoding="utf-8").read()
    except Exception:
        return ""

def read_meta(path: str):
    meta_path = path + ".meta.json"
    return read_json(meta_path) or {}

def strip_fences(s: str) -> str:
    s = s.strip()
    if s.startswith("```"):
        lines = s.splitlines()
        if lines and lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        return "\n".join(lines).strip()
    return s

def extract_balanced_object(s: str) -> str | None:
    start = s.find("{")
    if start == -1:
        return None
    depth = 0
    in_str = False
    esc = False
    for i in range(start, len(s)):
        ch = s[i]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
            continue
        else:
            if ch == '"':
                in_str = True
                continue
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return s[start : i + 1]
    return None

def extract_stats_ids(text: str) -> list[str]:
    return sorted(set(re.findall(r"\b(?:CX|CL)-\d+\b", text)))

def choose_stats_ref(ids: list[str]) -> list[str]:
    if not ids:
        return ["CL-1"]
    cl = [x for x in ids if x.startswith("CL-")]
    return [cl[0] if cl else ids[0]]

def extract_milestone_id(text: str) -> str:
    m = re.search(r'"milestone_id"\s*:\s*"([^"]+)"', text)
    if m:
        return m.group(1)
    m2 = re.search(r"\*\*Milestone:\*\*\s*(M\d+)\b", text)
    return m2.group(1) if m2 else "M0"

def redact_args(cmd: list[str]) -> list[str]:
    redacted = []
    patterns = [
        re.compile(r"sk-[A-Za-z0-9]{20,}"),
        re.compile(r"sk-ant-[A-Za-z0-9\\-]{10,}"),
        re.compile(r"AIza[0-9A-Za-z_\\-]{20,}"),
    ]
    for arg in cmd:
        clean = arg
        for pat in patterns:
            if pat.search(clean):
                clean = pat.sub("***", clean)
        redacted.append(clean)
    return redacted

def diagnostic_block(label: str, meta: dict, stderr_text: str) -> str:
    cmd = meta.get("cmd", [])
    if isinstance(cmd, list):
        cmd_text = " ".join(redact_args([str(c) for c in cmd]))
    else:
        cmd_text = "(unknown)"
    rc = meta.get("rc", "unknown")
    stderr_snip = (stderr_text or "")[:4096]
    return f"{label}: cmd={cmd_text}\\nrc={rc}\\nstderr={stderr_snip}"

def to_bool(x, default=False):
    return x if isinstance(x, bool) else default

def to_str(x, default=""):
    return x if isinstance(x, str) else default

def to_str_list(x):
    if not isinstance(x, list):
        return []
    return [i.strip() for i in x if isinstance(i, str) and i.strip()]

def normalize_requirement_progress(x):
    rp = x if isinstance(x, dict) else {}
    return {
        "covered_req_ids": to_str_list(rp.get("covered_req_ids", [])),
        "tests_added_or_modified": to_str_list(rp.get("tests_added_or_modified", [])),
        "commands_run": to_str_list(rp.get("commands_run", [])),
    }

def normalize_artifacts(x):
    if not isinstance(x, list):
        return []
    out = []
    for a in x:
        if not isinstance(a, dict):
            continue
        p = a.get("path")
        d = a.get("description")
        if isinstance(p, str) and p.strip() and isinstance(d, str) and d.strip():
            out.append({"path": p.strip(), "description": d.strip()})
    return out

def append_wrapper_meta(summary: str, status: str) -> str:
    parts = []
    base = summary.strip()
    if base:
        parts.append(base)
    if auth_warning:
        parts.append(auth_warning)
    parts.append(f"wrapper_status={status} auth_mode={auth_mode}")
    return "\\n".join(parts)

def wrapper_to_model_text(wrapper: dict) -> tuple[bool, str]:
    # Claude Code wrapper format commonly: {"type":"result", "is_error":..., "result":"..."}
    is_error = bool(wrapper.get("is_error", False)) if isinstance(wrapper, dict) else True
    text = ""
    if isinstance(wrapper, dict):
        for k in ("result", "response", "content", "output", "text", "message"):
            v = wrapper.get(k)
            if isinstance(v, str) and v.strip():
                text = v.strip()
                break
    return is_error, text

def parse_turn_from_text(model_text: str):
    model_text = strip_fences(model_text)
    turn = jload(model_text)
    if isinstance(turn, dict):
        return turn
    obj_txt = extract_balanced_object(model_text)
    if obj_txt:
        t2 = jload(obj_txt)
        if isinstance(t2, dict):
            return t2
    return None

# Try to extract turn from Claude CLI JSON Lines output.
# Claude CLI outputs JSON Lines with init events and assistant messages.
# The turn JSON is in message.content[0].text of the last assistant message.

best_turn = None
best_model_text = ""

# Try schema wrapper first (more likely to have structured output)
for wrapper_path in [wrap_schema, wrap_plain]:
    turn = extract_turn_from_claude_jsonlines(wrapper_path)
    if turn is not None:
        best_turn = turn
        # Read raw content for diagnostics
        try:
            best_model_text = open(wrapper_path, "r", encoding="utf-8").read()[:8000]
        except Exception:
            best_model_text = ""
        break

# Fallback: try legacy wrapper format (single JSON object with result/response keys)
if best_turn is None:
    for wrapper_path in [wrap_schema, wrap_plain]:
        ws = read_json(wrapper_path)
        if isinstance(ws, dict):
            is_error, mt = wrapper_to_model_text(ws)
            if mt:
                t = parse_turn_from_text(mt)
                if t is not None:
                    best_turn = t
                    best_model_text = mt
                    break

stats_ids = extract_stats_ids(prompt_text)
milestone_id = extract_milestone_id(prompt_text)

def synthesize(reason: str, model_text: str) -> dict:
    meta_schema = read_meta(wrap_schema)
    meta_plain = read_meta(wrap_plain)
    diag = "\\n\\n".join(
        [
            diagnostic_block("schema_attempt", meta_schema, read_text(err_schema)),
            diagnostic_block("plain_attempt", meta_plain, read_text(err_plain)),
        ]
    )
    summary = append_wrapper_meta(f"{reason} Diagnostics:\\n{diag}", "error")
    return {
        "agent": "claude",
        "milestone_id": milestone_id,
        "phase": "plan",
        "work_completed": False,
        "project_complete": False,
        "summary": summary,
        "gates_passed": [],
        "requirement_progress": {"covered_req_ids": [], "tests_added_or_modified": [], "commands_run": []},
        "next_agent": "codex",
        "next_prompt": "Use tools.verify output; fix remaining gates (ruff/git_guard/etc.), then commit + re-run verify.",
        "delegate_rationale": (strip_fences(model_text)[:4000] if model_text else "(empty model output)"),
        "stats_refs": choose_stats_ref(stats_ids),
        "needs_write_access": True,
        "artifacts": [],
    }

if best_turn is None:
    # Save raw output to debug file for diagnostics
    import pathlib
    out_path = pathlib.Path(sys.argv[1])  # wrap_schema path
    debug_path = out_path.parent / f"{out_path.stem}_claude_raw_stream.txt"
    raw_content = ""
    try:
        for p in [wrap_schema, wrap_plain]:
            try:
                raw_content += f"\\n=== {p} ===\\n"
                raw_content += open(p, "r", encoding="utf-8").read()
            except Exception:
                raw_content += "(could not read)\\n"
        debug_path.write_text(raw_content, encoding="utf-8")
    except Exception:
        pass

    # Write error to stderr, but still emit a schema-valid error turn.
    reason = "Claude did not emit a parseable JSON turn."
    diag = f"Raw output saved to: {debug_path}\\n"
    diag += f"wrap_schema: {wrap_schema}\\n"
    diag += f"wrap_plain: {wrap_plain}\\n"
    sys.stderr.write(f"ERROR: {reason}\\n{diag}\\n")

    error_turn = synthesize(f"{reason} Raw output saved to: {debug_path}", raw_content)
    print(json.dumps(error_turn, ensure_ascii=False, separators=(",", ":")))
    raise SystemExit(0)

# Normalize parsed turn to strict schema
t = best_turn
out = {}
out["agent"] = "claude"
out["milestone_id"] = milestone_id

phase = t.get("phase")
out["phase"] = phase if phase in ("plan", "implement", "verify", "finalize") else "plan"

out["work_completed"] = to_bool(t.get("work_completed"), False)
out["project_complete"] = to_bool(t.get("project_complete"), False)

out["summary"] = append_wrapper_meta(to_str(t.get("summary"), ""), "ok")
out["gates_passed"] = to_str_list(t.get("gates_passed", []))
out["requirement_progress"] = normalize_requirement_progress(t.get("requirement_progress"))

na = t.get("next_agent")
out["next_agent"] = na if na in ("codex", "claude") else "codex"
out["next_prompt"] = to_str(t.get("next_prompt"), "")
out["delegate_rationale"] = to_str(t.get("delegate_rationale"), "")

refs = to_str_list(t.get("stats_refs", []))
refs = [r for r in refs if r in stats_ids]
out["stats_refs"] = refs if refs else choose_stats_ref(stats_ids)

out["artifacts"] = normalize_artifacts(t.get("artifacts"))

# IMPORTANT: keep write access on if delegating to a coding agent.
if out["next_agent"] in ("codex", "claude"):
    out["needs_write_access"] = True
else:
    out["needs_write_access"] = to_bool(t.get("needs_write_access"), True)

# Emit only required keys
out = {k: out.get(k) for k in REQUIRED_KEYS}

# Validate against schema
try:
    import jsonschema
    jsonschema.validate(instance=out, schema=schema)
except ImportError:
    # jsonschema not available; skip validation (tests will catch schema issues)
    pass
except jsonschema.ValidationError as e:
    import pathlib
    out_path = pathlib.Path(sys.argv[1])
    debug_path = out_path.parent / f"{out_path.stem}_claude_raw_stream.txt"
    raw_content = ""
    try:
        for p in [wrap_schema, wrap_plain]:
            try:
                raw_content += f"\\n=== {p} ===\\n"
                raw_content += open(p, "r", encoding="utf-8").read()
            except Exception:
                raw_content += "(could not read)\\n"
        debug_path.write_text(raw_content, encoding="utf-8")
    except Exception:
        pass

    sys.stderr.write(f"ERROR: Normalized turn failed schema validation: {e.message}\\n")
    sys.stderr.write(f"Raw output saved to: {debug_path}\\n")
    fallback_text = best_model_text if best_model_text else raw_content
    error_turn = synthesize(
        f"Normalized turn failed schema validation: {e.message}",
        fallback_text,
    )
    print(json.dumps(error_turn, ensure_ascii=False, separators=(",", ":")))
    raise SystemExit(0)

print(json.dumps(out, ensure_ascii=False, separators=(",", ":")))
PY

# If normalization failed for some unexpected reason, exit with error instead of synthesizing.
if [[ ! -s "$OUT_FILE" ]]; then
  echo "ERROR: Claude wrapper produced empty output. Raw files:" >&2
  echo "  schema: $WRAP_SCHEMA" >&2
  echo "  plain: $WRAP_PLAIN" >&2
  echo "  stderr_schema: $ERR_SCHEMA" >&2
  echo "  stderr_plain: $ERR_PLAIN" >&2
  exit 1
fi

cat "$OUT_FILE"
exit 0

================================================================================
 FILE: agents\codex.sh
================================================================================

#!/usr/bin/env bash
set -euo pipefail

PROMPT_FILE="${1:?prompt_file}"
SCHEMA_FILE="${2:?schema_file}"
OUT_FILE="${3:?out_file}"

MODEL="${CODEX_MODEL:-gpt-5.2-codex}"
REASONING_EFFORT="${CODEX_REASONING_EFFORT:-xhigh}"
WRITE_ACCESS="${WRITE_ACCESS:-0}"
FF_SMOKE="${FF_SMOKE:-0}"
SMOKE_DIR="${FF_AGENT_SMOKE_DIR:-}"

# Different Codex CLI versions use either `-c key=value` or a dedicated flag.
# Official docs call out `-c` for per-invocation overrides.
CODEX_CONFIG_FLAG="${CODEX_CONFIG_FLAG:--c}"

# Optional knobs (override in env) to match your local Codex CLI version.
# Examples:
#   export CODEX_SANDBOX=workspace-write
#   export CODEX_ASK_FOR_APPROVAL=never
CODEX_SANDBOX="${CODEX_SANDBOX:-}"
CODEX_ASK_FOR_APPROVAL="${CODEX_ASK_FOR_APPROVAL:-}"
CODEX_EXTRA_GLOBAL_FLAGS="${CODEX_EXTRA_GLOBAL_FLAGS:-}"
CODEX_EXTRA_EXEC_FLAGS="${CODEX_EXTRA_EXEC_FLAGS:-}"
DEFAULT_TIMEOUT_S=86400
SMOKE_TIMEOUT_S=180
if [[ "$FF_SMOKE" == "1" ]]; then
  CODEX_TIMEOUT_S="${CODEX_TIMEOUT_S:-$SMOKE_TIMEOUT_S}"
else
  CODEX_TIMEOUT_S="${CODEX_TIMEOUT_S:-$DEFAULT_TIMEOUT_S}"
fi

cmd=(codex)

if [[ -n "$SMOKE_DIR" && "$WRITE_ACCESS" == "1" ]]; then
  mkdir -p "$SMOKE_DIR"
  printf '%s %s\n' "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" "codex" > "$SMOKE_DIR/codex.txt"
fi

# These global flags are supported by many Codex CLI versions.
if [[ -n "$MODEL" ]]; then
  cmd+=(--model "$MODEL")
fi
if [[ -n "$REASONING_EFFORT" ]]; then
  cmd+=($CODEX_CONFIG_FLAG "reasoning.effort=$REASONING_EFFORT")
fi
if [[ -n "$CODEX_SANDBOX" ]]; then
  cmd+=(--sandbox "$CODEX_SANDBOX")
fi
if [[ -n "$CODEX_ASK_FOR_APPROVAL" ]]; then
  cmd+=(--ask-for-approval "$CODEX_ASK_FOR_APPROVAL")
fi
if [[ -n "$CODEX_EXTRA_GLOBAL_FLAGS" ]]; then
  # shellcheck disable=SC2206
  cmd+=($CODEX_EXTRA_GLOBAL_FLAGS)
fi

cmd+=(exec)

# On newer Codex CLIs, this enables file edits + commands.
if [[ "$WRITE_ACCESS" == "1" ]]; then
  cmd+=(--full-auto)
fi

if [[ -n "$CODEX_EXTRA_EXEC_FLAGS" ]]; then
  # shellcheck disable=SC2206
  cmd+=($CODEX_EXTRA_EXEC_FLAGS)
fi

cmd+=(--output-schema "$SCHEMA_FILE" -o "$OUT_FILE" -)

python3 - <<'PY' "$CODEX_TIMEOUT_S" "$PROMPT_FILE" "${cmd[@]}"
import os
import signal
import subprocess
import sys

timeout_s = int(sys.argv[1])
prompt_path = sys.argv[2]
cmd = sys.argv[3:]

prompt = open(prompt_path, "r", encoding="utf-8").read()

try:
    proc = subprocess.Popen(
        cmd,
        text=True,
        stdin=subprocess.PIPE,
        stdout=sys.stdout,
        stderr=sys.stderr,
        start_new_session=True,
    )
    proc.communicate(input=prompt, timeout=timeout_s)
    raise SystemExit(proc.returncode)
except subprocess.TimeoutExpired:
    try:
        os.killpg(proc.pid, signal.SIGKILL)
    except Exception:
        pass
    sys.stderr.write(f"CODEX_TIMEOUT after {timeout_s}s\\n")
    raise SystemExit(124)
PY

cat "$OUT_FILE"

================================================================================
 FILE: loop_pkg\__init__.py
================================================================================

"""Loop package: Orchestration loop components.

This package contains the refactored orchestration loop components
for improved readability and maintainability within tooling constraints.

The main entrypoint remains bridge/loop.py which imports from this package.
"""

from bridge.loop_pkg.config import AGENTS, ParallelSettings, RunConfig, RunState
from bridge.loop_pkg.policy import AgentPolicy, AgentPolicyViolation, get_agent_policy, set_agent_policy
from bridge.loop_pkg.turn_normalizer import (
    NormalizationResult,
    TurnNormalizer,
    normalize_agent_output,
    validate_turn_lenient,
)

__all__ = [
    "AGENTS",
    "AgentPolicy",
    "AgentPolicyViolation",
    "ParallelSettings",
    "RunConfig",
    "RunState",
    "get_agent_policy",
    "set_agent_policy",
    # Turn normalization
    "NormalizationResult",
    "TurnNormalizer",
    "normalize_agent_output",
    "validate_turn_lenient",
]

================================================================================
 FILE: loop_pkg\config.py
================================================================================

"""Configuration and state classes for the orchestration loop."""

from __future__ import annotations

import dataclasses
from pathlib import Path
from typing import Any

AGENTS: tuple[str, ...] = ("codex", "claude")


@dataclasses.dataclass(frozen=True)
class ParallelSettings:
    """Settings for parallel worker execution."""

    max_workers_default: int = 8
    cpu_intensive_threshold_pct: float = 40.0
    mem_intensive_threshold_pct: float = 40.0
    sample_interval_s: float = 1.0
    consecutive_samples: int = 3
    kill_grace_s: float = 8.0
    terminal_max_bytes_per_worker: int = 40000
    terminal_max_line_length: int = 600
    disable_gpu_by_default: bool = True


@dataclasses.dataclass(frozen=True)
class AgentCapabilities:
    """Capabilities for an agent."""

    supports_tools: bool = True
    supports_fs_read: bool = True
    supports_fs_write: bool = True
    supports_bash: bool = True
    supports_write_access: bool = True


@dataclasses.dataclass(frozen=True)
class RunConfig:
    """Configuration for an orchestration run."""

    max_calls_per_agent: int
    quota_retry_attempts: int
    max_total_calls: int
    max_json_correction_attempts: int
    fallback_order: list[str]
    enable_agents: list[str]
    smoke_route: tuple[str, ...]

    agent_scripts: dict[str, str]
    agent_models: dict[str, str]
    quota_error_patterns: dict[str, list[str]]
    supports_write_access: dict[str, bool]
    agent_capabilities: dict[str, AgentCapabilities]  # New: full capabilities per agent
    parallel: ParallelSettings


@dataclasses.dataclass
class RunState:
    """Mutable state for a single orchestration run."""

    run_id: str
    project_root: Path
    runs_dir: Path
    schema_path: Path
    system_prompt_path: Path
    design_doc_path: Path
    smoke_route: tuple[str, ...] = tuple()

    total_calls: int = 0
    call_counts: dict[str, int] = dataclasses.field(default_factory=lambda: {a: 0 for a in AGENTS})
    quota_failures: dict[str, int] = dataclasses.field(default_factory=lambda: {a: 0 for a in AGENTS})
    disabled_by_quota: dict[str, bool] = dataclasses.field(default_factory=lambda: {a: False for a in AGENTS})
    history: list[dict[str, Any]] = dataclasses.field(default_factory=list)

    # Dynamic write access policy (set by previous turn)
    grant_write_access: bool = False

================================================================================
 FILE: loop_pkg\policy.py
================================================================================

"""Agent policy enforcement for the orchestration loop."""

from __future__ import annotations

import dataclasses
import datetime as dt
from pathlib import Path

from bridge.loop_pkg.config import AGENTS


class AgentPolicyViolation(Exception):
    """Raised when code attempts to use an agent that violates the policy."""

    pass


@dataclasses.dataclass
class AgentPolicy:
    """Centralized agent selection policy.

    When forced_agent is set (via --only-codex or --only-claude), ALL agent
    selections must go through this policy and will be overridden to use
    only the forced agent.
    """

    forced_agent: str | None = None  # Set by --only-* flags
    allowed_agents: tuple[str, ...] = AGENTS
    runs_dir: Path | None = None  # For writing violation artifacts

    def enforce(self, requested_agent: str, context: str = "") -> str:
        """Enforce the agent policy, returning the agent to use.

        Args:
            requested_agent: The agent that was requested
            context: Description of where this request originated (for error messages)

        Returns:
            The agent to actually use (forced_agent if set, otherwise requested)

        Raises:
            AgentPolicyViolation: If forced mode is active and code tries to use wrong agent
        """
        if self.forced_agent:
            if requested_agent != self.forced_agent and requested_agent in AGENTS:
                # Log the override
                print(f"[AgentPolicy] OVERRIDE: {requested_agent} -> {self.forced_agent} ({context})")
            return self.forced_agent

        # No forced agent - verify requested is allowed
        if requested_agent not in self.allowed_agents:
            if self.allowed_agents:
                return self.allowed_agents[0]
            return AGENTS[0]

        return requested_agent

    def enforce_strict(self, requested_agent: str, context: str = "") -> str:
        """Strict enforcement - raises exception if wrong agent is requested.

        Use this for code paths that should NEVER attempt to use the wrong agent
        (e.g., fallback logic that might try to switch agents).
        """
        if self.forced_agent and requested_agent != self.forced_agent:
            msg = (
                f"AGENT POLICY VIOLATION: Attempted to use '{requested_agent}' "
                f"when --only-{self.forced_agent} is active. Context: {context}"
            )
            self._write_violation_artifact(msg, requested_agent, context)
            raise AgentPolicyViolation(msg)
        return self.enforce(requested_agent, context)

    def _write_violation_artifact(self, msg: str, requested: str, context: str) -> None:
        """Write an artifact explaining the policy violation."""
        if not self.runs_dir:
            return
        artifact_path = self.runs_dir / "agent_policy_violation.txt"
        content = f"""AGENT POLICY VIOLATION
======================

Timestamp: {dt.datetime.utcnow().isoformat()}Z
Forced Agent: {self.forced_agent}
Requested Agent: {requested}
Context: {context}

Message:
{msg}

This file was created because code attempted to invoke an agent that
violates the --only-{self.forced_agent} flag. This indicates a bug in
the orchestrator's agent selection logic.
"""
        try:
            artifact_path.write_text(content, encoding="utf-8")
            print(f"[AgentPolicy] Violation artifact written to: {artifact_path}")
        except Exception as e:
            print(f"[AgentPolicy] Failed to write violation artifact: {e}")

    def is_forced_mode(self) -> bool:
        """Return True if a forced agent mode is active."""
        return self.forced_agent is not None

    def get_prompt_header(self) -> str:
        """Get a header to inject into prompts when in forced mode.

        This tells the agent it's the only one and must implement, not just review.
        """
        if not self.forced_agent:
            return ""

        return f"""## AGENT POLICY OVERRIDE

**IMPORTANT**: You are running in `--only-{self.forced_agent}` mode.

- You are the ONLY agent allowed in this session.
- You MUST implement all changes yourself. Do NOT suggest handing off to another agent.
- You MUST verify your own changes. Do NOT assume another agent will review.
- Set `next_agent` to `"{self.forced_agent}"` in your response (it will be enforced anyway).
- Focus on both implementation AND verification - you are responsible for the full cycle.

"""


# Global policy instance (set during main() based on CLI flags)
_agent_policy: AgentPolicy | None = None


def get_agent_policy() -> AgentPolicy:
    """Get the global agent policy. Returns a default policy if not set."""
    global _agent_policy
    if _agent_policy is None:
        _agent_policy = AgentPolicy()
    return _agent_policy


def set_agent_policy(policy: AgentPolicy) -> None:
    """Set the global agent policy."""
    global _agent_policy
    _agent_policy = policy

================================================================================
 FILE: loop_pkg\turn_normalizer.py
================================================================================

"""Turn normalization - Design-doc-agnostic output normalization.

This module handles normalizing messy agent output to the full Turn schema,
making the orchestrator robust to:
- Prose/markdown around JSON
- Missing or incorrect invariant fields (agent, milestone_id, stats_refs)
- Partial payloads with only required fields
"""

from __future__ import annotations

import dataclasses
import json
from typing import Any

AGENTS: tuple[str, ...] = ("codex", "claude")


@dataclasses.dataclass
class NormalizationResult:
    """Result of turn normalization."""

    success: bool
    turn: dict[str, Any] | None
    warnings: list[str]
    error: str | None = None


class TurnNormalizer:
    """Normalizes agent output to full Turn schema.

    This class extracts payload from messy agent output and fills invariant fields,
    making the orchestrator robust to:
    - Prose/markdown around JSON
    - Missing or incorrect invariant fields (agent, milestone_id, stats_refs)
    - Partial payloads with only required fields
    """

    PAYLOAD_REQUIRED = {"summary", "work_completed", "project_complete"}
    VALID_PHASES = ("plan", "implement", "verify", "finalize")
    VALID_AGENTS = ("codex", "claude")

    def __init__(
        self,
        expected_agent: str,
        expected_milestone_id: str,
        stats_id_set: set[str],
        default_phase: str = "implement",
    ):
        self.expected_agent = expected_agent
        self.expected_milestone_id = expected_milestone_id
        self.stats_id_set = stats_id_set
        self.default_phase = default_phase

    def normalize(self, raw_output: str) -> NormalizationResult:
        """Normalize raw agent output to full Turn.

        Returns NormalizationResult with:
        - success: True if normalization succeeded
        - turn: The normalized turn dict (if success)
        - warnings: List of auto-corrections made
        - error: Error message (if not success)
        """
        warnings: list[str] = []

        # Step 1: Extract JSON payload from raw output
        payload = self._extract_payload(raw_output)
        if payload is None:
            return NormalizationResult(
                success=False,
                turn=None,
                warnings=warnings,
                error="Cannot extract JSON payload from output",
            )

        # Step 2: Check required payload fields
        missing = self.PAYLOAD_REQUIRED - set(payload.keys())
        if missing:
            return NormalizationResult(
                success=False,
                turn=None,
                warnings=warnings,
                error=f"Missing required payload fields: {sorted(missing)}",
            )

        # Step 3: Build normalized turn with invariant overrides
        turn = self._build_normalized_turn(payload, warnings)

        return NormalizationResult(
            success=True,
            turn=turn,
            warnings=warnings,
            error=None,
        )

    def _extract_payload(self, raw: str) -> dict[str, Any] | None:
        """Extract JSON payload from raw output, tolerating prose/UI junk."""
        raw = raw.strip()
        if not raw:
            return None

        # Try direct JSON parse first
        try:
            obj = json.loads(raw)
            if isinstance(obj, dict):
                return obj
        except json.JSONDecodeError:
            pass

        # Strip markdown fences
        if raw.startswith("```"):
            lines = raw.split("\n")
            if lines[0].startswith("```"):
                lines = lines[1:]
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            raw = "\n".join(lines).strip()
            try:
                obj = json.loads(raw)
                if isinstance(obj, dict):
                    return obj
            except json.JSONDecodeError:
                pass

        # Extract first balanced JSON object
        start = raw.find("{")
        if start == -1:
            return None

        depth = 0
        in_str = False
        esc = False

        for i in range(start, len(raw)):
            ch = raw[i]
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
            else:
                if ch == '"':
                    in_str = True
                elif ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        candidate = raw[start : i + 1]
                        try:
                            obj = json.loads(candidate)
                            if isinstance(obj, dict):
                                return obj
                        except json.JSONDecodeError:
                            pass
                        break

        return None

    def _build_normalized_turn(self, payload: dict[str, Any], warnings: list[str]) -> dict[str, Any]:
        """Build normalized turn from payload, overriding invariants."""
        turn: dict[str, Any] = {}

        # INVARIANT: agent - always use expected, log if different
        payload_agent = payload.get("agent")
        if payload_agent and payload_agent != self.expected_agent:
            warnings.append(f"agent mismatch: payload={payload_agent}, expected={self.expected_agent} (auto-corrected)")
        turn["agent"] = self.expected_agent

        # INVARIANT: milestone_id - always use expected, log if different
        payload_milestone = payload.get("milestone_id")
        if payload_milestone and str(payload_milestone) != self.expected_milestone_id:
            warnings.append(
                f"milestone_id mismatch: payload={payload_milestone}, expected={self.expected_milestone_id} (auto-corrected)"
            )
        turn["milestone_id"] = self.expected_milestone_id

        # INVARIANT: stats_refs - derive from agent if missing/invalid
        payload_stats = payload.get("stats_refs", [])
        if isinstance(payload_stats, list):
            valid_stats = [s for s in payload_stats if isinstance(s, str) and s in self.stats_id_set]
        else:
            valid_stats = []
        if not valid_stats:
            default_stat = "CL-1" if self.expected_agent == "claude" else "CX-1"
            if default_stat in self.stats_id_set:
                valid_stats = [default_stat]
            elif self.stats_id_set:
                valid_stats = [sorted(self.stats_id_set)[0]]
            else:
                valid_stats = ["CL-1"]  # Fallback
            warnings.append(f"stats_refs defaulted to {valid_stats}")
        turn["stats_refs"] = valid_stats

        # phase - use payload if valid, else default
        payload_phase = payload.get("phase")
        if payload_phase in self.VALID_PHASES:
            turn["phase"] = payload_phase
        else:
            turn["phase"] = self.default_phase
            if payload_phase:
                warnings.append(f"invalid phase '{payload_phase}', using '{self.default_phase}'")

        # Required payload fields (already validated)
        turn["summary"] = str(payload.get("summary", ""))
        turn["work_completed"] = bool(payload.get("work_completed", False))

        # Optional fields with defaults
        turn["project_complete"] = bool(payload.get("project_complete", False))
        turn["gates_passed"] = self._to_str_list(payload.get("gates_passed", []))

        # requirement_progress
        rp = payload.get("requirement_progress", {})
        if not isinstance(rp, dict):
            rp = {}
        turn["requirement_progress"] = {
            "covered_req_ids": self._to_str_list(rp.get("covered_req_ids", [])),
            "tests_added_or_modified": self._to_str_list(rp.get("tests_added_or_modified", [])),
            "commands_run": self._to_str_list(rp.get("commands_run", [])),
        }

        # next_agent - use payload if valid, else same agent
        payload_next = payload.get("next_agent")
        if payload_next in self.VALID_AGENTS:
            turn["next_agent"] = payload_next
        else:
            turn["next_agent"] = self.expected_agent

        turn["next_prompt"] = str(payload.get("next_prompt", ""))
        turn["delegate_rationale"] = str(payload.get("delegate_rationale", ""))

        # needs_write_access - default True for coding agents
        nwa = payload.get("needs_write_access")
        if isinstance(nwa, bool):
            turn["needs_write_access"] = nwa
        else:
            turn["needs_write_access"] = True

        # artifacts
        turn["artifacts"] = self._normalize_artifacts(payload.get("artifacts", []))

        return turn

    def _to_str_list(self, val: Any) -> list[str]:
        """Convert to list of strings."""
        if not isinstance(val, list):
            return []
        return [str(x).strip() for x in val if isinstance(x, str) and x.strip()]

    def _normalize_artifacts(self, val: Any) -> list[dict[str, str]]:
        """Normalize artifacts list."""
        if not isinstance(val, list):
            return []
        result = []
        for item in val:
            if isinstance(item, dict):
                path = item.get("path")
                desc = item.get("description")
                if isinstance(path, str) and path.strip() and isinstance(desc, str):
                    result.append({"path": path.strip(), "description": desc.strip()})
        return result


def normalize_agent_output(
    raw_output: str,
    expected_agent: str,
    expected_milestone_id: str,
    stats_id_set: set[str],
    default_phase: str = "implement",
) -> NormalizationResult:
    """Convenience function to normalize agent output.

    This is the main entry point for the TurnNormalizer.
    """
    normalizer = TurnNormalizer(
        expected_agent=expected_agent,
        expected_milestone_id=expected_milestone_id,
        stats_id_set=stats_id_set,
        default_phase=default_phase,
    )
    return normalizer.normalize(raw_output)


def validate_turn_lenient(
    obj: Any,
    *,
    expected_agent: str,
    expected_milestone_id: str | None = None,
    stats_id_set: set[str],
) -> tuple[bool, str, list[str]]:
    """Lenient turn validation that auto-corrects mismatches.

    Returns: (is_valid, error_or_ok, warnings)

    Unlike strict validation, this:
    - Auto-corrects agent/milestone_id mismatches (with warnings)
    - Does not fail on these mismatches
    """
    warnings: list[str] = []

    if not isinstance(obj, dict):
        return False, "turn is not an object", warnings

    required_keys = [
        "agent",
        "milestone_id",
        "phase",
        "work_completed",
        "project_complete",
        "summary",
        "gates_passed",
        "requirement_progress",
        "next_agent",
        "next_prompt",
        "delegate_rationale",
        "stats_refs",
        "needs_write_access",
        "artifacts",
    ]

    for k in required_keys:
        if k not in obj:
            return False, f"missing key: {k}", warnings

    # Agent mismatch - WARN but don't fail, auto-correct
    if obj.get("agent") != expected_agent:
        warnings.append(f"agent mismatch: expected {expected_agent}, got {obj.get('agent')} (auto-corrected)")
        obj["agent"] = expected_agent

    # Milestone mismatch - WARN but don't fail, auto-correct
    if expected_milestone_id is not None and str(obj.get("milestone_id")) != expected_milestone_id:
        warnings.append(
            f"milestone_id mismatch: expected {expected_milestone_id}, got {obj.get('milestone_id')} (auto-corrected)"
        )
        obj["milestone_id"] = expected_milestone_id

    if obj["agent"] not in AGENTS or obj["next_agent"] not in AGENTS:
        return False, "invalid agent id in agent/next_agent", warnings

    if obj.get("phase") not in ("plan", "implement", "verify", "finalize"):
        return False, "invalid phase", warnings

    if not isinstance(obj["work_completed"], bool) or not isinstance(obj["project_complete"], bool):
        return False, "work_completed/project_complete must be boolean", warnings

    for k in ("summary", "next_prompt", "delegate_rationale"):
        if not isinstance(obj.get(k), str):
            return False, f"{k} must be a string", warnings

    if not isinstance(obj["needs_write_access"], bool):
        return False, "needs_write_access must be boolean", warnings

    if not isinstance(obj["gates_passed"], list) or not all(isinstance(x, str) for x in obj["gates_passed"]):
        return False, "gates_passed must be array of strings", warnings

    if not isinstance(obj["stats_refs"], list) or not all(isinstance(x, str) for x in obj["stats_refs"]):
        return False, "stats_refs must be array of strings", warnings
    if not obj["stats_refs"]:
        return False, "stats_refs is empty", warnings
    unknown = [x for x in obj["stats_refs"] if x not in stats_id_set]
    if unknown:
        # Auto-correct unknown stats_refs
        valid_refs = [x for x in obj["stats_refs"] if x in stats_id_set]
        if not valid_refs:
            default_ref = "CL-1" if expected_agent == "claude" else "CX-1"
            valid_refs = [default_ref] if default_ref in stats_id_set else list(stats_id_set)[:1] or ["CL-1"]
        warnings.append(f"unknown stats_refs {unknown} removed, using {valid_refs}")
        obj["stats_refs"] = valid_refs

    rp = obj.get("requirement_progress")
    if not isinstance(rp, dict):
        return False, "requirement_progress must be object", warnings
    for k in ("covered_req_ids", "tests_added_or_modified", "commands_run"):
        if k not in rp:
            return False, f"requirement_progress missing key: {k}", warnings
        if not isinstance(rp[k], list) or not all(isinstance(x, str) for x in rp[k]):
            return False, f"requirement_progress.{k} must be array of strings", warnings

    if not isinstance(obj["artifacts"], list):
        return False, "artifacts must be an array", warnings
    for i, a in enumerate(obj["artifacts"]):
        if not isinstance(a, dict):
            return False, f"artifact[{i}] must be object", warnings
        if set(a.keys()) != {"path", "description"}:
            return (
                False,
                f"artifact[{i}] must have exactly keys: path, description",
                warnings,
            )
        if not isinstance(a.get("path"), str) or not isinstance(a.get("description"), str):
            return False, f"artifact[{i}] path/description must be strings", warnings

    extra = set(obj.keys()) - set(required_keys)
    if extra:
        return False, f"unexpected keys present: {sorted(extra)}", warnings

    return True, "ok", warnings

================================================================================
 FILE: mock_scenarios\dual_agent_smoke.json
================================================================================

{
  "agents": {
    "codex": [
      {
        "type": "ok",
        "response": {
          "agent": "codex",
          "milestone_id": "M0",
          "phase": "implement",
          "work_completed": true,
          "project_complete": false,
          "summary": "Mock implementation step completed.",
          "gates_passed": ["pytest"],
          "requirement_progress": {
            "covered_req_ids": ["REQ-M0-002"],
            "tests_added_or_modified": [],
            "commands_run": ["python -m pytest -q"]
          },
          "next_agent": "claude",
          "next_prompt": "Review the changes and confirm completion gates.",
          "delegate_rationale": "Handing off to Claude for verification.",
          "stats_refs": ["CX-2"],
          "needs_write_access": false,
          "artifacts": []
        }
      }
    ],
    "claude": [
      {
        "type": "ok",
        "response": {
          "agent": "claude",
          "milestone_id": "M0",
          "phase": "finalize",
          "work_completed": true,
          "project_complete": true,
          "summary": "Mock dual-agent completion; request gates.",
          "gates_passed": ["tools.verify"],
          "requirement_progress": {
            "covered_req_ids": ["REQ-M0-003"],
            "tests_added_or_modified": [],
            "commands_run": ["python -m tools.verify --strict-git"]
          },
          "next_agent": "codex",
          "next_prompt": "(mock run complete)",
          "delegate_rationale": "Completion gates should pass; end run.",
          "stats_refs": ["CL-4"],
          "needs_write_access": false,
          "artifacts": []
        }
      }
    ]
  }
}

================================================================================
 FILE: mock_scenarios\milestone_demo.json
================================================================================

{
  "agents": {
    "codex": [
      {
        "type": "ok",
        "response": {
          "agent": "codex",
          "milestone_id": "M0",
          "phase": "finalize",
          "work_completed": true,
          "project_complete": true,
          "summary": "Mock completion: demonstrating JSON handoff + completion gating.",
          "gates_passed": ["read_stats", "read_design_document"],
          "requirement_progress": {
            "covered_req_ids": ["REQ-M0-001", "REQ-M0-002", "REQ-M0-003", "REQ-M0-004", "REQ-M0-005"],
            "tests_added_or_modified": [],
            "commands_run": []
          },
          "next_agent": "claude",
          "next_prompt": "(mock run done)",
          "delegate_rationale": "No next step; this is a deterministic mock run.",
          "stats_refs": ["CX-4"],
          "needs_write_access": false,
          "artifacts": []
        }
      }
    ],
    "claude": []
  }
}

================================================================================
 FILE: mock_scenarios\smoke_route.json
================================================================================

{
  "agents": {
    "codex": [
      {
        "type": "ok",
        "response": {
          "agent": "codex",
          "milestone_id": "M4",
          "phase": "plan",
          "work_completed": true,
          "project_complete": false,
          "summary": "Smoke-route mock: codex step 1.",
          "gates_passed": [],
          "requirement_progress": {
            "covered_req_ids": [],
            "tests_added_or_modified": [],
            "commands_run": []
          },
          "next_agent": "claude",
          "next_prompt": "Smoke-route handoff to Claude.",
          "delegate_rationale": "Continue smoke-route sequence.",
          "stats_refs": ["CX-1"],
          "needs_write_access": false,
          "artifacts": []
        }
      },
      {
        "type": "ok",
        "response": {
          "agent": "codex",
          "milestone_id": "M4",
          "phase": "verify",
          "work_completed": true,
          "project_complete": false,
          "summary": "Smoke-route mock: codex step 2.",
          "gates_passed": [],
          "requirement_progress": {
            "covered_req_ids": [],
            "tests_added_or_modified": [],
            "commands_run": []
          },
          "next_agent": "claude",
          "next_prompt": "Smoke-route follow-up to Claude.",
          "delegate_rationale": "Complete second smoke-route cycle.",
          "stats_refs": ["CX-1"],
          "needs_write_access": false,
          "artifacts": []
        }
      }
    ],
    "claude": [
      {
        "type": "ok",
        "response": {
          "agent": "claude",
          "milestone_id": "M4",
          "phase": "implement",
          "work_completed": true,
          "project_complete": false,
          "summary": "Smoke-route mock: claude step 1.",
          "gates_passed": [],
          "requirement_progress": {
            "covered_req_ids": [],
            "tests_added_or_modified": [],
            "commands_run": []
          },
          "next_agent": "codex",
          "next_prompt": "Smoke-route handoff to Codex.",
          "delegate_rationale": "Continue smoke-route sequence.",
          "stats_refs": ["CL-1"],
          "needs_write_access": false,
          "artifacts": []
        }
      },
      {
        "type": "ok",
        "response": {
          "agent": "claude",
          "milestone_id": "M4",
          "phase": "finalize",
          "work_completed": true,
          "project_complete": false,
          "summary": "Smoke-route mock: claude step 2.",
          "gates_passed": [],
          "requirement_progress": {
            "covered_req_ids": [],
            "tests_added_or_modified": [],
            "commands_run": []
          },
          "next_agent": "codex",
          "next_prompt": "Smoke-route follow-up to Codex.",
          "delegate_rationale": "Complete second smoke-route cycle.",
          "stats_refs": ["CL-1"],
          "needs_write_access": false,
          "artifacts": []
        }
      }
    ]
  }
}

================================================================================
 FILE: mock_scenarios\streaming_demo.json
================================================================================

{
  "agents": {
    "codex": [
      {
        "type": "error",
        "exit_code": 0,
        "stdout": "{\n  \"agent\": \"codex\",\n  \"milestone_id\": \"M4\",\n  \"phase\": \"implement\",\n  \"work_completed\": true,\n  \"project_complete\": false,\n  \"summary\": \"Mock streaming demo: stdout/stderr lines for manual inspection.\",\n  \"gates_passed\": [\"mock_stream\"],\n  \"requirement_progress\": {\n    \"covered_req_ids\": [],\n    \"tests_added_or_modified\": [],\n    \"commands_run\": []\n  },\n  \"next_agent\": \"claude\",\n  \"next_prompt\": \"Verify streaming output and close out the demo.\",\n  \"delegate_rationale\": \"Handing off after streaming output.\",\n  \"stats_refs\": [\"CX-1\"],\n  \"needs_write_access\": false,\n  \"artifacts\": []\n}\n",
        "stderr": "stderr: stream line 1\nstderr: stream line 2\n"
      }
    ],
    "claude": [
      {
        "type": "error",
        "exit_code": 0,
        "stdout": "{\n  \"agent\": \"claude\",\n  \"milestone_id\": \"M4\",\n  \"phase\": \"finalize\",\n  \"work_completed\": true,\n  \"project_complete\": true,\n  \"summary\": \"Mock streaming demo complete; stdout/stderr captured.\",\n  \"gates_passed\": [\"mock_stream\"],\n  \"requirement_progress\": {\n    \"covered_req_ids\": [],\n    \"tests_added_or_modified\": [],\n    \"commands_run\": []\n  },\n  \"next_agent\": \"codex\",\n  \"next_prompt\": \"(mock run complete)\",\n  \"delegate_rationale\": \"Completion reached; end demo.\",\n  \"stats_refs\": [\"CL-1\"],\n  \"needs_write_access\": false,\n  \"artifacts\": []\n}\n",
        "stderr": "stderr: stream line 1\nstderr: stream line 2\n"
      }
    ]
  }
}

================================================================================
 FILE: prompts\system.md
================================================================================

# SYSTEM

You are one of **two** collaborating coding agents (**codex** or **claude**) operating inside a shared Git repository.

You must follow these rules and output contract exactly.

## Output contract

You MUST output **exactly one** JSON object and NOTHING ELSE (no markdown, no code fences, no prose).

Your output MUST validate against `bridge/turn.schema.json` and contain **exactly** these keys:

- `agent`: "codex" or "claude"
- `milestone_id`: e.g., "M0"
- `phase`: one of "plan", "implement", "verify", "finalize"
- `work_completed`: boolean
- `project_complete`: boolean
- `summary`: string
- `gates_passed`: array of strings (may be empty)
- `requirement_progress`: object with keys:
  - `covered_req_ids`: array of strings
  - `tests_added_or_modified`: array of strings
  - `commands_run`: array of strings
- `next_agent`: "codex" or "claude"
- `next_prompt`: string (can be empty only in parallel-worker mode)
- `delegate_rationale`: string
- `stats_refs`: array of strings referencing IDs in `STATS.md` (e.g., "CX-1", "CL-1"). **Must be non-empty.**
- `needs_write_access`: boolean
- `artifacts`: array of objects; each object must have exactly:
  - `path`: string
  - `description`: string

## Collaboration protocol

Both agents are capable of full implementation and review. Agent selection is dynamic based on task requirements:

- **Implementation tasks**: Both Codex and Claude can implement changes, keep diffs tight, and run checks.
- **Review tasks**: Both Codex and Claude can review code, catch edge cases, check prompt/schema compliance, and propose safer plans.
- **Task assignment**: The orchestrator assigns tasks based on heuristics (keywords, workload balance) or explicit policy flags (`--only-codex`, `--only-claude`).

When working in sequential mode, hand off to the other agent unless completing the project. When in `--only-*` mode, you are the sole agent and must handle all tasks yourself.

## Rules

1. **No tool markup**: do NOT output `<task>`, `<read>`, `<edit>`, `<bash>` blocks.
2. **Always set `milestone_id` and `phase`** based on the current work.
3. **`stats_refs`** must contain only `CX-*` and `CL-*` IDs found in `STATS.md`. Do not invent IDs.
4. **Sequential vs parallel-worker mode**:
   - The orchestrator state may include `runner_mode`.
   - If `runner_mode` is **"sequential"** (default): unless `project_complete=true`, you should hand off to the *other* agent (`codex` <-> `claude`) and provide a concrete `next_prompt`.
   - If `runner_mode` is **"parallel-worker"** (or state includes a `worker_id`): you may set `next_agent` to yourself and `next_prompt` to `""` once your assigned task is complete.
5. **Routing override note**: `next_agent` may be overridden by smoke-route; still populate it with your best handoff choice.
6. **Streaming logging note**: streaming model output is logged during runs; only the final JSON turn is the contract.
7. **Resource-intensive commands**:
   - Assume multiple agents may be running concurrently.
   - If the next step requires a potentially resource-intensive local command (likely to exceed ~40% CPU or RAM), **do not run it automatically**.
   - Instead: explain in `summary` what should be run and why, and provide a *manual command/prompt* the user can run later in a single-agent setting.
8. Keep `summary` and `delegate_rationale` concise but specific.

================================================================================
 FILE: prompts\system_engineering.md
================================================================================

# SYSTEM (ENGINEERING MODE)

You are one of two collaborating coding agents (codex or claude) operating inside a shared Git repository.

This is ENGINEERING MODE. Every call must produce real, end-to-end progress on the assigned task.
No filler work, no placeholders, no TODO-only responses.

## Output contract

You MUST output exactly one JSON object and NOTHING ELSE (no markdown, no code fences, no prose).

Your output MUST validate against bridge/turn.schema.json and contain exactly these keys:

- agent: "codex" or "claude"
- milestone_id: e.g., "M0"
- phase: one of "plan", "implement", "verify", "finalize"
- work_completed: boolean
- project_complete: boolean
- summary: string
- gates_passed: array of strings (may be empty)
- requirement_progress: object with keys:
  - covered_req_ids: array of strings
  - tests_added_or_modified: array of strings
  - commands_run: array of strings
- next_agent: "codex" or "claude"
- next_prompt: string (can be empty only in parallel-worker mode)
- delegate_rationale: string
- stats_refs: array of strings referencing IDs in STATS.md (e.g., "CX-1", "CL-1"). Must be non-empty.
- needs_write_access: boolean
- artifacts: array of objects; each object must have exactly:
  - path: string
  - description: string

## Engineering mode rules

1. You own the task end-to-end. Do not delegate unless explicitly required by constraints.
2. No placeholders, no TODO-only outputs, no plan-only responses.
3. If blocked, produce a failure report in your summary with:
   - repro steps (commands)
   - diagnosis
   - concrete next action
4. Your summary MUST include a "Work Report" section with:
   - commands run (or "none")
   - files changed (or "no changes")
   - tests run (or "not run")
   - blockers / next steps
5. Do not claim tools are disabled. Tools ARE enabled.

## Collaboration protocol

Both agents are capable of full implementation and review. Agent selection is dynamic based on task requirements.

- Implementation tasks: both agents can implement changes, keep diffs tight, and run checks.
- Review tasks: both agents can review code, catch edge cases, check prompt/schema compliance, and propose safer plans.
- Task assignment: the orchestrator assigns tasks based on heuristics (keywords, workload balance) or explicit policy flags.

When working in sequential mode, hand off to the other agent unless completing the project. When in --only-* mode, you are the sole agent and must handle all tasks yourself.

## Rules

1. No tool markup: do NOT output <task>, <read>, <edit>, <bash> blocks.
2. Always set milestone_id and phase based on the current work.
3. stats_refs must contain only CX-* and CL-* IDs found in STATS.md. Do not invent IDs.
4. Sequential vs parallel-worker mode:
   - If runner_mode is "sequential" (default): unless project_complete=true, hand off to the other agent and provide a concrete next_prompt.
   - If runner_mode is "parallel-worker" (or state includes a worker_id): you may set next_agent to yourself and next_prompt to "" once your assigned task is complete.
5. Routing override note: next_agent may be overridden by smoke-route; still populate it with your best handoff choice.
6. Streaming logging note: streaming model output is logged during runs; only the final JSON turn is the contract.
7. Resource-intensive commands:
   - Assume multiple agents may be running concurrently.
   - If the next step requires a potentially resource-intensive local command (likely to exceed ~40% CPU or RAM), do not run it automatically.
   - Instead: explain in summary what should be run and why, and provide a manual command the user can run later.
8. Keep summary and delegate_rationale concise but specific.

================================================================================
 FILE: verify_repair\__init__.py
================================================================================

"""Verify auto-repair module for the orchestrator.

This module provides automatic repair functionality for verify gate failures
including failure classification, deterministic auto-fixes, and agent-driven
repair task generation.

The repair loop runs until verify passes, max_attempts is reached, or
repeated identical failures indicate no progress is being made.

STALL PREVENTION: The executor module provides the agent_task_callback that
enables the repair loop to automatically execute repairs instead of waiting
for manual intervention.
"""

from bridge.verify_repair.classify import (
    FailureCategory,
    classify_failures,
    compute_failure_signature,
)
from bridge.verify_repair.data import (
    RepairAttemptRecord,
    RepairLoopReport,
    VerifyGateResult,
    VerifySummary,
)
from bridge.verify_repair.executor import (
    RepairExecutionResult,
    RepairExecutor,
    create_repair_callback,
)
from bridge.verify_repair.loop import (
    RepairLoopResult,
    run_verify_repair_loop,
    write_repair_report,
)

__all__ = [
    # Data structures
    "VerifyGateResult",
    "VerifySummary",
    "RepairAttemptRecord",
    "RepairLoopReport",
    # Classification
    "FailureCategory",
    "classify_failures",
    "compute_failure_signature",
    # Loop
    "RepairLoopResult",
    "run_verify_repair_loop",
    "write_repair_report",
    # Executor (STALL PREVENTION)
    "RepairExecutionResult",
    "RepairExecutor",
    "create_repair_callback",
]

================================================================================
 FILE: verify_repair\agent_tasks.py
================================================================================

"""Agent-driven repair task generation (Layer 2).

When deterministic repairs (Layer 1) cannot fix the failures, this module
generates targeted repair tasks that can be executed by agents.

The generated tasks are:
- Scoped to only the failing gates
- Include reproduction commands
- Include explicit "do not touch" rules
- Require agents to provide minimal plans and concrete diffs
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any

from bridge.verify_repair.classify import (
    FailureCategory,
    extract_import_errors,
    extract_mypy_errors,
)
from bridge.verify_repair.data import VerifySummary


@dataclass
class RepairTask:
    """A repair task to be executed by an agent."""

    id: str
    title: str
    description: str
    agent: str = "claude"
    intensity: str = "medium"
    failing_gates: list[str] = field(default_factory=list)
    target_files: list[str] = field(default_factory=list)
    reproduction_commands: list[str] = field(default_factory=list)
    do_not_touch: list[str] = field(default_factory=list)
    constraints: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dict for task scheduling."""
        return {
            "id": self.id,
            "title": self.title,
            "description": self.description,
            "agent": self.agent,
            "intensity": self.intensity,
        }


def _build_import_error_task(
    summary: VerifySummary,
    import_errors: list[dict[str, Any]],
) -> RepairTask | None:
    """Build a repair task for import errors."""
    if not import_errors:
        return None

    # Group by type
    cannot_import = [e for e in import_errors if e["type"] == "import_error"]
    module_not_found = [e for e in import_errors if e["type"] == "module_not_found"]
    internal_missing = [e for e in module_not_found if e.get("is_internal")]

    if not cannot_import and not internal_missing:
        # Only external missing modules - should be handled by bootstrap
        return None

    # Build description
    desc_parts = [
        "## Verify Repair Task: Fix Import Errors",
        "",
        "The following import errors are blocking pytest/spec_lint:",
        "",
    ]

    if cannot_import:
        desc_parts.append("### ImportError (cannot import name)")
        for err in cannot_import[:5]:  # Limit to 5
            desc_parts.append(f"- Cannot import `{err['name']}` (detected in {err['source_gate']})")
        desc_parts.append("")

    if internal_missing:
        desc_parts.append("### ModuleNotFoundError (internal modules)")
        for err in internal_missing[:5]:
            desc_parts.append(f"- Missing module `{err['module']}` (detected in {err['source_gate']})")
        desc_parts.append("")

    # Add reproduction commands
    desc_parts.extend(
        [
            "## Reproduction",
            "",
            "Run the following to reproduce:",
            "```bash",
            "python -m pytest -q --collect-only 2>&1 | head -100",
            "```",
            "",
            "## Requirements",
            "",
            "1. Fix ONLY the import errors listed above",
            "2. Do NOT add new features or refactor unrelated code",
            "3. Do NOT modify tests - fix the source modules",
            "4. Verify your fix with: `python -m pytest -q --collect-only`",
            "",
        ]
    )

    # Get target files from error context
    target_files: list[str] = []
    for err in import_errors:
        if "module" in err:
            # Convert module path to file path guess
            module_path = err["module"].replace(".", "/")
            target_files.append(f"src/{module_path}.py")
            target_files.append(f"src/{module_path}/__init__.py")

    return RepairTask(
        id="REPAIR-IMPORT-ERRORS",
        title="Fix import errors blocking test collection",
        description="\n".join(desc_parts),
        agent="claude",
        intensity="medium",
        failing_gates=["pytest", "spec_lint"],
        target_files=list(set(target_files))[:10],
        reproduction_commands=[
            "python -m pytest -q --collect-only",
            "python -m tools.spec_lint DESIGN_DOCUMENT.md --collect",
        ],
        do_not_touch=["tests/**", "bridge/**", "tools/**"],
        constraints=[
            "Fix only the import errors listed",
            "Do not add new features",
            "Do not modify test files",
        ],
    )


def _build_mypy_error_task(
    summary: VerifySummary,
    mypy_errors: list[dict[str, str]],
) -> RepairTask | None:
    """Build a repair task for mypy errors."""
    if not mypy_errors:
        return None

    desc_parts = [
        "## Verify Repair Task: Fix Type Errors",
        "",
        "The following mypy type errors need to be fixed:",
        "",
    ]

    target_files: set[str] = set()
    for err in mypy_errors[:10]:  # Limit to 10
        desc_parts.append(f"- `{err['file']}:{err['line']}`: {err['message']}")
        target_files.add(err["file"])

    desc_parts.extend(
        [
            "",
            "## Reproduction",
            "",
            "```bash",
            "mypy .",
            "```",
            "",
            "## Requirements",
            "",
            "1. Fix ONLY the type errors listed above",
            "2. Use proper type annotations, not `# type: ignore`",
            "3. If a type error reveals a real bug, fix the bug",
            "4. Do NOT change unrelated code",
            "",
        ]
    )

    return RepairTask(
        id="REPAIR-MYPY-ERRORS",
        title="Fix mypy type errors",
        description="\n".join(desc_parts),
        agent="claude",
        intensity="low",
        failing_gates=["mypy"],
        target_files=list(target_files)[:10],
        reproduction_commands=["mypy ."],
        do_not_touch=["tests/**", "bridge/**", "tools/**"],
        constraints=[
            "Fix only the type errors listed",
            "Do not use # type: ignore",
            "Do not change unrelated code",
        ],
    )


def _build_test_failure_task(
    summary: VerifySummary,
) -> RepairTask | None:
    """Build a repair task for test failures (not collection errors)."""
    gate = summary.results_by_gate.get("pytest")
    if not gate:
        return None

    combined = gate.stdout + "\n" + gate.stderr

    # Extract failed test names
    failed_tests: list[str] = []
    for line in combined.split("\n"):
        if line.startswith("FAILED "):
            test_name = line.split(" ")[1].split("::")[0] if "::" in line else ""
            if test_name and test_name not in failed_tests:
                failed_tests.append(test_name)

    if not failed_tests:
        return None

    desc_parts = [
        "## Verify Repair Task: Fix Failing Tests",
        "",
        "The following tests are failing:",
        "",
    ]

    for test in failed_tests[:10]:
        desc_parts.append(f"- `{test}`")

    desc_parts.extend(
        [
            "",
            "## Reproduction",
            "",
            "```bash",
            "python -m pytest -q",
            "```",
            "",
            "## Requirements",
            "",
            "1. Analyze WHY the tests are failing",
            "2. Fix the SOURCE CODE if there's a bug",
            "3. Fix the TEST if the test is incorrect",
            "4. Do NOT delete or skip tests",
            "",
        ]
    )

    return RepairTask(
        id="REPAIR-TEST-FAILURES",
        title="Fix failing tests",
        description="\n".join(desc_parts),
        agent="claude",
        intensity="medium",
        failing_gates=["pytest"],
        target_files=failed_tests[:10],
        reproduction_commands=["python -m pytest -q"],
        constraints=[
            "Do not delete or skip tests",
            "Fix bugs in source code when appropriate",
        ],
    )


def generate_repair_tasks(
    summary: VerifySummary,
    classification: dict[str, list[FailureCategory]],
) -> list[RepairTask]:
    """Generate repair tasks for the given failures.

    Returns a list of RepairTask objects that can be executed by agents.
    Tasks are prioritized by likelihood of success.
    """
    tasks: list[RepairTask] = []

    # Get all categories
    all_cats: set[FailureCategory] = set()
    for cats in classification.values():
        all_cats.update(cats)

    # 1. Import error repairs (highest priority - blocks everything)
    if FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR in all_cats:
        import_errors = extract_import_errors(summary)
        task = _build_import_error_task(summary, import_errors)
        if task:
            tasks.append(task)

    # 2. Mypy error repairs
    if FailureCategory.TYPECHECK_MYPY in all_cats:
        mypy_errors = extract_mypy_errors(summary)
        task = _build_mypy_error_task(summary, mypy_errors)
        if task:
            tasks.append(task)

    # 3. Test failure repairs (lower priority - may be fixed by above)
    if FailureCategory.PYTEST_TEST_FAILURE in all_cats:
        task = _build_test_failure_task(summary)
        if task:
            tasks.append(task)

    return tasks


def build_repair_task_prompt(task: RepairTask) -> str:
    """Build a complete prompt for executing a repair task.

    This prompt is designed to be given to an agent with clear boundaries.
    """
    prompt_parts = [
        "# AUTOMATED REPAIR TASK",
        "",
        f"Task ID: {task.id}",
        f"Title: {task.title}",
        "",
        "---",
        "",
        task.description,
        "",
        "---",
        "",
        "## Constraints",
        "",
    ]

    for constraint in task.constraints:
        prompt_parts.append(f"- {constraint}")

    if task.do_not_touch:
        prompt_parts.extend(
            [
                "",
                "## DO NOT MODIFY",
                "",
            ]
        )
        for pattern in task.do_not_touch:
            prompt_parts.append(f"- `{pattern}`")

    prompt_parts.extend(
        [
            "",
            "## Expected Output",
            "",
            "1. A brief analysis of the root cause",
            "2. The specific files you will modify",
            "3. The changes (as diffs or complete new content)",
            "4. Verification that the fix works",
            "",
        ]
    )

    return "\n".join(prompt_parts)

================================================================================
 FILE: verify_repair\bootstrap.py
================================================================================

"""Environment bootstrap and dependency management.

Provides controlled environment setup to prevent "missing install" failures
during verify. The bootstrap step:
1. Runs deterministic install commands (pip install -e .[dev] or uv sync)
2. Logs all operations
3. Does NOT perform arbitrary package installs

This module is called:
- Once at orchestrator run start
- During repair when MISSING_DEPENDENCY category is detected
"""

from __future__ import annotations

import os
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import TextIO


@dataclass
class BootstrapResult:
    """Result of a bootstrap operation."""

    success: bool
    command: list[str]
    returncode: int
    stdout: str
    stderr: str
    elapsed_s: float
    skipped: bool = False
    skip_reason: str = ""


def _get_install_command(project_root: Path) -> list[str] | None:
    """Determine the appropriate install command for this project.

    Returns None if no recognizable package manager setup is found.
    """
    # Check for uv.lock (uv project)
    if (project_root / "uv.lock").exists():
        return ["uv", "sync", "--dev"]

    # Check for pyproject.toml
    pyproject = project_root / "pyproject.toml"
    if pyproject.exists():
        content = pyproject.read_text(encoding="utf-8")
        # Check if it has [project] section (PEP 621)
        if "[project]" in content:
            # Use pip install -e .[dev] - most common pattern
            return [sys.executable, "-m", "pip", "install", "-e", ".[dev]", "--quiet"]

    # Check for setup.py (legacy)
    if (project_root / "setup.py").exists():
        return [sys.executable, "-m", "pip", "install", "-e", ".[dev]", "--quiet"]

    # Check for requirements.txt as fallback
    if (project_root / "requirements.txt").exists():
        return [sys.executable, "-m", "pip", "install", "-r", "requirements.txt", "--quiet"]

    return None


def run_bootstrap(
    project_root: Path,
    log_path: Path | None = None,
    *,
    force: bool = False,
    verbose: bool = True,
    log_file: TextIO | None = None,
) -> BootstrapResult:
    """Run environment bootstrap.

    Args:
        project_root: Project root directory
        log_path: Path to write bootstrap log (optional)
        force: Force reinstall even if already bootstrapped
        verbose: Print progress messages
        log_file: Optional file handle to write logs to

    Returns:
        BootstrapResult with operation details
    """
    start_time = datetime.now(timezone.utc)

    def _log(msg: str) -> None:
        if verbose:
            print(f"[bootstrap] {msg}")
        if log_file:
            log_file.write(f"[{datetime.now(timezone.utc).isoformat()}] {msg}\n")
            log_file.flush()

    # Determine install command
    cmd = _get_install_command(project_root)
    if cmd is None:
        _log("No package manager detected, skipping bootstrap")
        return BootstrapResult(
            success=True,
            command=[],
            returncode=0,
            stdout="",
            stderr="",
            elapsed_s=0.0,
            skipped=True,
            skip_reason="No package manager detected (no pyproject.toml, setup.py, or requirements.txt)",
        )

    # Check for marker file to skip redundant bootstraps
    marker_path = project_root / ".bootstrap_done"
    if not force and marker_path.exists():
        # Check if marker is recent (within last hour)
        marker_age = datetime.now(timezone.utc).timestamp() - marker_path.stat().st_mtime
        if marker_age < 3600:  # 1 hour
            _log(f"Bootstrap marker exists and is recent ({marker_age:.0f}s old), skipping")
            return BootstrapResult(
                success=True,
                command=cmd,
                returncode=0,
                stdout="",
                stderr="",
                elapsed_s=0.0,
                skipped=True,
                skip_reason="Recent bootstrap marker exists",
            )

    _log(f"Running: {' '.join(cmd)}")

    try:
        proc = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=600,  # 10 minute timeout
            env=os.environ.copy(),
        )

        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()

        if proc.returncode == 0:
            _log(f"Bootstrap completed successfully in {elapsed:.1f}s")
            # Write marker file
            marker_path.write_text(datetime.now(timezone.utc).isoformat(), encoding="utf-8")
        else:
            _log(f"Bootstrap failed with rc={proc.returncode}")
            if proc.stderr:
                _log(f"stderr: {proc.stderr[:500]}")

        # Write to log file if provided
        if log_path:
            log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"Command: {' '.join(cmd)}\n")
                f.write(f"Return code: {proc.returncode}\n")
                f.write(f"Elapsed: {elapsed:.1f}s\n")
                f.write("\n=== STDOUT ===\n")
                f.write(proc.stdout)
                f.write("\n=== STDERR ===\n")
                f.write(proc.stderr)

        return BootstrapResult(
            success=(proc.returncode == 0),
            command=cmd,
            returncode=proc.returncode,
            stdout=proc.stdout,
            stderr=proc.stderr,
            elapsed_s=elapsed,
        )

    except subprocess.TimeoutExpired:
        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
        _log("Bootstrap timed out after 600s")
        return BootstrapResult(
            success=False,
            command=cmd,
            returncode=-1,
            stdout="",
            stderr="Bootstrap timed out after 600s",
            elapsed_s=elapsed,
        )
    except Exception as e:
        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
        _log(f"Bootstrap error: {e}")
        return BootstrapResult(
            success=False,
            command=cmd,
            returncode=-1,
            stdout="",
            stderr=str(e),
            elapsed_s=elapsed,
        )


def clear_bootstrap_marker(project_root: Path) -> None:
    """Clear the bootstrap marker to force re-bootstrap on next run."""
    marker_path = project_root / ".bootstrap_done"
    if marker_path.exists():
        marker_path.unlink()

================================================================================
 FILE: verify_repair\classify.py
================================================================================

"""Failure classification for verify repair.

Classifies verify failures into categories to enable targeted repair strategies.
"""

from __future__ import annotations

import hashlib
import re
from enum import Enum
from typing import Any

from bridge.verify_repair.data import VerifyGateResult, VerifySummary


class FailureCategory(str, Enum):
    """Categories of verify failures."""

    # Lint failures (often auto-fixable)
    LINT_RUFF = "lint_ruff"
    LINT_FORMAT = "lint_format"

    # Type checking failures (require code changes)
    TYPECHECK_MYPY = "typecheck_mypy"

    # Test failures
    PYTEST_COLLECTION_IMPORT_ERROR = "pytest_collection_import_error"
    PYTEST_TEST_FAILURE = "pytest_test_failure"

    # Spec lint
    SPEC_LINT_FAILURE = "spec_lint_failure"

    # Environment/dependency issues
    MISSING_DEPENDENCY = "missing_dependency"
    MISSING_MODULE_INTERNAL = "missing_module_internal"  # Internal module not found

    # Tooling issues
    TOOLING_ERROR = "tooling_error"
    TOOLING_TIMEOUT = "tooling_timeout"

    # Git issues
    GIT_DIRTY = "git_dirty"

    # Unknown
    UNKNOWN = "unknown"


# Patterns for detecting specific failure types
IMPORT_ERROR_RE = re.compile(r"ImportError: cannot import name ['\"](\w+)['\"]")
MODULE_NOT_FOUND_RE = re.compile(r"ModuleNotFoundError: No module named ['\"]([^'\"]+)['\"]")
COLLECTION_ERROR_RE = re.compile(r"ERROR collecting .+\.py")
MYPY_ERROR_RE = re.compile(r"^(.+\.py):(\d+): error: (.+)$", re.MULTILINE)

# Known internal module prefixes (not pip-installable)
INTERNAL_MODULE_PREFIXES = (
    "formula_foundry",
    "tools",
    "bridge",
    "tests",
    "benchmarks",
)

# Known external packages that can be bootstrap-installed
BOOTSTRAP_INSTALLABLE = {
    "numpy",
    "pytest",
    "ruff",
    "mypy",
    "pyyaml",
    "yaml",
    "mlflow",
    "cupy",
}


def _classify_pytest_failure(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify pytest failure into specific categories."""
    categories: list[FailureCategory] = []
    combined = gate.stdout + "\n" + gate.stderr

    # Check for collection errors first (import errors during collection)
    if COLLECTION_ERROR_RE.search(combined):
        # Distinguish between import errors and module not found
        if IMPORT_ERROR_RE.search(combined):
            categories.append(FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR)
        elif MODULE_NOT_FOUND_RE.search(combined):
            # Check if it's an internal or external module
            for match in MODULE_NOT_FOUND_RE.finditer(combined):
                module = match.group(1).split(".")[0]
                if module in BOOTSTRAP_INSTALLABLE:
                    categories.append(FailureCategory.MISSING_DEPENDENCY)
                elif any(module.startswith(p) for p in INTERNAL_MODULE_PREFIXES):
                    categories.append(FailureCategory.MISSING_MODULE_INTERNAL)
                else:
                    # Could be external or internal - check more carefully
                    categories.append(FailureCategory.MISSING_DEPENDENCY)
            categories.append(FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR)
        else:
            categories.append(FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR)

    # Check for actual test failures (passed collection but tests failed)
    if "FAILED" in combined and "passed" in combined.lower():
        categories.append(FailureCategory.PYTEST_TEST_FAILURE)

    # If no specific category found, mark as test failure
    if not categories and gate.returncode not in (None, 0):
        categories.append(FailureCategory.PYTEST_TEST_FAILURE)

    return categories


def _classify_mypy_failure(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify mypy failure."""
    categories: list[FailureCategory] = []
    combined = gate.stdout + "\n" + gate.stderr

    if "error:" in combined:
        categories.append(FailureCategory.TYPECHECK_MYPY)

    # Check for import errors in mypy output
    if MODULE_NOT_FOUND_RE.search(combined):
        for match in MODULE_NOT_FOUND_RE.finditer(combined):
            module = match.group(1).split(".")[0]
            if module in BOOTSTRAP_INSTALLABLE:
                categories.append(FailureCategory.MISSING_DEPENDENCY)

    if not categories:
        categories.append(FailureCategory.TYPECHECK_MYPY)

    return categories


def _classify_ruff_failure(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify ruff failure."""
    return [FailureCategory.LINT_RUFF]


def _classify_spec_lint_failure(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify spec_lint failure."""
    categories: list[FailureCategory] = []
    combined = gate.stdout + "\n" + gate.stderr

    # Check if spec_lint failure is due to pytest collection errors reported in stderr
    if COLLECTION_ERROR_RE.search(combined) or "ERROR collecting" in combined:
        categories.append(FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR)

    categories.append(FailureCategory.SPEC_LINT_FAILURE)
    return categories


def _classify_git_guard_failure(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify git_guard failure."""
    return [FailureCategory.GIT_DIRTY]


def _classify_gate(gate: VerifyGateResult) -> list[FailureCategory]:
    """Classify a single gate failure."""
    if gate.passed:
        return []

    if gate.note and "timeout" in gate.note.lower():
        return [FailureCategory.TOOLING_TIMEOUT]

    name = gate.name.lower()

    if name == "pytest":
        return _classify_pytest_failure(gate)
    elif name == "mypy":
        return _classify_mypy_failure(gate)
    elif name == "ruff":
        return _classify_ruff_failure(gate)
    elif name == "spec_lint":
        return _classify_spec_lint_failure(gate)
    elif name == "git_guard":
        return _classify_git_guard_failure(gate)
    else:
        return [FailureCategory.UNKNOWN]


def classify_failures(summary: VerifySummary) -> dict[str, list[FailureCategory]]:
    """Classify all failures in a verify summary.

    Returns a dict mapping gate name to list of failure categories.
    """
    result: dict[str, list[FailureCategory]] = {}
    for gate_name in summary.failed_gates:
        gate = summary.results_by_gate.get(gate_name)
        if gate:
            result[gate_name] = _classify_gate(gate)
        else:
            result[gate_name] = [FailureCategory.UNKNOWN]
    return result


def get_all_categories(classification: dict[str, list[FailureCategory]]) -> set[FailureCategory]:
    """Get all unique categories from a classification result."""
    categories: set[FailureCategory] = set()
    for cats in classification.values():
        categories.update(cats)
    return categories


def extract_import_errors(summary: VerifySummary) -> list[dict[str, Any]]:
    """Extract detailed import error information from verify summary.

    Returns list of dicts with:
    - type: 'import_error' or 'module_not_found'
    - name: the name that couldn't be imported
    - module: the module it was being imported from (for import errors)
    - source_file: the file that triggered the error
    - is_internal: whether this appears to be an internal module
    """
    errors: list[dict[str, Any]] = []
    seen: set[str] = set()

    for gate in summary.results_by_gate.values():
        combined = gate.stdout + "\n" + gate.stderr

        # ImportError: cannot import name 'X' from 'Y'
        for match in IMPORT_ERROR_RE.finditer(combined):
            name = match.group(1)
            key = f"import:{name}"
            if key not in seen:
                seen.add(key)
                errors.append(
                    {
                        "type": "import_error",
                        "name": name,
                        "source_gate": gate.name,
                    }
                )

        # ModuleNotFoundError: No module named 'X'
        for match in MODULE_NOT_FOUND_RE.finditer(combined):
            module = match.group(1)
            key = f"module:{module}"
            if key not in seen:
                seen.add(key)
                root_module = module.split(".")[0]
                is_internal = any(root_module.startswith(p) for p in INTERNAL_MODULE_PREFIXES)
                errors.append(
                    {
                        "type": "module_not_found",
                        "module": module,
                        "root_module": root_module,
                        "is_internal": is_internal,
                        "is_bootstrap_installable": root_module in BOOTSTRAP_INSTALLABLE,
                        "source_gate": gate.name,
                    }
                )

    return errors


def extract_mypy_errors(summary: VerifySummary) -> list[dict[str, str]]:
    """Extract mypy error details from verify summary.

    Returns list of dicts with:
    - file: path to the file
    - line: line number
    - message: error message
    """
    errors: list[dict[str, str]] = []
    gate = summary.results_by_gate.get("mypy")
    if not gate:
        return errors

    combined = gate.stdout + "\n" + gate.stderr
    for match in MYPY_ERROR_RE.finditer(combined):
        errors.append(
            {
                "file": match.group(1),
                "line": match.group(2),
                "message": match.group(3),
            }
        )

    return errors


def compute_failure_signature(summary: VerifySummary) -> str:
    """Compute a signature for the current failure state.

    Used to detect when the same failures repeat (no progress).
    The signature is based on:
    - The set of failed gates
    - First N lines of error output from each failed gate
    """
    parts: list[str] = []
    parts.append(",".join(sorted(summary.failed_gates)))

    for gate_name in sorted(summary.failed_gates):
        gate = summary.results_by_gate.get(gate_name)
        if gate:
            # Take first 20 lines of stderr/stdout for signature
            combined = (gate.stderr + "\n" + gate.stdout).strip()
            lines = combined.split("\n")[:20]
            # Normalize: strip, remove timestamps/paths that change
            normalized = []
            for line in lines:
                line = line.strip()
                # Remove path prefixes that might change
                line = re.sub(r"/[^\s]+/([^/\s]+\.py)", r"\1", line)
                # Remove timestamps
                line = re.sub(r"\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}", "TIMESTAMP", line)
                if line:
                    normalized.append(line)
            parts.append(gate_name + ":" + "|".join(normalized[:10]))

    signature_text = "|||".join(parts)
    return hashlib.sha256(signature_text.encode()).hexdigest()[:16]

================================================================================
 FILE: verify_repair\data.py
================================================================================

"""Data structures for verify repair operations."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any


@dataclass
class VerifyGateResult:
    """Result from a single verify gate."""

    name: str
    returncode: int | None
    passed: bool
    stdout: str
    stderr: str
    cmd: list[str] | None = None
    note: str = ""

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> VerifyGateResult:
        """Create from verify JSON gate result."""
        return cls(
            name=data.get("name", ""),
            returncode=data.get("returncode"),
            passed=data.get("passed", False),
            stdout=data.get("stdout", ""),
            stderr=data.get("stderr", ""),
            cmd=data.get("cmd"),
            note=data.get("note", ""),
        )


@dataclass
class VerifySummary:
    """Summary of a verify run."""

    ok: bool
    failed_gates: list[str]
    first_failed_gate: str
    results_by_gate: dict[str, VerifyGateResult]

    @classmethod
    def from_json(cls, data: dict[str, Any]) -> VerifySummary:
        """Create from verify JSON output."""
        results_by_gate: dict[str, VerifyGateResult] = {}
        for r in data.get("results", []):
            gate = VerifyGateResult.from_dict(r)
            results_by_gate[gate.name] = gate
        return cls(
            ok=data.get("ok", False),
            failed_gates=data.get("failed_gates", []),
            first_failed_gate=data.get("first_failed_gate", ""),
            results_by_gate=results_by_gate,
        )


@dataclass
class RepairAttemptRecord:
    """Record of a single repair attempt."""

    attempt_index: int
    detected_categories: list[str]
    actions_taken: list[str]
    verify_before: VerifySummary | None
    verify_after: VerifySummary | None
    diff_applied: bool
    elapsed_s: float

    def to_dict(self) -> dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "attempt_index": self.attempt_index,
            "detected_categories": self.detected_categories,
            "actions_taken": self.actions_taken,
            "diff_applied": self.diff_applied,
            "elapsed_s": self.elapsed_s,
            "verify_before_ok": self.verify_before.ok if self.verify_before else None,
            "verify_before_failed": self.verify_before.failed_gates if self.verify_before else [],
            "verify_after_ok": self.verify_after.ok if self.verify_after else None,
            "verify_after_failed": self.verify_after.failed_gates if self.verify_after else [],
        }


@dataclass
class RepairLoopReport:
    """Final report of the repair loop."""

    success: bool
    total_attempts: int
    final_failed_gates: list[str]
    elapsed_s: float
    stable_failure_signature_count: int
    artifacts_written: list[str]
    attempts: list[RepairAttemptRecord] = field(default_factory=list)
    early_stop_reason: str = ""

    def to_dict(self) -> dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "success": self.success,
            "total_attempts": self.total_attempts,
            "final_failed_gates": self.final_failed_gates,
            "elapsed_s": self.elapsed_s,
            "stable_failure_signature_count": self.stable_failure_signature_count,
            "artifacts_written": self.artifacts_written,
            "early_stop_reason": self.early_stop_reason,
            "attempts": [a.to_dict() for a in self.attempts],
        }

================================================================================
 FILE: verify_repair\executor.py
================================================================================

"""Repair task executor for the verify repair loop.

This module provides the callback function that executes repair tasks
generated by the verify repair loop. It ensures:

1. Tasks are executed with proper scope enforcement
2. Only orchestrator-scoped changes are allowed
3. Artifacts are written for all executions
4. The orchestrator does NOT stall after verify failures

CRITICAL: This executor is designed to run INTERNAL orchestrator repairs,
not general project repairs. It enforces strict scope boundaries.
"""

from __future__ import annotations

import json
import subprocess
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from bridge.patch_integration import ScopeGuard
from bridge.verify_repair.agent_tasks import RepairTask, build_repair_task_prompt


@dataclass
class RepairExecutionResult:
    """Result of executing repair tasks."""

    success: bool
    tasks_executed: int
    tasks_succeeded: int
    tasks_failed: int
    artifacts_written: list[str] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "success": self.success,
            "tasks_executed": self.tasks_executed,
            "tasks_succeeded": self.tasks_succeeded,
            "tasks_failed": self.tasks_failed,
            "artifacts_written": self.artifacts_written,
            "errors": self.errors,
        }


class RepairExecutor:
    """Executes repair tasks with scope enforcement.

    This class is responsible for:
    1. Filtering tasks to only those that can be repaired within scope
    2. Executing deterministic repairs (ruff --fix, etc.)
    3. Optionally scheduling agent-driven repairs
    4. Writing execution artifacts
    """

    # Scope for orchestrator-only repairs
    ORCHESTRATOR_ALLOWLIST = (
        "bridge/**",
        "tests/test_orchestrator*.py",
        "tests/test_verify_repair*.py",
    )

    ORCHESTRATOR_DENYLIST = (
        "src/**",
        "tools/**",
        "docs/**",
        "DESIGN_DOCUMENT.md",
        "pyproject.toml",
    )

    def __init__(
        self,
        project_root: Path,
        runs_dir: Path,
        verbose: bool = True,
    ) -> None:
        """Initialize the executor.

        Args:
            project_root: Root of the project
            runs_dir: Directory for writing artifacts
            verbose: Whether to print progress
        """
        self.project_root = project_root
        self.runs_dir = runs_dir
        self.verbose = verbose
        self.scope_guard = ScopeGuard(
            allowlist=self.ORCHESTRATOR_ALLOWLIST,
            denylist=self.ORCHESTRATOR_DENYLIST,
            runs_dir=runs_dir,
        )

    def _log(self, msg: str) -> None:
        """Log a message if verbose."""
        if self.verbose:
            print(f"[repair_executor] {msg}")

    def execute_deterministic_repairs(self) -> RepairExecutionResult:
        """Execute deterministic repairs that don't require agent intervention.

        This includes:
        - ruff --fix for lint errors
        - ruff format for formatting
        - isort for import sorting

        These are safe to run and won't break anything.
        """
        self._log("Running deterministic repairs...")
        artifacts: list[str] = []
        errors: list[str] = []

        # Run ruff --fix
        try:
            self._log("Running ruff check --fix")
            proc = subprocess.run(
                ["ruff", "check", ".", "--fix", "--unsafe-fixes"],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=120,
            )
            if proc.stdout:
                self._log(f"ruff output: {proc.stdout[:500]}")
        except FileNotFoundError:
            errors.append("ruff not found in PATH")
        except subprocess.TimeoutExpired:
            errors.append("ruff timed out after 120s")
        except Exception as e:
            errors.append(f"ruff error: {e}")

        # Run ruff format
        try:
            self._log("Running ruff format")
            proc = subprocess.run(
                ["ruff", "format", "."],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=120,
            )
        except FileNotFoundError:
            pass  # ruff not found already logged
        except subprocess.TimeoutExpired:
            errors.append("ruff format timed out")
        except Exception as e:
            errors.append(f"ruff format error: {e}")

        return RepairExecutionResult(
            success=len(errors) == 0,
            tasks_executed=2,  # ruff check + ruff format
            tasks_succeeded=2 - len(errors),
            tasks_failed=len(errors),
            artifacts_written=artifacts,
            errors=errors,
        )

    def create_agent_task_callback(
        self,
        scheduler_callback: Callable[[list[dict[str, Any]]], bool] | None = None,
    ) -> Callable[[list[RepairTask]], bool]:
        """Create the callback function for run_verify_repair_loop.

        This callback is invoked when the repair loop needs agent-driven repairs.
        It either:
        1. Schedules tasks through the provided scheduler callback, OR
        2. Executes deterministic repairs and writes pending tasks for manual pickup

        Args:
            scheduler_callback: Optional callback to schedule tasks through orchestrator.
                Signature: callback(tasks: list[dict]) -> bool

        Returns:
            A callback function with signature (tasks: list[RepairTask]) -> bool
        """

        def callback(tasks: list[RepairTask]) -> bool:
            """Execute or schedule repair tasks."""
            if not tasks:
                return True

            self._log(f"Processing {len(tasks)} repair task(s)")
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

            # Write repair plan artifact
            plan_path = self.runs_dir / f"repair_plan_{timestamp}.json"
            plan_data = {
                "timestamp": timestamp,
                "tasks": [t.to_dict() for t in tasks],
                "prompts": [build_repair_task_prompt(t) for t in tasks],
            }
            plan_path.write_text(json.dumps(plan_data, indent=2), encoding="utf-8")
            self._log(f"Wrote repair plan: {plan_path}")

            # Filter tasks to those within scope
            orchestrator_tasks = []
            out_of_scope_tasks = []
            for task in tasks:
                # Check if task targets are within scope
                scope_result = self.scope_guard.check_paths(task.target_files)
                if scope_result.allowed or not task.target_files:
                    orchestrator_tasks.append(task)
                else:
                    out_of_scope_tasks.append(task)
                    self._log(f"Task {task.id} targets out-of-scope files, skipping: {[v.path for v in scope_result.violations]}")

            # Run deterministic repairs first
            det_result = self.execute_deterministic_repairs()
            if det_result.errors:
                self._log(f"Deterministic repair errors: {det_result.errors}")

            # If we have a scheduler callback, use it for orchestrator tasks
            if scheduler_callback and orchestrator_tasks:
                self._log(f"Scheduling {len(orchestrator_tasks)} orchestrator repair task(s)")
                task_dicts = [t.to_dict() for t in orchestrator_tasks]
                try:
                    success = scheduler_callback(task_dicts)
                    if success:
                        self._log("Scheduler accepted repair tasks")
                        return True
                    else:
                        self._log("Scheduler rejected repair tasks")
                except Exception as e:
                    self._log(f"Scheduler callback error: {e}")

            # Write out-of-scope tasks for manual review
            if out_of_scope_tasks:
                out_of_scope_path = self.runs_dir / "out_of_scope_repairs.json"
                out_of_scope_data = {
                    "timestamp": timestamp,
                    "reason": "These repair tasks target files outside orchestrator scope",
                    "tasks": [t.to_dict() for t in out_of_scope_tasks],
                    "note": "These require manual intervention or a broader repair scope",
                }
                out_of_scope_path.write_text(json.dumps(out_of_scope_data, indent=2), encoding="utf-8")
                self._log(f"Wrote out-of-scope repairs: {out_of_scope_path}")

            # Return True if deterministic repairs succeeded (partial success is OK)
            return det_result.tasks_succeeded > 0

        return callback


def create_repair_callback(
    project_root: Path,
    runs_dir: Path,
    verbose: bool = True,
    scheduler_callback: Callable[[list[dict[str, Any]]], bool] | None = None,
) -> Callable[[list[Any]], bool]:
    """Factory function to create a repair callback for run_verify_repair_loop.

    This is the primary API for integrating the repair executor with the
    verify repair loop.

    Args:
        project_root: Root of the project
        runs_dir: Directory for artifacts
        verbose: Whether to print progress
        scheduler_callback: Optional callback for scheduling agent tasks

    Returns:
        A callback function suitable for run_verify_repair_loop's agent_task_callback
    """
    executor = RepairExecutor(
        project_root=project_root,
        runs_dir=runs_dir,
        verbose=verbose,
    )
    return executor.create_agent_task_callback(scheduler_callback)

================================================================================
 FILE: verify_repair\loop.py
================================================================================

"""Verify auto-repair loop implementation.

This is the main repair loop that orchestrates:
1. Running verify
2. Classifying failures
3. Applying Layer 1 deterministic repairs
4. Generating Layer 2 agent tasks when needed
5. Tracking progress and detecting stalls
6. Writing reports and artifacts
"""

from __future__ import annotations

import json
import os
import subprocess
import sys
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from bridge.verify_repair.bootstrap import run_bootstrap
from bridge.verify_repair.classify import (
    FailureCategory,
    classify_failures,
    compute_failure_signature,
    get_all_categories,
)
from bridge.verify_repair.data import (
    RepairAttemptRecord,
    RepairLoopReport,
    VerifySummary,
)
from bridge.verify_repair.repairs import apply_repair, get_applicable_repairs


@dataclass
class RepairLoopResult:
    """Result of the complete repair loop (backwards compatible)."""

    success: bool
    total_attempts: int
    final_exit_code: int
    reports: list[Any] = field(default_factory=list)  # Legacy format
    remaining_failures: list[str] = field(default_factory=list)
    repair_report: RepairLoopReport | None = None


def _run_verify(
    project_root: Path,
    out_json: Path,
    strict_git: bool,
) -> tuple[int, str, str]:
    """Run tools.verify and capture output."""
    cmd = [sys.executable, "-m", "tools.verify", "--json", str(out_json)]
    if strict_git:
        cmd.append("--strict-git")

    env = os.environ.copy()
    # Ensure src is in PYTHONPATH
    pythonpath = env.get("PYTHONPATH", "")
    if "src" not in pythonpath:
        env["PYTHONPATH"] = f"src:{pythonpath}"

    try:
        proc = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=300,
            env=env,
        )
        return proc.returncode, proc.stdout, proc.stderr
    except subprocess.TimeoutExpired:
        return -1, "", "verify timeout after 300s"
    except Exception as e:
        return -1, "", str(e)


def _parse_verify_json(json_path: Path) -> VerifySummary | None:
    """Parse verify JSON output into VerifySummary."""
    if not json_path.exists():
        return None
    try:
        data = json.loads(json_path.read_text(encoding="utf-8"))
        return VerifySummary.from_json(data)
    except (json.JSONDecodeError, OSError):
        return None


def run_verify_repair_loop(
    project_root: Path,
    verify_json_path: Path,
    *,
    max_attempts: int = 5,
    strict_git: bool = True,
    verbose: bool = True,
    runs_dir: Path | None = None,
    bootstrap_on_start: bool = True,
    agent_task_callback: Callable[[list[Any]], bool] | None = None,
) -> RepairLoopResult:
    """Run verify with automatic repair loop.

    This is the main entry point for the verify auto-repair system.

    Args:
        project_root: Root directory of the project
        verify_json_path: Path to write verify JSON output
        max_attempts: Maximum repair attempts before giving up (default 5)
        strict_git: Whether to use --strict-git flag
        verbose: Whether to print progress
        runs_dir: Directory for run artifacts (optional)
        bootstrap_on_start: Run bootstrap before first verify
        agent_task_callback: Optional callback to execute agent tasks
            Signature: callback(tasks) -> bool (success)
            If not provided, agent tasks are generated but not executed

    Returns:
        RepairLoopResult containing success status and repair reports
    """
    start_time = datetime.now(timezone.utc)
    attempts: list[RepairAttemptRecord] = []
    artifacts_written: list[str] = []
    signature_history: list[str] = []
    stable_signature_count = 0

    def _log(msg: str) -> None:
        if verbose:
            print(f"[verify_repair] {msg}")

    # Ensure runs_dir exists
    if runs_dir is None:
        runs_dir = verify_json_path.parent
    runs_dir.mkdir(parents=True, exist_ok=True)

    # Bootstrap on start if requested
    if bootstrap_on_start:
        _log("Running initial bootstrap")
        bootstrap_log = runs_dir / "bootstrap.log"
        bootstrap_result = run_bootstrap(
            project_root,
            log_path=bootstrap_log,
            verbose=verbose,
        )
        if not bootstrap_result.skipped:
            artifacts_written.append(str(bootstrap_log))
        if not bootstrap_result.success:
            _log(f"Bootstrap failed: {bootstrap_result.stderr[:200]}")
            # Continue anyway - verify will tell us what's wrong

    for attempt_idx in range(1, max_attempts + 1):
        attempt_start = datetime.now(timezone.utc)
        _log(f"=== Attempt {attempt_idx}/{max_attempts} ===")

        # Run verify
        rc, stdout, stderr = _run_verify(project_root, verify_json_path, strict_git)

        if rc == 0:
            _log(f"SUCCESS after {attempt_idx} attempt(s)")
            elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
            return RepairLoopResult(
                success=True,
                total_attempts=attempt_idx,
                final_exit_code=0,
                remaining_failures=[],
                repair_report=RepairLoopReport(
                    success=True,
                    total_attempts=attempt_idx,
                    final_failed_gates=[],
                    elapsed_s=elapsed,
                    stable_failure_signature_count=stable_signature_count,
                    artifacts_written=artifacts_written,
                    attempts=attempts,
                ),
            )

        # Parse verify result
        summary = _parse_verify_json(verify_json_path)
        if summary is None:
            _log("Failed to parse verify JSON")
            summary = VerifySummary(
                ok=False,
                failed_gates=["unknown"],
                first_failed_gate="unknown",
                results_by_gate={},
            )

        _log(f"Failed gates: {summary.failed_gates}")

        # Compute failure signature for stall detection
        # EARLY STOP: If we see the same failure signature twice, repairs are ineffective.
        # Lowered from >= 2 to >= 1 to stop faster and prevent thrashing.
        signature = compute_failure_signature(summary)
        if signature in signature_history:
            stable_signature_count += 1
            _log(f"Repeated failure signature detected (count: {stable_signature_count}), signature: {signature[:16]}")
            if stable_signature_count >= 1:
                _log("EARLY STOP: Same failure signature repeated - repairs are not fixing the root cause")
                elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
                return RepairLoopResult(
                    success=False,
                    total_attempts=attempt_idx,
                    final_exit_code=rc,
                    remaining_failures=summary.failed_gates,
                    repair_report=RepairLoopReport(
                        success=False,
                        total_attempts=attempt_idx,
                        final_failed_gates=summary.failed_gates,
                        elapsed_s=elapsed,
                        stable_failure_signature_count=stable_signature_count,
                        artifacts_written=artifacts_written,
                        attempts=attempts,
                        early_stop_reason="Repeated identical failure signature",
                    ),
                )
        else:
            stable_signature_count = 0
        signature_history.append(signature)

        # Classify failures
        classification = classify_failures(summary)
        all_categories = get_all_categories(classification)
        category_names = [c.value for c in all_categories]
        _log(f"Failure categories: {category_names}")

        # Attempt Layer 1 repairs (deterministic)
        actions_taken: list[str] = []
        repairs_applied = False

        # Check for bootstrap-fixable issues
        if FailureCategory.MISSING_DEPENDENCY in all_categories:
            _log("Running bootstrap for missing dependency")
            bootstrap_log = runs_dir / f"bootstrap_repair_{attempt_idx}.log"
            bootstrap_result = run_bootstrap(
                project_root,
                log_path=bootstrap_log,
                force=True,
                verbose=verbose,
            )
            artifacts_written.append(str(bootstrap_log))
            if bootstrap_result.success:
                actions_taken.append("bootstrap_reinstall")
                repairs_applied = True

        # Apply applicable Layer 1 repairs
        applicable = get_applicable_repairs(all_categories)
        for repair_name in applicable:
            if repair_name == "bootstrap":
                continue  # Already handled above
            _log(f"Applying repair: {repair_name}")
            action = apply_repair(repair_name, project_root, verbose=verbose)
            actions_taken.append(f"{repair_name}:{action.success}")
            if action.success and action.files_modified > 0:
                repairs_applied = True
                _log(f"  Modified {action.files_modified} files")

        # Check if we need Layer 2 (agent-driven repairs)
        needs_agent_repair = False
        agent_categories = {
            FailureCategory.PYTEST_COLLECTION_IMPORT_ERROR,
            FailureCategory.PYTEST_TEST_FAILURE,
            FailureCategory.TYPECHECK_MYPY,
            FailureCategory.SPEC_LINT_FAILURE,
            FailureCategory.MISSING_MODULE_INTERNAL,
        }
        if all_categories & agent_categories:
            needs_agent_repair = True

        # Generate and potentially execute agent tasks
        if needs_agent_repair and agent_task_callback is not None:
            _log("Generating agent repair tasks")
            from bridge.verify_repair.agent_tasks import generate_repair_tasks

            repair_tasks = generate_repair_tasks(summary, classification)
            if repair_tasks:
                _log(f"Generated {len(repair_tasks)} repair task(s)")
                # Write task specs to artifacts
                tasks_file = runs_dir / f"repair_tasks_{attempt_idx}.json"
                tasks_data = [t.to_dict() for t in repair_tasks]
                tasks_file.write_text(json.dumps(tasks_data, indent=2), encoding="utf-8")
                artifacts_written.append(str(tasks_file))

                # Execute via callback
                success = agent_task_callback(repair_tasks)
                if success:
                    actions_taken.append("agent_repair_tasks")
                    repairs_applied = True
                else:
                    actions_taken.append("agent_repair_tasks:failed")

        # Record attempt
        attempt_elapsed = (datetime.now(timezone.utc) - attempt_start).total_seconds()
        attempts.append(
            RepairAttemptRecord(
                attempt_index=attempt_idx,
                detected_categories=category_names,
                actions_taken=actions_taken,
                verify_before=summary,
                verify_after=None,  # Will be set by next iteration
                diff_applied=repairs_applied,
                elapsed_s=attempt_elapsed,
            )
        )

        if not repairs_applied:
            _log("No repairs could be applied")
            # Check if we have agent tasks but no callback
            if needs_agent_repair and agent_task_callback is None:
                _log("Agent repairs needed but no callback provided")
                # Write pending tasks for orchestrator to pick up
                from bridge.verify_repair.agent_tasks import generate_repair_tasks

                repair_tasks = generate_repair_tasks(summary, classification)
                if repair_tasks:
                    pending_tasks_file = runs_dir / "pending_repair_tasks.json"
                    tasks_data = [t.to_dict() for t in repair_tasks]
                    # Include full description for manual review
                    for i, task in enumerate(repair_tasks):
                        tasks_data[i]["full_description"] = task.description
                    pending_tasks_file.write_text(json.dumps(tasks_data, indent=2), encoding="utf-8")
                    artifacts_written.append(str(pending_tasks_file))
                    _log(f"Wrote pending repair tasks to: {pending_tasks_file}")
            break

    # Final verify to get current state
    rc, _, _ = _run_verify(project_root, verify_json_path, strict_git)
    final_summary = _parse_verify_json(verify_json_path)
    final_failed = final_summary.failed_gates if final_summary else ["unknown"]

    elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()

    _log(f"{'SUCCESS' if rc == 0 else 'FAILED'} after {len(attempts)} attempt(s)")
    if rc != 0:
        _log(f"Remaining failures: {final_failed}")

    return RepairLoopResult(
        success=(rc == 0),
        total_attempts=len(attempts),
        final_exit_code=rc,
        remaining_failures=final_failed if rc != 0 else [],
        repair_report=RepairLoopReport(
            success=(rc == 0),
            total_attempts=len(attempts),
            final_failed_gates=final_failed if rc != 0 else [],
            elapsed_s=elapsed,
            stable_failure_signature_count=stable_signature_count,
            artifacts_written=artifacts_written,
            attempts=attempts,
        ),
    )


def write_repair_report(
    result: RepairLoopResult,
    output_path: Path,
) -> None:
    """Write repair loop report to JSON file."""
    if result.repair_report:
        report_data = result.repair_report.to_dict()
    else:
        # Legacy format fallback
        report_data = {
            "success": result.success,
            "total_attempts": result.total_attempts,
            "final_exit_code": result.final_exit_code,
            "remaining_failures": result.remaining_failures,
        }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(report_data, indent=2), encoding="utf-8")


# Legacy compatibility - maintain original API
RepairReport = RepairAttemptRecord  # Alias for backwards compatibility

================================================================================
 FILE: verify_repair\repairs.py
================================================================================

"""Deterministic repair actions (Layer 1).

This module provides safe, deterministic auto-fix operations that do not
require LLM intervention:
- ruff --fix for lint errors
- ruff format for formatting
- Bootstrap reinstall for missing dependencies

These are "Layer 1" repairs - they can be run automatically without
spawning agent tasks.
"""

from __future__ import annotations

import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Any


@dataclass
class RepairAction:
    """A repair action that was taken."""

    name: str
    command: list[str] | None
    success: bool
    output: str
    files_modified: int = 0


def repair_ruff(project_root: Path, *, verbose: bool = True) -> RepairAction:
    """Run ruff with auto-fix.

    This fixes many lint issues including:
    - Import sorting (I001)
    - Unused imports (F401)
    - Various auto-fixable rules
    """
    if verbose:
        print("[repair] Running ruff check --fix")

    try:
        # First run ruff check --fix
        proc = subprocess.run(
            ["ruff", "check", ".", "--fix", "--unsafe-fixes"],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=120,
        )

        output_parts = [f"ruff check --fix: rc={proc.returncode}"]
        if proc.stdout:
            output_parts.append(proc.stdout[:2000])

        # Count fixed files from output
        files_fixed = proc.stdout.count("Fixed") if proc.stdout else 0

        # Then run ruff format
        if verbose:
            print("[repair] Running ruff format")

        proc2 = subprocess.run(
            ["ruff", "format", "."],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=120,
        )

        output_parts.append(f"ruff format: rc={proc2.returncode}")
        if proc2.stdout:
            output_parts.append(proc2.stdout[:500])
            # Count formatted files
            formatted_count = proc2.stdout.count("reformatted")
            files_fixed += formatted_count

        return RepairAction(
            name="ruff_autofix",
            command=["ruff", "check", ".", "--fix"],
            success=True,
            output="\n".join(output_parts),
            files_modified=files_fixed,
        )

    except FileNotFoundError:
        return RepairAction(
            name="ruff_autofix",
            command=["ruff", "check", ".", "--fix"],
            success=False,
            output="ruff not found in PATH",
        )
    except subprocess.TimeoutExpired:
        return RepairAction(
            name="ruff_autofix",
            command=["ruff", "check", ".", "--fix"],
            success=False,
            output="ruff timed out after 120s",
        )
    except Exception as e:
        return RepairAction(
            name="ruff_autofix",
            command=["ruff", "check", ".", "--fix"],
            success=False,
            output=str(e),
        )


def repair_isort(project_root: Path, *, verbose: bool = True) -> RepairAction:
    """Run isort to fix import ordering (fallback if ruff unavailable)."""
    if verbose:
        print("[repair] Running isort")

    try:
        proc = subprocess.run(
            ["isort", "."],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=120,
        )

        return RepairAction(
            name="isort",
            command=["isort", "."],
            success=(proc.returncode == 0),
            output=proc.stdout[:2000] if proc.stdout else "",
        )

    except FileNotFoundError:
        return RepairAction(
            name="isort",
            command=["isort", "."],
            success=False,
            output="isort not found",
        )
    except Exception as e:
        return RepairAction(
            name="isort",
            command=["isort", "."],
            success=False,
            output=str(e),
        )


def get_applicable_repairs(
    categories: set[str],
    summary_data: dict[str, Any] | None = None,
) -> list[str]:
    """Determine which Layer 1 repairs are applicable for the given categories.

    Returns list of repair names that should be attempted.
    """
    repairs: list[str] = []

    from bridge.verify_repair.classify import FailureCategory

    # Convert string categories to enum values for comparison
    cat_set = {c.value if hasattr(c, "value") else c for c in categories}

    if FailureCategory.LINT_RUFF.value in cat_set:
        repairs.append("ruff_autofix")

    if FailureCategory.LINT_FORMAT.value in cat_set:
        repairs.append("ruff_format")

    if FailureCategory.MISSING_DEPENDENCY.value in cat_set:
        repairs.append("bootstrap")

    return repairs


def apply_repair(
    repair_name: str,
    project_root: Path,
    *,
    verbose: bool = True,
) -> RepairAction:
    """Apply a specific repair by name."""
    if repair_name == "ruff_autofix":
        return repair_ruff(project_root, verbose=verbose)
    elif repair_name == "ruff_format":
        # ruff_autofix already includes format, but this allows explicit format-only
        return repair_ruff(project_root, verbose=verbose)
    elif repair_name == "isort":
        return repair_isort(project_root, verbose=verbose)
    elif repair_name == "bootstrap":
        # Bootstrap is handled separately by the loop
        return RepairAction(
            name="bootstrap",
            command=None,
            success=True,
            output="Bootstrap delegated to loop",
        )
    else:
        return RepairAction(
            name=repair_name,
            command=None,
            success=False,
            output=f"Unknown repair: {repair_name}",
        )
